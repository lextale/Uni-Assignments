{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xk706_rEyYB",
        "outputId": "95f331bd-98c2-4fc4-a8ac-0963b56c01de"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfKOkBBb2wLr",
        "outputId": "6cfb8ed8-2e72-4131-c74c-11cfc4e45cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pickle\n",
        "import contractions\n",
        "import string\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1fqS3ySp2zpO"
      },
      "outputs": [],
      "source": [
        "def clean_up_sentence(sentence):\n",
        "  sentence = contractions.fix(sentence.lower())  \n",
        "  sentence_words = nltk.word_tokenize(sentence.translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
        "  sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
        "  return sentence_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Rkt6ZhwN24Pj"
      },
      "outputs": [],
      "source": [
        "def one_hot_encryption(sentence):\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "  mapping_sentence = []\n",
        "  for w in sentence_words:\n",
        "    if(w in vocabulary):\n",
        "      mapping_sentence.append(vocabulary.index(w))\n",
        "    else:\n",
        "      mapping_sentence.append(0)\n",
        "  padded_sentence = mapping_sentence.copy()\n",
        "  while(len(padded_sentence)<MAXLEN):\n",
        "    padded_sentence.append(0)\n",
        "  return np.array(padded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oPhTGU-M3wv5"
      },
      "outputs": [],
      "source": [
        "def predict_class(sentence):\n",
        "  ERROR_THRESHOLD = 0.5\n",
        "  res = model.predict(np.array([one_hot_encryption(sentence)]))[0]\n",
        "  results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
        "  results.sort(key=lambda x: x[1], reverse=True)\n",
        "  return_list = []\n",
        "  # print(results) -----> [[answer_id, possibility], ...]\n",
        "  for r in results:\n",
        "      return_list.append({\"intent\": r[0], \"question\": questions[int(r[0])], \"probability\": str(r[1])})\n",
        "  #print('predict_class:\\t', results)\n",
        "  return return_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2tQN_4sh5MP6"
      },
      "outputs": [],
      "source": [
        "def get_response(intents_list, id, ans):\n",
        "  #print(intents_list)\n",
        "  #print('\\n')\n",
        "  if(intents_list == []):\n",
        "      result = 'Please provide additional information.'\n",
        "  else:\n",
        "    tag = intents_list[0]['intent'] #! tag = intents_list[0]['intent']\n",
        "    #list_of_intents = intents_json['intents']\n",
        "    result = ans[id.index(tag)]\n",
        "    #print(questions[id.index(tag)])\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_PMH0Xmy5rZ_"
      },
      "outputs": [],
      "source": [
        "def chatbot_response(msg):\n",
        "    ints = predict_class(msg)\n",
        "    res = get_response(ints, id, answers)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "5wDf_4qv5BOK"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "questions = []\n",
        "answers = []\n",
        "id = []\n",
        "MAXLEN = pickle.load(open('/content/drive/MyDrive/_PTYXIAKI/RUN_3/MAXLEN_3.pkl', 'rb'))\n",
        "\n",
        "vocabulary = pickle.load(open('/content/drive/MyDrive/_PTYXIAKI/RUN_3/vocabulary_3.pkl', 'rb'))\n",
        "#classes = pickle.load(open('classes.pkl', 'rb'))\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/_PTYXIAKI/RUN_3/chatbot_model_3.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYiy7VQe5Eso",
        "outputId": "fa7f2a47-6484-4943-e214-4c305470f6d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset in use:\t /content/drive/MyDrive/_PTYXIAKI/RUN_3/training_data_3.txt\n"
          ]
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/_PTYXIAKI/RUN_3/training_data_3.txt', 'r') as file:\n",
        "  print('Dataset in use:\\t', file.name)\n",
        "  for i, line in enumerate(file):\n",
        "    if(i%3==1):\n",
        "      questions.append(line)\n",
        "    elif(i%3==2):\n",
        "      answers.append(line)\n",
        "    elif(i%3==0):\n",
        "      id.append(int(line))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/_PTYXIAKI/intents.txt', 'r') as file:\n",
        "  tmp = []\n",
        "  for i, line in enumerate(file):\n",
        "    if(i%2==0):\n",
        "      tmp.append(line)\n",
        "\n",
        "for item1, item2, item3 in zip(questions, answers, id):\n",
        "  #print(tmp[item3],item1,item3)\n",
        "  #print(questions[questions.index(item1)],tmp[item3])\n",
        "  questions[questions.index(item1)] = tmp[item3]"
      ],
      "metadata": {
        "id": "cPiFshLK5zct"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "with open('output.txt', 'w') as f:\n",
        "  for quest, expected_answer in zip(questions, answers):\n",
        "    f.write('Question:\\t'+quest+'Answer: ')\n",
        "    response = chatbot_response(quest)\n",
        "    f.write(response+'\\n')\n",
        "    if(response == expected_answer):\n",
        "      f.write('Valid Answer\\n----------------------------\\n')\n",
        "    else:\n",
        "      f.write('Irrelevant Answer\\n----------------------------\\n')"
      ],
      "metadata": {
        "id": "GxIyiJG1FC74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUq7VfTQ5tRy"
      },
      "outputs": [],
      "source": [
        "# Main\n",
        "while(1):\n",
        "    print(chatbot_response(input('\\n')))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}