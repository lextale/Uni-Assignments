{
"cells":
    [
        {
        "cell_type": "code",
        "execution_count": 23,
        "metadata":
            {
            "colab":
                {
                "base_uri": "https://localhost:8080/"
                },
            "executionInfo":
                {
                "elapsed": 37,
                "status": "ok",
                "timestamp": 1683882343332
                }
            },
        "outputs":
            [
                {
                "name": "stderr",
                "output_type": "stream",
                "text":
                    [
                    "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
                    "[nltk_data]   Package punkt is already up-to-date!\n",
                    "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
                    "[nltk_data]   Package wordnet is already up-to-date!\n"
                    ]
                }
            ],
        "source":
            [
            "# Import Libraries\n",
            "import string\n",
            "import contractions\n",
            "import pickle\n",
            "import nltk\n",
            "nltk.download('punkt')\n",
            "nltk.download('wordnet')\n",
            "from nltk.stem import WordNetLemmatizer\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras.models import Sequential\n",
            "from tensorflow.keras.layers import Dense, Activation, Dropout, Embedding, SpatialDropout1D, LSTM\n",
            "from tensorflow.keras.optimizers import SGD\n",
            "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
            "from tensorflow.keras.regularizers import l2\n",
            "import numpy as np"
            ]
        },
        {
        "cell_type": "code",
        "execution_count": 24,
        "metadata":
            {
            "executionInfo":
                {
                "elapsed": 28,
                "status": "ok",
                "timestamp": 1683882343334
                }
            },
        "outputs":
            [
            ],
        "source":
            [
            "# Load Dataset\n",
            "def Load_Dataset(filename):\n",
            "  try:\n",
            "    with open(filename, 'r') as file:\n",
            "      print('Dataset in use:\\t', file.name)\n",
            "      documents = []\n",
            "      for i, line in enumerate(file):\n",
            "        if(i%3==1):\n",
            "          question = line  # Question\n",
            "        elif(i%3==2):\n",
            "          answer = line  # Answer\n",
            "          documents.append([feature, question, answer])\n",
            "        else:\n",
            "          feature = int(line) # Question \n",
            "      pass\n",
            "    return documents\n",
            "  except FileNotFoundError:\n",
            "    print(\"File \\'{}\\' not found.\".format(filename))\n",
            "  except:\n",
            "    print(\"An error occurred while accessing the file.\")"
            ]
        },
        {
        "cell_type": "code",
        "execution_count": 25,
        "metadata":
            {
            "executionInfo":
                {
                "elapsed": 28,
                "status": "ok",
                "timestamp": 1683882343335
				},
            "id": "fbio8B3MXjw1"
            },
        "outputs":
            [
            ],
        "source":
            [
            "def Clean_Tokenize(sentence):\n",
            "  # Characters to be removed\n",
            "  ignore_letters = '!\"#$%&\\'()*+,-./:;=?@[\\]^_`{|}~`<>\\n'\n",
            "  # Tokenize question while panctuation and EOL exluded\n",
            "  words = nltk.word_tokenize(contractions.fix(sentence.lower()).translate(str.maketrans(\"\", \"\", ignore_letters)))\n",
            "  \n",
            "  if('<PAD>' in sentence):\n",
            "    words[words.index('pad')] = '<PAD>'\n",
            "  \n",
            "  # Lemmatize list's words\n",
            "  words = [lemmatizer.lemmatize(word) for word in words]\n",
            "  return words"
            ]
        },
        {
        "cell_type": "code",
        "execution_count": 26,
        "metadata":
            {
            "executionInfo":
                {
                "elapsed": 28,
                "status": "ok",
                "timestamp": 1683882343337
                },
            "id": "J5yrGNt86yQP"
            },
        "outputs":
            [
            ],
        "source":
            [
            "# Create Vocabulary\n",
            "def Create_Vocabulary(documents):\n",
            "  vocabulary = ['<PAD>']\n",
            "  for document in documents:\n",
            "    # Lemmatize question's words\n",
            "    words = Clean_Tokenize(document[1])\n",
            "    #Add words to Vocabulary\n",
            "    for word in words:\n",
            "      if(word not in vocabulary):\n",
            "        vocabulary.append(word)\n",
            "  return vocabulary"
            ]
        },
        {
        "cell_type": "code",
        "execution_count": 27,
        "metadata":
            {
            "executionInfo":
                {
                "elapsed": 28,
                "status": "ok",
                "timestamp": 1683882343338
                },
            "id": "PNiuiQCW7Ihp"
            },
        "outputs":
            [
            ],
        "source":
            [
            "# Mapping Dictionary\n",
            "def Mapping_Dictionary(documents, vocabulary):\n",
            "  mapping_dictionary = []\n",
            "  mapping_question = []\n",
            "  for document in documents:\n",
            "    for token in document[3]:\n",
            "      try:\n",
            "        mapping_question.append(vocabulary.index(Clean_Tokenize(token)[0]))\n",
            "      except ValueError:\n",
            "        # Handle the case where the token is not in the vocabulary\n",
            "        print(token, Clean_Tokenize(token)[0], document[3])\n",
            "    mapping_dictionary.append(sorted(mapping_question.copy(), reverse=True))\n",
            "    mapping_question.clear()\n",
            "  return mapping_dictionary"
            ]
        },
        {
        "cell_type": "code",
        "execution_count": 28,
        "metadata":
            {
            "executionInfo":
                {
                "elapsed": 20,
                "status": "ok",
                "timestamp": 1683882343680
                },
            "id": "clTfkCdBxw-b"
            },
        "outputs":
            [
            ],
        "source":
            [
            "def One_Hot_Features(documents, FEATURES_NUM):\n",
            "  one_hot_features = []\n",
            "  for ft in documents:\n",
            "    one_hot_feature = [0] * FEATURES_NUM\n",
            "    one_hot_feature[int(ft[0])] = 1\n",
            "    one_hot_features.append(one_hot_feature.copy())\n",
            "    one_hot_feature.clear()\n",
            "  return one_hot_features"
            ]
        },
        {
        "cell_type": "code",
        "execution_count": 29,
        "metadata":
            {
            "executionInfo":
                {
                "elapsed": 20,
                "status": "ok",
                "timestamp": 1683882343682
                },
            "id": "BUKn1POx6Dyj"
            },
        "outputs":
            [
            ],
        "source":
            [
            "# Initialization\n",
            "\n",
            "# Lemmatizer\n",
            "lemmatizer = WordNetLemmatizer()\n",
            "\n",
            "# Lists\n",
            "questions = []\n",
            "answers = []\n",
            "vocabulary = []\n",
            "documents = []\n",
            "feature = []\n",
            "FEATURES_NUM = 267"
            ]
        },
        {
        "cell_type": "code",
        "execution_count": 30,
        "metadata":
            {
            "colab":
                {
                "base_uri": "https://localhost:8080/"
                },
            "executionInfo":
                {
                "elapsed": 336,
                "status": "ok",
                "timestamp": 1683882343999
                },
            "id": "bZWdwZwV6q6o",
            "outputId": "d0a64194-e320-42cc-befe-7da20eb2f238"
            },
        "outputs":
            [
                {
                "name": "stdout",
                "output_type": "stream",
                "text":
                    [
                    "Dataset in use:\t /content/drive/MyDrive/_PTYXIAKI/RUN_3/training_data_3.txt\n",
                    "Dataset in use:\t /content/drive/MyDrive/_PTYXIAKI/RUN_3/validation_data_3.txt\n"
                    ]
                }
            ],
        "source":
            [
            "# Load Training Data\n",
            "train_docs = Load_Dataset('/content/drive/MyDrive/_PTYXIAKI/RUN_3/training_data_3.txt')\n",
            "\n",
            "# Load Validation Data\n",
            "val_docs = Load_Dataset('/content/drive/MyDrive/_PTYXIAKI/RUN_3/validation_data_3.txt')"
            ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "metadata":
            {
            "id": "4OdTLOb_GHa2"
            },
        "outputs":
            [
            ],
        "source":
            [
            "# Create Vocabulary\n",
            "all_docs = train_docs.copy()\n",
            "all_docs.extend(val_docs)\n",
            "vocabulary = Create_Vocabulary(all_docs)"
            ]
        },
        {
        "cell_type": "code",
        "execution_count": 32,
        "metadata":
            {
            "executionInfo":
                {
                "elapsed": 15786,
                "status": "ok",
                "timestamp": 1683882363309
                },
            "id": "qX-yqeWa7F8r"
            },
        "outputs":
            [
            ],
        "source":
            [
            "\n",
            "train_documents = [x.copy()+[Clean_Tokenize(x[1])] for x in train_docs]\n",
            "val_documents = [x.copy()+[Clean_Tokenize(x[1])] for x in val_docs]\n",
            "\n",
            "# Map Data\n",
            "train_data_map = Mapping_Dictionary(train_documents, vocabulary)\n",
            "val_data_map = Mapping_Dictionary(val_documents, vocabulary)\n",
            "\n",
            "train_features = One_Hot_Features(train_documents, FEATURES_NUM)\n",
            "val_features = One_Hot_Features(val_documents, FEATURES_NUM)"
            ]
        },
        {
        "cell_type": "code",
        "execution_count": 33,
        "metadata":
            {
            "executionInfo":
                {
                "elapsed": 3711,
                "status": "ok",
                "timestamp": 1683882366986
                },
            "id": "hO07j1mjNIrY"
            },
        "outputs":
            [
            ],
        "source":
            [
            "all_docs = [x.copy()+[Clean_Tokenize(x[1])] for x in all_docs]\n",
            "MAXLEN = len(max(np.array(all_docs, dtype=object)[:,3]))\n",
            "if(len(max(np.array(all_docs, dtype=object)[:,3]))<25):\n",
            "  MAXLEN = 25\n",
            "\n",
            "pickle.dump(MAXLEN, open('/content/drive/MyDrive/_PTYXIAKI/MAXLEN_4.pkl', 'wb'))\n",
            "pickle.dump(vocabulary, open('/content/drive/MyDrive/_PTYXIAKI/vocabulary_4.pkl', 'wb'))\n",
            "#pickle.dump(np.array(documents)[:, 1].tolist(), open('classes.pkl', 'wb'))\n",
            "\n",
            "#Zero Padding\n",
            "train_padded_sequences = pad_sequences(train_data_map, padding='post', maxlen=MAXLEN)\n",
            "val_padded_sequences = pad_sequences(val_data_map, padding='post', maxlen=MAXLEN)"
            ]
        },
        {
        "cell_type": "code",
        "execution_count": 34,
        "metadata":
            {
            "colab":
                {
                "base_uri": "https://localhost:8080/"
                },
            "executionInfo":
                {
                "elapsed": 42,
                "status": "ok",
                "timestamp": 1683882366988
                },
            "id": "AeXYro4T8Vgq",
            "outputId": "776ef383-e8dc-4f15-de9f-741fa6b507d3"
            },
        "outputs":
            [
                {
                "name": "stderr",
                "output_type": "stream",
                "text":
                    [
                    "<ipython-input-34-947e8195fc6d>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
                    "  np.array(all_docs)[:, 0]\n"
                    ]
                },
                {
                "data":
                    {
                    "text/plain":
                        [
                        "array([169, 254, 173, ..., 95, 151, 123], dtype=object)"
                        ]
                    },
                "execution_count": 34,
                "metadata":
                    {
                    },
                "output_type": "execute_result"
                }
            ],
        "source":
            [
            "np.array(all_docs)[:, 0]"
            ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "metadata":
            {
            "colab":
                {
                "base_uri": "https://localhost:8080/"
                },
            "id": "M9R7qbLZPcd6",
            "outputId": "6497ca0d-a867-4d4d-cb08-5bcecea80cd6"
            },
        "outputs":
            [
                {
                "name": "stdout",
                "output_type": "stream",
                "text":
                    [
                    "Epoch 1/1000\n",
                    "274/274 [==============================] - 30s 86ms/step - loss: 5.7769 - accuracy: 0.0029 - val_loss: 5.5881 - val_accuracy: 0.0056\n",
                    "Epoch 2/1000\n",
                    "274/274 [==============================] - 21s 77ms/step - loss: 4.7981 - accuracy: 0.0205 - val_loss: 4.1869 - val_accuracy: 0.0356\n",
                    "Epoch 3/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 4.1037 - accuracy: 0.0340 - val_loss: 3.8291 - val_accuracy: 0.0412\n",
                    "Epoch 4/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 3.8852 - accuracy: 0.0380 - val_loss: 3.6717 - val_accuracy: 0.0431\n",
                    "Epoch 5/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 3.7162 - accuracy: 0.0469 - val_loss: 3.5459 - val_accuracy: 0.0581\n",
                    "Epoch 6/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 3.6341 - accuracy: 0.0500 - val_loss: 3.4314 - val_accuracy: 0.0730\n",
                    "Epoch 7/1000\n",
                    "274/274 [==============================] - 26s 96ms/step - loss: 3.5837 - accuracy: 0.0525 - val_loss: 3.3612 - val_accuracy: 0.0712\n",
                    "Epoch 8/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 3.5291 - accuracy: 0.0501 - val_loss: 3.3569 - val_accuracy: 0.0618\n",
                    "Epoch 9/1000\n",
                    "274/274 [==============================] - 21s 77ms/step - loss: 3.4761 - accuracy: 0.0568 - val_loss: 3.3072 - val_accuracy: 0.0712\n",
                    "Epoch 10/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 3.4301 - accuracy: 0.0631 - val_loss: 3.2631 - val_accuracy: 0.0674\n",
                    "Epoch 11/1000\n",
                    "274/274 [==============================] - 21s 77ms/step - loss: 3.4167 - accuracy: 0.0577 - val_loss: 3.2044 - val_accuracy: 0.0861\n",
                    "Epoch 12/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 3.3650 - accuracy: 0.0631 - val_loss: 3.1475 - val_accuracy: 0.1049\n",
                    "Epoch 13/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 3.3463 - accuracy: 0.0672 - val_loss: 3.1204 - val_accuracy: 0.1067\n",
                    "Epoch 14/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 3.3047 - accuracy: 0.0735 - val_loss: 3.0815 - val_accuracy: 0.1086\n",
                    "Epoch 15/1000\n",
                    "274/274 [==============================] - 21s 77ms/step - loss: 3.2810 - accuracy: 0.0769 - val_loss: 3.0587 - val_accuracy: 0.0936\n",
                    "Epoch 16/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 3.2671 - accuracy: 0.0773 - val_loss: 3.0424 - val_accuracy: 0.1180\n",
                    "Epoch 17/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 3.1847 - accuracy: 0.0900 - val_loss: 2.9694 - val_accuracy: 0.1142\n",
                    "Epoch 18/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 3.1718 - accuracy: 0.0935 - val_loss: 2.9536 - val_accuracy: 0.1236\n",
                    "Epoch 19/1000\n",
                    "274/274 [==============================] - 21s 77ms/step - loss: 3.1120 - accuracy: 0.1093 - val_loss: 2.9226 - val_accuracy: 0.1461\n",
                    "Epoch 20/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 3.0759 - accuracy: 0.1112 - val_loss: 2.8910 - val_accuracy: 0.1311\n",
                    "Epoch 21/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 3.0229 - accuracy: 0.1232 - val_loss: 2.7498 - val_accuracy: 0.1629\n",
                    "Epoch 22/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 3.0779 - accuracy: 0.1328 - val_loss: 2.7185 - val_accuracy: 0.1704\n",
                    "Epoch 23/1000\n",
                    "274/274 [==============================] - 21s 77ms/step - loss: 2.9787 - accuracy: 0.1486 - val_loss: 2.9836 - val_accuracy: 0.1479\n",
                    "Epoch 24/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 2.9365 - accuracy: 0.1539 - val_loss: 2.6334 - val_accuracy: 0.1910\n",
                    "Epoch 25/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 2.7695 - accuracy: 0.1732 - val_loss: 2.4872 - val_accuracy: 0.2397\n",
                    "Epoch 26/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 2.6747 - accuracy: 0.1995 - val_loss: 2.5182 - val_accuracy: 0.2285\n",
                    "Epoch 27/1000\n",
                    "274/274 [==============================] - 21s 77ms/step - loss: 2.6251 - accuracy: 0.2072 - val_loss: 2.3932 - val_accuracy: 0.2603\n",
                    "Epoch 28/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 2.5524 - accuracy: 0.2261 - val_loss: 2.2457 - val_accuracy: 0.3109\n",
                    "Epoch 29/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 2.4937 - accuracy: 0.2383 - val_loss: 2.2430 - val_accuracy: 0.3127\n",
                    "Epoch 30/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 2.4433 - accuracy: 0.2560 - val_loss: 2.1379 - val_accuracy: 0.3483\n",
                    "Epoch 31/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 2.3719 - accuracy: 0.2777 - val_loss: 2.0921 - val_accuracy: 0.3558\n",
                    "Epoch 32/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 2.2992 - accuracy: 0.2855 - val_loss: 1.9520 - val_accuracy: 0.4176\n",
                    "Epoch 33/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 2.2382 - accuracy: 0.2997 - val_loss: 1.9316 - val_accuracy: 0.4213\n",
                    "Epoch 34/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 2.1780 - accuracy: 0.3203 - val_loss: 1.8546 - val_accuracy: 0.4326\n",
                    "Epoch 35/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 2.1169 - accuracy: 0.3336 - val_loss: 1.8389 - val_accuracy: 0.4232\n",
                    "Epoch 36/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 2.0990 - accuracy: 0.3386 - val_loss: 2.5340 - val_accuracy: 0.2846\n",
                    "Epoch 37/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 2.1748 - accuracy: 0.3354 - val_loss: 1.6700 - val_accuracy: 0.5356\n",
                    "Epoch 38/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 1.9787 - accuracy: 0.3852 - val_loss: 1.6309 - val_accuracy: 0.5019\n",
                    "Epoch 39/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 1.9454 - accuracy: 0.3987 - val_loss: 1.6623 - val_accuracy: 0.4944\n",
                    "Epoch 40/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 1.9084 - accuracy: 0.4052 - val_loss: 1.5885 - val_accuracy: 0.5075\n",
                    "Epoch 41/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 1.8302 - accuracy: 0.4228 - val_loss: 1.5161 - val_accuracy: 0.5318\n",
                    "Epoch 42/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 1.7907 - accuracy: 0.4492 - val_loss: 1.4831 - val_accuracy: 0.5768\n",
                    "Epoch 43/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 1.7014 - accuracy: 0.4773 - val_loss: 1.3657 - val_accuracy: 0.5787\n",
                    "Epoch 44/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 1.6451 - accuracy: 0.4919 - val_loss: 1.2539 - val_accuracy: 0.6760\n",
                    "Epoch 45/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 1.5742 - accuracy: 0.5223 - val_loss: 1.1927 - val_accuracy: 0.6760\n",
                    "Epoch 46/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 1.5521 - accuracy: 0.5287 - val_loss: 1.1452 - val_accuracy: 0.7135\n",
                    "Epoch 47/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 1.4772 - accuracy: 0.5534 - val_loss: 1.1272 - val_accuracy: 0.6985\n",
                    "Epoch 48/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 1.4716 - accuracy: 0.5655 - val_loss: 1.0722 - val_accuracy: 0.7210\n",
                    "Epoch 49/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 1.4043 - accuracy: 0.5756 - val_loss: 1.0430 - val_accuracy: 0.7453\n",
                    "Epoch 50/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 1.3691 - accuracy: 0.5917 - val_loss: 0.9995 - val_accuracy: 0.7715\n",
                    "Epoch 51/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 1.3042 - accuracy: 0.6113 - val_loss: 0.9386 - val_accuracy: 0.7472\n",
                    "Epoch 52/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 1.2538 - accuracy: 0.6371 - val_loss: 0.9441 - val_accuracy: 0.7622\n",
                    "Epoch 53/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 1.2478 - accuracy: 0.6373 - val_loss: 0.8761 - val_accuracy: 0.7940\n",
                    "Epoch 54/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 1.1911 - accuracy: 0.6541 - val_loss: 0.7704 - val_accuracy: 0.8352\n",
                    "Epoch 55/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 1.1351 - accuracy: 0.6776 - val_loss: 1.0071 - val_accuracy: 0.7228\n",
                    "Epoch 56/1000\n",
                    "274/274 [==============================] - 21s 77ms/step - loss: 1.1181 - accuracy: 0.6826 - val_loss: 0.7391 - val_accuracy: 0.8521\n",
                    "Epoch 57/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 1.1022 - accuracy: 0.6898 - val_loss: 0.7366 - val_accuracy: 0.8390\n",
                    "Epoch 58/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 1.0518 - accuracy: 0.7093 - val_loss: 0.7219 - val_accuracy: 0.8408\n",
                    "Epoch 59/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 1.0077 - accuracy: 0.7197 - val_loss: 0.6939 - val_accuracy: 0.8502\n",
                    "Epoch 60/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.9957 - accuracy: 0.7251 - val_loss: 0.6137 - val_accuracy: 0.8876\n",
                    "Epoch 61/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.9823 - accuracy: 0.7320 - val_loss: 0.6594 - val_accuracy: 0.8502\n",
                    "Epoch 62/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.9630 - accuracy: 0.7370 - val_loss: 0.6288 - val_accuracy: 0.8801\n",
                    "Epoch 63/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 0.8761 - accuracy: 0.7725 - val_loss: 0.5794 - val_accuracy: 0.9064\n",
                    "Epoch 64/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.8779 - accuracy: 0.7682 - val_loss: 0.5830 - val_accuracy: 0.8801\n",
                    "Epoch 65/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.8332 - accuracy: 0.7855 - val_loss: 0.5517 - val_accuracy: 0.9045\n",
                    "Epoch 66/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.7962 - accuracy: 0.7972 - val_loss: 0.5090 - val_accuracy: 0.9176\n",
                    "Epoch 67/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.7912 - accuracy: 0.7948 - val_loss: 0.5065 - val_accuracy: 0.9139\n",
                    "Epoch 68/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.7568 - accuracy: 0.8040 - val_loss: 0.5425 - val_accuracy: 0.9082\n",
                    "Epoch 69/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.7256 - accuracy: 0.8189 - val_loss: 0.4743 - val_accuracy: 0.9232\n",
                    "Epoch 70/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.7349 - accuracy: 0.8167 - val_loss: 0.4132 - val_accuracy: 0.9326\n",
                    "Epoch 71/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.7058 - accuracy: 0.8251 - val_loss: 0.4333 - val_accuracy: 0.9251\n",
                    "Epoch 72/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.7440 - accuracy: 0.8140 - val_loss: 0.4558 - val_accuracy: 0.9270\n",
                    "Epoch 73/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.6732 - accuracy: 0.8397 - val_loss: 0.4199 - val_accuracy: 0.9307\n",
                    "Epoch 74/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.6305 - accuracy: 0.8544 - val_loss: 0.3899 - val_accuracy: 0.9457\n",
                    "Epoch 75/1000\n",
                    "274/274 [==============================] - 21s 77ms/step - loss: 0.6190 - accuracy: 0.8561 - val_loss: 0.3744 - val_accuracy: 0.9476\n",
                    "Epoch 76/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.6195 - accuracy: 0.8565 - val_loss: 0.3514 - val_accuracy: 0.9588\n",
                    "Epoch 77/1000\n",
                    "274/274 [==============================] - 21s 77ms/step - loss: 0.6192 - accuracy: 0.8493 - val_loss: 0.3743 - val_accuracy: 0.9551\n",
                    "Epoch 78/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.5759 - accuracy: 0.8618 - val_loss: 0.3476 - val_accuracy: 0.9438\n",
                    "Epoch 79/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.5838 - accuracy: 0.8639 - val_loss: 0.3420 - val_accuracy: 0.9569\n",
                    "Epoch 80/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.5596 - accuracy: 0.8762 - val_loss: 0.3931 - val_accuracy: 0.9251\n",
                    "Epoch 81/1000\n",
                    "274/274 [==============================] - 21s 77ms/step - loss: 0.5769 - accuracy: 0.8686 - val_loss: 0.3798 - val_accuracy: 0.9494\n",
                    "Epoch 82/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 0.5250 - accuracy: 0.8866 - val_loss: 0.3292 - val_accuracy: 0.9607\n",
                    "Epoch 83/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.5305 - accuracy: 0.8800 - val_loss: 0.3507 - val_accuracy: 0.9476\n",
                    "Epoch 84/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.5204 - accuracy: 0.8843 - val_loss: 0.3350 - val_accuracy: 0.9513\n",
                    "Epoch 85/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.4958 - accuracy: 0.8933 - val_loss: 0.3016 - val_accuracy: 0.9607\n",
                    "Epoch 86/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.5382 - accuracy: 0.8862 - val_loss: 0.3273 - val_accuracy: 0.9625\n",
                    "Epoch 87/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.5228 - accuracy: 0.8863 - val_loss: 0.3172 - val_accuracy: 0.9513\n",
                    "Epoch 88/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.4891 - accuracy: 0.8938 - val_loss: 0.2874 - val_accuracy: 0.9663\n",
                    "Epoch 89/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.4438 - accuracy: 0.9052 - val_loss: 0.2783 - val_accuracy: 0.9607\n",
                    "Epoch 90/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.4732 - accuracy: 0.8983 - val_loss: 0.4489 - val_accuracy: 0.9401\n",
                    "Epoch 91/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.5681 - accuracy: 0.8841 - val_loss: 0.2972 - val_accuracy: 0.9625\n",
                    "Epoch 92/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.4518 - accuracy: 0.9097 - val_loss: 0.3311 - val_accuracy: 0.9494\n",
                    "Epoch 93/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.4498 - accuracy: 0.9059 - val_loss: 0.2736 - val_accuracy: 0.9644\n",
                    "Epoch 94/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.4049 - accuracy: 0.9183 - val_loss: 0.2662 - val_accuracy: 0.9700\n",
                    "Epoch 95/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.4677 - accuracy: 0.8955 - val_loss: 0.2985 - val_accuracy: 0.9682\n",
                    "Epoch 96/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.4500 - accuracy: 0.9030 - val_loss: 0.2469 - val_accuracy: 0.9682\n",
                    "Epoch 97/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.4246 - accuracy: 0.9097 - val_loss: 0.2646 - val_accuracy: 0.9663\n",
                    "Epoch 98/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.3993 - accuracy: 0.9161 - val_loss: 0.2272 - val_accuracy: 0.9757\n",
                    "Epoch 99/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.4274 - accuracy: 0.9106 - val_loss: 0.2595 - val_accuracy: 0.9738\n",
                    "Epoch 100/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.4125 - accuracy: 0.9201 - val_loss: 0.2519 - val_accuracy: 0.9682\n",
                    "Epoch 101/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.3813 - accuracy: 0.9256 - val_loss: 0.2789 - val_accuracy: 0.9625\n",
                    "Epoch 102/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.3971 - accuracy: 0.9164 - val_loss: 0.2305 - val_accuracy: 0.9738\n",
                    "Epoch 103/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.3766 - accuracy: 0.9277 - val_loss: 0.2429 - val_accuracy: 0.9607\n",
                    "Epoch 104/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.3844 - accuracy: 0.9177 - val_loss: 0.2781 - val_accuracy: 0.9682\n",
                    "Epoch 105/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.3997 - accuracy: 0.9163 - val_loss: 0.2547 - val_accuracy: 0.9663\n",
                    "Epoch 106/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.3651 - accuracy: 0.9271 - val_loss: 0.2443 - val_accuracy: 0.9682\n",
                    "Epoch 107/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.3741 - accuracy: 0.9233 - val_loss: 0.2452 - val_accuracy: 0.9719\n",
                    "Epoch 108/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.3813 - accuracy: 0.9193 - val_loss: 0.2571 - val_accuracy: 0.9682\n",
                    "Epoch 109/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.3586 - accuracy: 0.9294 - val_loss: 0.2381 - val_accuracy: 0.9682\n",
                    "Epoch 110/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.3493 - accuracy: 0.9303 - val_loss: 0.2365 - val_accuracy: 0.9682\n",
                    "Epoch 111/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.3584 - accuracy: 0.9243 - val_loss: 0.2433 - val_accuracy: 0.9700\n",
                    "Epoch 112/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.3673 - accuracy: 0.9224 - val_loss: 0.2616 - val_accuracy: 0.9700\n",
                    "Epoch 113/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.3590 - accuracy: 0.9296 - val_loss: 0.2094 - val_accuracy: 0.9794\n",
                    "Epoch 114/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.3597 - accuracy: 0.9297 - val_loss: 0.2172 - val_accuracy: 0.9719\n",
                    "Epoch 115/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.3504 - accuracy: 0.9320 - val_loss: 0.2136 - val_accuracy: 0.9738\n",
                    "Epoch 116/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.3303 - accuracy: 0.9357 - val_loss: 0.2151 - val_accuracy: 0.9757\n",
                    "Epoch 117/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.3588 - accuracy: 0.9282 - val_loss: 0.2184 - val_accuracy: 0.9794\n",
                    "Epoch 118/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.3262 - accuracy: 0.9347 - val_loss: 0.2352 - val_accuracy: 0.9719\n",
                    "Epoch 119/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.3163 - accuracy: 0.9353 - val_loss: 0.2190 - val_accuracy: 0.9775\n",
                    "Epoch 120/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.3014 - accuracy: 0.9417 - val_loss: 0.2022 - val_accuracy: 0.9775\n",
                    "Epoch 121/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.2915 - accuracy: 0.9465 - val_loss: 0.1937 - val_accuracy: 0.9738\n",
                    "Epoch 122/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.3087 - accuracy: 0.9389 - val_loss: 0.2278 - val_accuracy: 0.9700\n",
                    "Epoch 123/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.3288 - accuracy: 0.9329 - val_loss: 0.2153 - val_accuracy: 0.9757\n",
                    "Epoch 124/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.3068 - accuracy: 0.9388 - val_loss: 0.2161 - val_accuracy: 0.9738\n",
                    "Epoch 125/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.3364 - accuracy: 0.9318 - val_loss: 0.2167 - val_accuracy: 0.9738\n",
                    "Epoch 126/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.3476 - accuracy: 0.9345 - val_loss: 0.2232 - val_accuracy: 0.9757\n",
                    "Epoch 127/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.4553 - accuracy: 0.9168 - val_loss: 0.3159 - val_accuracy: 0.9813\n",
                    "Epoch 128/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.3920 - accuracy: 0.9386 - val_loss: 0.2480 - val_accuracy: 0.9813\n",
                    "Epoch 129/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.3384 - accuracy: 0.9442 - val_loss: 0.2121 - val_accuracy: 0.9794\n",
                    "Epoch 130/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.3187 - accuracy: 0.9430 - val_loss: 0.2078 - val_accuracy: 0.9813\n",
                    "Epoch 131/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.3039 - accuracy: 0.9459 - val_loss: 0.2118 - val_accuracy: 0.9700\n",
                    "Epoch 132/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.3300 - accuracy: 0.9418 - val_loss: 0.2186 - val_accuracy: 0.9738\n",
                    "Epoch 133/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.3383 - accuracy: 0.9370 - val_loss: 0.2210 - val_accuracy: 0.9738\n",
                    "Epoch 134/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.3006 - accuracy: 0.9448 - val_loss: 0.1976 - val_accuracy: 0.9794\n",
                    "Epoch 135/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.2741 - accuracy: 0.9519 - val_loss: 0.1835 - val_accuracy: 0.9757\n",
                    "Epoch 136/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.2710 - accuracy: 0.9510 - val_loss: 0.1831 - val_accuracy: 0.9775\n",
                    "Epoch 137/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.2702 - accuracy: 0.9470 - val_loss: 0.2115 - val_accuracy: 0.9775\n",
                    "Epoch 138/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.2682 - accuracy: 0.9471 - val_loss: 0.2178 - val_accuracy: 0.9682\n",
                    "Epoch 139/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.2943 - accuracy: 0.9436 - val_loss: 0.2058 - val_accuracy: 0.9700\n",
                    "Epoch 140/1000\n",
                    "274/274 [==============================] - 32s 118ms/step - loss: 0.2532 - accuracy: 0.9524 - val_loss: 0.1944 - val_accuracy: 0.9738\n",
                    "Epoch 141/1000\n",
                    "274/274 [==============================] - 42s 155ms/step - loss: 0.2731 - accuracy: 0.9451 - val_loss: 0.1954 - val_accuracy: 0.9794\n",
                    "Epoch 142/1000\n",
                    "274/274 [==============================] - 28s 104ms/step - loss: 0.2671 - accuracy: 0.9516 - val_loss: 0.2007 - val_accuracy: 0.9738\n",
                    "Epoch 143/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.2796 - accuracy: 0.9436 - val_loss: 0.2055 - val_accuracy: 0.9794\n",
                    "Epoch 144/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.2624 - accuracy: 0.9513 - val_loss: 0.1714 - val_accuracy: 0.9813\n",
                    "Epoch 145/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.2545 - accuracy: 0.9521 - val_loss: 0.2347 - val_accuracy: 0.9682\n",
                    "Epoch 146/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.2574 - accuracy: 0.9494 - val_loss: 0.1937 - val_accuracy: 0.9738\n",
                    "Epoch 147/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.2467 - accuracy: 0.9521 - val_loss: 0.1741 - val_accuracy: 0.9813\n",
                    "Epoch 148/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.2483 - accuracy: 0.9518 - val_loss: 0.1918 - val_accuracy: 0.9775\n",
                    "Epoch 149/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.2553 - accuracy: 0.9503 - val_loss: 0.1914 - val_accuracy: 0.9738\n",
                    "Epoch 150/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.2657 - accuracy: 0.9486 - val_loss: 0.1952 - val_accuracy: 0.9757\n",
                    "Epoch 151/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.2824 - accuracy: 0.9472 - val_loss: 0.1842 - val_accuracy: 0.9775\n",
                    "Epoch 152/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.2577 - accuracy: 0.9522 - val_loss: 0.2211 - val_accuracy: 0.9682\n",
                    "Epoch 153/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.2523 - accuracy: 0.9553 - val_loss: 0.1981 - val_accuracy: 0.9794\n",
                    "Epoch 154/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.2601 - accuracy: 0.9518 - val_loss: 0.1929 - val_accuracy: 0.9775\n",
                    "Epoch 155/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.2279 - accuracy: 0.9572 - val_loss: 0.1955 - val_accuracy: 0.9757\n",
                    "Epoch 156/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.2288 - accuracy: 0.9588 - val_loss: 0.1706 - val_accuracy: 0.9794\n",
                    "Epoch 157/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.2334 - accuracy: 0.9547 - val_loss: 0.1878 - val_accuracy: 0.9719\n",
                    "Epoch 158/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.2435 - accuracy: 0.9493 - val_loss: 0.2082 - val_accuracy: 0.9700\n",
                    "Epoch 159/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.2413 - accuracy: 0.9544 - val_loss: 0.1722 - val_accuracy: 0.9831\n",
                    "Epoch 160/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 0.2623 - accuracy: 0.9499 - val_loss: 0.1775 - val_accuracy: 0.9831\n",
                    "Epoch 161/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.2142 - accuracy: 0.9611 - val_loss: 0.1586 - val_accuracy: 0.9794\n",
                    "Epoch 162/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 0.2218 - accuracy: 0.9567 - val_loss: 0.1909 - val_accuracy: 0.9738\n",
                    "Epoch 163/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.2371 - accuracy: 0.9540 - val_loss: 0.1710 - val_accuracy: 0.9775\n",
                    "Epoch 164/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.2753 - accuracy: 0.9500 - val_loss: 0.1895 - val_accuracy: 0.9831\n",
                    "Epoch 165/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.2183 - accuracy: 0.9617 - val_loss: 0.1674 - val_accuracy: 0.9813\n",
                    "Epoch 166/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.2189 - accuracy: 0.9617 - val_loss: 0.1812 - val_accuracy: 0.9850\n",
                    "Epoch 167/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.2406 - accuracy: 0.9553 - val_loss: 0.1899 - val_accuracy: 0.9757\n",
                    "Epoch 168/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.2616 - accuracy: 0.9519 - val_loss: 0.1865 - val_accuracy: 0.9738\n",
                    "Epoch 169/1000\n",
                    "274/274 [==============================] - 21s 78ms/step - loss: 0.2338 - accuracy: 0.9569 - val_loss: 0.2014 - val_accuracy: 0.9719\n",
                    "Epoch 170/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.2477 - accuracy: 0.9557 - val_loss: 0.1763 - val_accuracy: 0.9850\n",
                    "Epoch 171/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.2100 - accuracy: 0.9638 - val_loss: 0.1643 - val_accuracy: 0.9775\n",
                    "Epoch 172/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.2284 - accuracy: 0.9579 - val_loss: 0.1688 - val_accuracy: 0.9794\n",
                    "Epoch 173/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.2261 - accuracy: 0.9548 - val_loss: 0.1899 - val_accuracy: 0.9738\n",
                    "Epoch 174/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.2297 - accuracy: 0.9588 - val_loss: 0.1758 - val_accuracy: 0.9775\n",
                    "Epoch 175/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.2256 - accuracy: 0.9586 - val_loss: 0.1863 - val_accuracy: 0.9757\n",
                    "Epoch 176/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.2115 - accuracy: 0.9626 - val_loss: 0.1763 - val_accuracy: 0.9794\n",
                    "Epoch 177/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.2218 - accuracy: 0.9578 - val_loss: 0.1816 - val_accuracy: 0.9794\n",
                    "Epoch 178/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.1985 - accuracy: 0.9646 - val_loss: 0.1623 - val_accuracy: 0.9794\n",
                    "Epoch 179/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.2510 - accuracy: 0.9512 - val_loss: 0.1811 - val_accuracy: 0.9794\n",
                    "Epoch 180/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.2054 - accuracy: 0.9655 - val_loss: 0.1789 - val_accuracy: 0.9794\n",
                    "Epoch 181/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.2223 - accuracy: 0.9600 - val_loss: 0.2204 - val_accuracy: 0.9700\n",
                    "Epoch 182/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.2689 - accuracy: 0.9519 - val_loss: 0.2285 - val_accuracy: 0.9700\n",
                    "Epoch 183/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.2042 - accuracy: 0.9665 - val_loss: 0.1625 - val_accuracy: 0.9775\n",
                    "Epoch 184/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1865 - accuracy: 0.9660 - val_loss: 0.1522 - val_accuracy: 0.9813\n",
                    "Epoch 185/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1921 - accuracy: 0.9652 - val_loss: 0.1621 - val_accuracy: 0.9831\n",
                    "Epoch 186/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.2033 - accuracy: 0.9613 - val_loss: 0.1794 - val_accuracy: 0.9775\n",
                    "Epoch 187/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.2061 - accuracy: 0.9619 - val_loss: 0.1755 - val_accuracy: 0.9757\n",
                    "Epoch 188/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1930 - accuracy: 0.9643 - val_loss: 0.1785 - val_accuracy: 0.9738\n",
                    "Epoch 189/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.2307 - accuracy: 0.9591 - val_loss: 0.1630 - val_accuracy: 0.9813\n",
                    "Epoch 190/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1909 - accuracy: 0.9668 - val_loss: 0.1748 - val_accuracy: 0.9719\n",
                    "Epoch 191/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.2301 - accuracy: 0.9588 - val_loss: 0.2088 - val_accuracy: 0.9794\n",
                    "Epoch 192/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.2244 - accuracy: 0.9627 - val_loss: 0.1588 - val_accuracy: 0.9850\n",
                    "Epoch 193/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.1850 - accuracy: 0.9665 - val_loss: 0.1569 - val_accuracy: 0.9850\n",
                    "Epoch 194/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1833 - accuracy: 0.9651 - val_loss: 0.1614 - val_accuracy: 0.9738\n",
                    "Epoch 195/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.2618 - accuracy: 0.9516 - val_loss: 0.2018 - val_accuracy: 0.9775\n",
                    "Epoch 196/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.2041 - accuracy: 0.9649 - val_loss: 0.1919 - val_accuracy: 0.9700\n",
                    "Epoch 197/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.2072 - accuracy: 0.9645 - val_loss: 0.1756 - val_accuracy: 0.9831\n",
                    "Epoch 198/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.2042 - accuracy: 0.9636 - val_loss: 0.1593 - val_accuracy: 0.9775\n",
                    "Epoch 199/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.2362 - accuracy: 0.9569 - val_loss: 0.1844 - val_accuracy: 0.9813\n",
                    "Epoch 200/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1903 - accuracy: 0.9674 - val_loss: 0.1826 - val_accuracy: 0.9757\n",
                    "Epoch 201/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1919 - accuracy: 0.9665 - val_loss: 0.1641 - val_accuracy: 0.9794\n",
                    "Epoch 202/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1970 - accuracy: 0.9641 - val_loss: 0.1732 - val_accuracy: 0.9757\n",
                    "Epoch 203/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.2332 - accuracy: 0.9567 - val_loss: 0.1999 - val_accuracy: 0.9738\n",
                    "Epoch 204/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.2080 - accuracy: 0.9670 - val_loss: 0.1565 - val_accuracy: 0.9813\n",
                    "Epoch 205/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1784 - accuracy: 0.9696 - val_loss: 0.1488 - val_accuracy: 0.9813\n",
                    "Epoch 206/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1871 - accuracy: 0.9654 - val_loss: 0.1517 - val_accuracy: 0.9794\n",
                    "Epoch 207/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.2020 - accuracy: 0.9633 - val_loss: 0.1588 - val_accuracy: 0.9794\n",
                    "Epoch 208/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1704 - accuracy: 0.9699 - val_loss: 0.1325 - val_accuracy: 0.9831\n",
                    "Epoch 209/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1898 - accuracy: 0.9670 - val_loss: 0.1603 - val_accuracy: 0.9831\n",
                    "Epoch 210/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.2319 - accuracy: 0.9585 - val_loss: 0.1952 - val_accuracy: 0.9757\n",
                    "Epoch 211/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1888 - accuracy: 0.9686 - val_loss: 0.1612 - val_accuracy: 0.9775\n",
                    "Epoch 212/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1646 - accuracy: 0.9718 - val_loss: 0.1499 - val_accuracy: 0.9831\n",
                    "Epoch 213/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1719 - accuracy: 0.9683 - val_loss: 0.1647 - val_accuracy: 0.9775\n",
                    "Epoch 214/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.2010 - accuracy: 0.9613 - val_loss: 0.1604 - val_accuracy: 0.9794\n",
                    "Epoch 215/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.1881 - accuracy: 0.9670 - val_loss: 0.1868 - val_accuracy: 0.9738\n",
                    "Epoch 216/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1793 - accuracy: 0.9677 - val_loss: 0.1796 - val_accuracy: 0.9738\n",
                    "Epoch 217/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1919 - accuracy: 0.9638 - val_loss: 0.1732 - val_accuracy: 0.9757\n",
                    "Epoch 218/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1823 - accuracy: 0.9654 - val_loss: 0.1869 - val_accuracy: 0.9813\n",
                    "Epoch 219/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1655 - accuracy: 0.9702 - val_loss: 0.1550 - val_accuracy: 0.9813\n",
                    "Epoch 220/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1832 - accuracy: 0.9649 - val_loss: 0.1795 - val_accuracy: 0.9738\n",
                    "Epoch 221/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1792 - accuracy: 0.9686 - val_loss: 0.1463 - val_accuracy: 0.9850\n",
                    "Epoch 222/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1939 - accuracy: 0.9641 - val_loss: 0.1748 - val_accuracy: 0.9775\n",
                    "Epoch 223/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1831 - accuracy: 0.9668 - val_loss: 0.1651 - val_accuracy: 0.9813\n",
                    "Epoch 224/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.2001 - accuracy: 0.9613 - val_loss: 0.1576 - val_accuracy: 0.9850\n",
                    "Epoch 225/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1932 - accuracy: 0.9654 - val_loss: 0.1560 - val_accuracy: 0.9794\n",
                    "Epoch 226/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1837 - accuracy: 0.9677 - val_loss: 0.1752 - val_accuracy: 0.9775\n",
                    "Epoch 227/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1883 - accuracy: 0.9667 - val_loss: 0.1673 - val_accuracy: 0.9813\n",
                    "Epoch 228/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.2048 - accuracy: 0.9654 - val_loss: 0.1773 - val_accuracy: 0.9831\n",
                    "Epoch 229/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1690 - accuracy: 0.9714 - val_loss: 0.1687 - val_accuracy: 0.9813\n",
                    "Epoch 230/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1737 - accuracy: 0.9662 - val_loss: 0.1998 - val_accuracy: 0.9738\n",
                    "Epoch 231/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.2324 - accuracy: 0.9601 - val_loss: 0.1830 - val_accuracy: 0.9813\n",
                    "Epoch 232/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1624 - accuracy: 0.9740 - val_loss: 0.1694 - val_accuracy: 0.9813\n",
                    "Epoch 233/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1607 - accuracy: 0.9728 - val_loss: 0.1689 - val_accuracy: 0.9775\n",
                    "Epoch 234/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.1848 - accuracy: 0.9670 - val_loss: 0.1754 - val_accuracy: 0.9738\n",
                    "Epoch 235/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1687 - accuracy: 0.9706 - val_loss: 0.1513 - val_accuracy: 0.9775\n",
                    "Epoch 236/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1783 - accuracy: 0.9668 - val_loss: 0.1736 - val_accuracy: 0.9831\n",
                    "Epoch 237/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.1985 - accuracy: 0.9648 - val_loss: 0.2622 - val_accuracy: 0.9663\n",
                    "Epoch 238/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.2269 - accuracy: 0.9611 - val_loss: 0.2147 - val_accuracy: 0.9757\n",
                    "Epoch 239/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1700 - accuracy: 0.9752 - val_loss: 0.1670 - val_accuracy: 0.9813\n",
                    "Epoch 240/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1448 - accuracy: 0.9737 - val_loss: 0.1440 - val_accuracy: 0.9813\n",
                    "Epoch 241/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 0.1602 - accuracy: 0.9698 - val_loss: 0.1779 - val_accuracy: 0.9794\n",
                    "Epoch 242/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1634 - accuracy: 0.9702 - val_loss: 0.2171 - val_accuracy: 0.9757\n",
                    "Epoch 243/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1839 - accuracy: 0.9679 - val_loss: 0.1876 - val_accuracy: 0.9813\n",
                    "Epoch 244/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 0.1795 - accuracy: 0.9677 - val_loss: 0.1656 - val_accuracy: 0.9813\n",
                    "Epoch 245/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1805 - accuracy: 0.9658 - val_loss: 0.2027 - val_accuracy: 0.9794\n",
                    "Epoch 246/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 0.1956 - accuracy: 0.9660 - val_loss: 0.1750 - val_accuracy: 0.9775\n",
                    "Epoch 247/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1593 - accuracy: 0.9724 - val_loss: 0.1697 - val_accuracy: 0.9775\n",
                    "Epoch 248/1000\n",
                    "274/274 [==============================] - 22s 79ms/step - loss: 0.1545 - accuracy: 0.9722 - val_loss: 0.1884 - val_accuracy: 0.9719\n",
                    "Epoch 249/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1662 - accuracy: 0.9706 - val_loss: 0.1572 - val_accuracy: 0.9813\n",
                    "Epoch 250/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1571 - accuracy: 0.9715 - val_loss: 0.1823 - val_accuracy: 0.9794\n",
                    "Epoch 251/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.2056 - accuracy: 0.9643 - val_loss: 0.1861 - val_accuracy: 0.9738\n",
                    "Epoch 252/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 0.1543 - accuracy: 0.9746 - val_loss: 0.1628 - val_accuracy: 0.9813\n",
                    "Epoch 253/1000\n",
                    "274/274 [==============================] - 28s 101ms/step - loss: 0.1616 - accuracy: 0.9712 - val_loss: 0.1499 - val_accuracy: 0.9831\n",
                    "Epoch 254/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1731 - accuracy: 0.9714 - val_loss: 0.1714 - val_accuracy: 0.9794\n",
                    "Epoch 255/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.2528 - accuracy: 0.9626 - val_loss: 0.2272 - val_accuracy: 0.9794\n",
                    "Epoch 256/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.2400 - accuracy: 0.9677 - val_loss: 0.2061 - val_accuracy: 0.9813\n",
                    "Epoch 257/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1725 - accuracy: 0.9752 - val_loss: 0.2010 - val_accuracy: 0.9757\n",
                    "Epoch 258/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1583 - accuracy: 0.9741 - val_loss: 0.1753 - val_accuracy: 0.9794\n",
                    "Epoch 259/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1466 - accuracy: 0.9749 - val_loss: 0.1607 - val_accuracy: 0.9794\n",
                    "Epoch 260/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1522 - accuracy: 0.9727 - val_loss: 0.1481 - val_accuracy: 0.9813\n",
                    "Epoch 261/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1723 - accuracy: 0.9661 - val_loss: 0.1963 - val_accuracy: 0.9775\n",
                    "Epoch 262/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1622 - accuracy: 0.9737 - val_loss: 0.1734 - val_accuracy: 0.9775\n",
                    "Epoch 263/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1578 - accuracy: 0.9709 - val_loss: 0.1979 - val_accuracy: 0.9757\n",
                    "Epoch 264/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1442 - accuracy: 0.9746 - val_loss: 0.1471 - val_accuracy: 0.9831\n",
                    "Epoch 265/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1460 - accuracy: 0.9724 - val_loss: 0.1637 - val_accuracy: 0.9831\n",
                    "Epoch 266/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1556 - accuracy: 0.9709 - val_loss: 0.1658 - val_accuracy: 0.9794\n",
                    "Epoch 267/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1652 - accuracy: 0.9706 - val_loss: 0.1901 - val_accuracy: 0.9813\n",
                    "Epoch 268/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1835 - accuracy: 0.9674 - val_loss: 0.1676 - val_accuracy: 0.9794\n",
                    "Epoch 269/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1703 - accuracy: 0.9718 - val_loss: 0.2098 - val_accuracy: 0.9738\n",
                    "Epoch 270/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1739 - accuracy: 0.9700 - val_loss: 0.1802 - val_accuracy: 0.9850\n",
                    "Epoch 271/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1740 - accuracy: 0.9719 - val_loss: 0.1625 - val_accuracy: 0.9813\n",
                    "Epoch 272/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1597 - accuracy: 0.9719 - val_loss: 0.1581 - val_accuracy: 0.9850\n",
                    "Epoch 273/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1458 - accuracy: 0.9749 - val_loss: 0.1570 - val_accuracy: 0.9831\n",
                    "Epoch 274/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1693 - accuracy: 0.9700 - val_loss: 0.1776 - val_accuracy: 0.9757\n",
                    "Epoch 275/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1582 - accuracy: 0.9733 - val_loss: 0.1623 - val_accuracy: 0.9794\n",
                    "Epoch 276/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1447 - accuracy: 0.9738 - val_loss: 0.1695 - val_accuracy: 0.9813\n",
                    "Epoch 277/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1669 - accuracy: 0.9696 - val_loss: 0.1840 - val_accuracy: 0.9831\n",
                    "Epoch 278/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1967 - accuracy: 0.9692 - val_loss: 0.2122 - val_accuracy: 0.9757\n",
                    "Epoch 279/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1799 - accuracy: 0.9716 - val_loss: 0.1888 - val_accuracy: 0.9813\n",
                    "Epoch 280/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1578 - accuracy: 0.9727 - val_loss: 0.1508 - val_accuracy: 0.9850\n",
                    "Epoch 281/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1567 - accuracy: 0.9725 - val_loss: 0.1756 - val_accuracy: 0.9831\n",
                    "Epoch 282/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1414 - accuracy: 0.9768 - val_loss: 0.1360 - val_accuracy: 0.9813\n",
                    "Epoch 283/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1572 - accuracy: 0.9725 - val_loss: 0.1708 - val_accuracy: 0.9813\n",
                    "Epoch 284/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1543 - accuracy: 0.9719 - val_loss: 0.1679 - val_accuracy: 0.9813\n",
                    "Epoch 285/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1478 - accuracy: 0.9734 - val_loss: 0.1587 - val_accuracy: 0.9831\n",
                    "Epoch 286/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1935 - accuracy: 0.9686 - val_loss: 0.1846 - val_accuracy: 0.9850\n",
                    "Epoch 287/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1605 - accuracy: 0.9733 - val_loss: 0.1504 - val_accuracy: 0.9831\n",
                    "Epoch 288/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1546 - accuracy: 0.9733 - val_loss: 0.1622 - val_accuracy: 0.9869\n",
                    "Epoch 289/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1522 - accuracy: 0.9740 - val_loss: 0.1612 - val_accuracy: 0.9813\n",
                    "Epoch 290/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1578 - accuracy: 0.9714 - val_loss: 0.1936 - val_accuracy: 0.9757\n",
                    "Epoch 291/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1515 - accuracy: 0.9735 - val_loss: 0.1664 - val_accuracy: 0.9813\n",
                    "Epoch 292/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1316 - accuracy: 0.9776 - val_loss: 0.1444 - val_accuracy: 0.9831\n",
                    "Epoch 293/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1291 - accuracy: 0.9763 - val_loss: 0.1455 - val_accuracy: 0.9813\n",
                    "Epoch 294/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1744 - accuracy: 0.9698 - val_loss: 0.1712 - val_accuracy: 0.9813\n",
                    "Epoch 295/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1845 - accuracy: 0.9689 - val_loss: 0.1920 - val_accuracy: 0.9794\n",
                    "Epoch 296/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1284 - accuracy: 0.9807 - val_loss: 0.1545 - val_accuracy: 0.9831\n",
                    "Epoch 297/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1486 - accuracy: 0.9716 - val_loss: 0.1662 - val_accuracy: 0.9794\n",
                    "Epoch 298/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1397 - accuracy: 0.9743 - val_loss: 0.1495 - val_accuracy: 0.9831\n",
                    "Epoch 299/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1394 - accuracy: 0.9744 - val_loss: 0.1481 - val_accuracy: 0.9831\n",
                    "Epoch 300/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1455 - accuracy: 0.9746 - val_loss: 0.2243 - val_accuracy: 0.9775\n",
                    "Epoch 301/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1795 - accuracy: 0.9695 - val_loss: 0.1743 - val_accuracy: 0.9813\n",
                    "Epoch 302/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1688 - accuracy: 0.9730 - val_loss: 0.1698 - val_accuracy: 0.9850\n",
                    "Epoch 303/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1533 - accuracy: 0.9750 - val_loss: 0.1847 - val_accuracy: 0.9775\n",
                    "Epoch 304/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1411 - accuracy: 0.9766 - val_loss: 0.1580 - val_accuracy: 0.9850\n",
                    "Epoch 305/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1324 - accuracy: 0.9766 - val_loss: 0.1516 - val_accuracy: 0.9850\n",
                    "Epoch 306/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1956 - accuracy: 0.9654 - val_loss: 0.1902 - val_accuracy: 0.9813\n",
                    "Epoch 307/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1770 - accuracy: 0.9696 - val_loss: 0.1878 - val_accuracy: 0.9831\n",
                    "Epoch 308/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1595 - accuracy: 0.9760 - val_loss: 0.1378 - val_accuracy: 0.9869\n",
                    "Epoch 309/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1338 - accuracy: 0.9787 - val_loss: 0.1583 - val_accuracy: 0.9775\n",
                    "Epoch 310/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1376 - accuracy: 0.9766 - val_loss: 0.1848 - val_accuracy: 0.9794\n",
                    "Epoch 311/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1195 - accuracy: 0.9794 - val_loss: 0.1612 - val_accuracy: 0.9757\n",
                    "Epoch 312/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1250 - accuracy: 0.9756 - val_loss: 0.1901 - val_accuracy: 0.9813\n",
                    "Epoch 313/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1472 - accuracy: 0.9766 - val_loss: 0.1500 - val_accuracy: 0.9813\n",
                    "Epoch 314/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1557 - accuracy: 0.9741 - val_loss: 0.1470 - val_accuracy: 0.9850\n",
                    "Epoch 315/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1505 - accuracy: 0.9754 - val_loss: 0.1500 - val_accuracy: 0.9850\n",
                    "Epoch 316/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1533 - accuracy: 0.9721 - val_loss: 0.2146 - val_accuracy: 0.9775\n",
                    "Epoch 317/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1703 - accuracy: 0.9728 - val_loss: 0.1947 - val_accuracy: 0.9813\n",
                    "Epoch 318/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1660 - accuracy: 0.9744 - val_loss: 0.1710 - val_accuracy: 0.9813\n",
                    "Epoch 319/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1462 - accuracy: 0.9760 - val_loss: 0.1713 - val_accuracy: 0.9757\n",
                    "Epoch 320/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1286 - accuracy: 0.9798 - val_loss: 0.1741 - val_accuracy: 0.9775\n",
                    "Epoch 321/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1759 - accuracy: 0.9705 - val_loss: 0.1918 - val_accuracy: 0.9775\n",
                    "Epoch 322/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1365 - accuracy: 0.9775 - val_loss: 0.1644 - val_accuracy: 0.9831\n",
                    "Epoch 323/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1580 - accuracy: 0.9705 - val_loss: 0.1563 - val_accuracy: 0.9794\n",
                    "Epoch 324/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1472 - accuracy: 0.9752 - val_loss: 0.1814 - val_accuracy: 0.9775\n",
                    "Epoch 325/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1326 - accuracy: 0.9759 - val_loss: 0.1845 - val_accuracy: 0.9757\n",
                    "Epoch 326/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1409 - accuracy: 0.9760 - val_loss: 0.1760 - val_accuracy: 0.9813\n",
                    "Epoch 327/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1271 - accuracy: 0.9790 - val_loss: 0.1548 - val_accuracy: 0.9831\n",
                    "Epoch 328/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1326 - accuracy: 0.9768 - val_loss: 0.1658 - val_accuracy: 0.9757\n",
                    "Epoch 329/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1377 - accuracy: 0.9781 - val_loss: 0.1452 - val_accuracy: 0.9831\n",
                    "Epoch 330/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1364 - accuracy: 0.9763 - val_loss: 0.1757 - val_accuracy: 0.9794\n",
                    "Epoch 331/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1254 - accuracy: 0.9773 - val_loss: 0.1753 - val_accuracy: 0.9794\n",
                    "Epoch 332/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1379 - accuracy: 0.9762 - val_loss: 0.2012 - val_accuracy: 0.9757\n",
                    "Epoch 333/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1587 - accuracy: 0.9724 - val_loss: 0.1720 - val_accuracy: 0.9813\n",
                    "Epoch 334/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1541 - accuracy: 0.9744 - val_loss: 0.1943 - val_accuracy: 0.9719\n",
                    "Epoch 335/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1400 - accuracy: 0.9768 - val_loss: 0.1529 - val_accuracy: 0.9813\n",
                    "Epoch 336/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1265 - accuracy: 0.9772 - val_loss: 0.1842 - val_accuracy: 0.9813\n",
                    "Epoch 337/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1582 - accuracy: 0.9709 - val_loss: 0.1665 - val_accuracy: 0.9850\n",
                    "Epoch 338/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1672 - accuracy: 0.9731 - val_loss: 0.1866 - val_accuracy: 0.9831\n",
                    "Epoch 339/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1922 - accuracy: 0.9692 - val_loss: 0.1834 - val_accuracy: 0.9831\n",
                    "Epoch 340/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1359 - accuracy: 0.9784 - val_loss: 0.1497 - val_accuracy: 0.9850\n",
                    "Epoch 341/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 0.1350 - accuracy: 0.9772 - val_loss: 0.1491 - val_accuracy: 0.9775\n",
                    "Epoch 342/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1321 - accuracy: 0.9762 - val_loss: 0.1403 - val_accuracy: 0.9850\n",
                    "Epoch 343/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1371 - accuracy: 0.9775 - val_loss: 0.1497 - val_accuracy: 0.9831\n",
                    "Epoch 344/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1333 - accuracy: 0.9768 - val_loss: 0.1479 - val_accuracy: 0.9813\n",
                    "Epoch 345/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1303 - accuracy: 0.9763 - val_loss: 0.1707 - val_accuracy: 0.9813\n",
                    "Epoch 346/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1414 - accuracy: 0.9752 - val_loss: 0.1688 - val_accuracy: 0.9775\n",
                    "Epoch 347/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1447 - accuracy: 0.9760 - val_loss: 0.1443 - val_accuracy: 0.9831\n",
                    "Epoch 348/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1133 - accuracy: 0.9810 - val_loss: 0.1473 - val_accuracy: 0.9813\n",
                    "Epoch 349/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1158 - accuracy: 0.9798 - val_loss: 0.1456 - val_accuracy: 0.9813\n",
                    "Epoch 350/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1480 - accuracy: 0.9733 - val_loss: 0.1699 - val_accuracy: 0.9775\n",
                    "Epoch 351/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1368 - accuracy: 0.9772 - val_loss: 0.1816 - val_accuracy: 0.9794\n",
                    "Epoch 352/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1644 - accuracy: 0.9741 - val_loss: 0.1855 - val_accuracy: 0.9794\n",
                    "Epoch 353/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1670 - accuracy: 0.9753 - val_loss: 0.1997 - val_accuracy: 0.9775\n",
                    "Epoch 354/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1398 - accuracy: 0.9765 - val_loss: 0.1661 - val_accuracy: 0.9775\n",
                    "Epoch 355/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1698 - accuracy: 0.9725 - val_loss: 0.1696 - val_accuracy: 0.9794\n",
                    "Epoch 356/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1325 - accuracy: 0.9797 - val_loss: 0.1546 - val_accuracy: 0.9850\n",
                    "Epoch 357/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1365 - accuracy: 0.9773 - val_loss: 0.1523 - val_accuracy: 0.9850\n",
                    "Epoch 358/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1288 - accuracy: 0.9787 - val_loss: 0.1789 - val_accuracy: 0.9813\n",
                    "Epoch 359/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1344 - accuracy: 0.9776 - val_loss: 0.1640 - val_accuracy: 0.9794\n",
                    "Epoch 360/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1239 - accuracy: 0.9772 - val_loss: 0.1485 - val_accuracy: 0.9831\n",
                    "Epoch 361/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1147 - accuracy: 0.9797 - val_loss: 0.1509 - val_accuracy: 0.9850\n",
                    "Epoch 362/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1588 - accuracy: 0.9734 - val_loss: 0.2125 - val_accuracy: 0.9794\n",
                    "Epoch 363/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1576 - accuracy: 0.9750 - val_loss: 0.1996 - val_accuracy: 0.9794\n",
                    "Epoch 364/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1286 - accuracy: 0.9765 - val_loss: 0.1642 - val_accuracy: 0.9813\n",
                    "Epoch 365/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1359 - accuracy: 0.9766 - val_loss: 0.1972 - val_accuracy: 0.9794\n",
                    "Epoch 366/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1202 - accuracy: 0.9795 - val_loss: 0.1658 - val_accuracy: 0.9831\n",
                    "Epoch 367/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 0.1436 - accuracy: 0.9737 - val_loss: 0.1947 - val_accuracy: 0.9813\n",
                    "Epoch 368/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1474 - accuracy: 0.9760 - val_loss: 0.2123 - val_accuracy: 0.9775\n",
                    "Epoch 369/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1393 - accuracy: 0.9771 - val_loss: 0.1582 - val_accuracy: 0.9850\n",
                    "Epoch 370/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 0.1174 - accuracy: 0.9814 - val_loss: 0.1575 - val_accuracy: 0.9831\n",
                    "Epoch 371/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1274 - accuracy: 0.9771 - val_loss: 0.1626 - val_accuracy: 0.9831\n",
                    "Epoch 372/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1413 - accuracy: 0.9752 - val_loss: 0.1715 - val_accuracy: 0.9831\n",
                    "Epoch 373/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1330 - accuracy: 0.9800 - val_loss: 0.1822 - val_accuracy: 0.9757\n",
                    "Epoch 374/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1195 - accuracy: 0.9778 - val_loss: 0.1455 - val_accuracy: 0.9850\n",
                    "Epoch 375/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1372 - accuracy: 0.9765 - val_loss: 0.1548 - val_accuracy: 0.9794\n",
                    "Epoch 376/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1208 - accuracy: 0.9782 - val_loss: 0.1355 - val_accuracy: 0.9813\n",
                    "Epoch 377/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1271 - accuracy: 0.9765 - val_loss: 0.1611 - val_accuracy: 0.9813\n",
                    "Epoch 378/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1530 - accuracy: 0.9728 - val_loss: 0.1593 - val_accuracy: 0.9813\n",
                    "Epoch 379/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1173 - accuracy: 0.9809 - val_loss: 0.1595 - val_accuracy: 0.9831\n",
                    "Epoch 380/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1128 - accuracy: 0.9797 - val_loss: 0.1403 - val_accuracy: 0.9850\n",
                    "Epoch 381/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1163 - accuracy: 0.9797 - val_loss: 0.1885 - val_accuracy: 0.9794\n",
                    "Epoch 382/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1453 - accuracy: 0.9757 - val_loss: 0.1708 - val_accuracy: 0.9850\n",
                    "Epoch 383/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1449 - accuracy: 0.9746 - val_loss: 0.1662 - val_accuracy: 0.9831\n",
                    "Epoch 384/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1181 - accuracy: 0.9794 - val_loss: 0.1892 - val_accuracy: 0.9738\n",
                    "Epoch 385/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1332 - accuracy: 0.9762 - val_loss: 0.1439 - val_accuracy: 0.9850\n",
                    "Epoch 386/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 0.1684 - accuracy: 0.9714 - val_loss: 0.2018 - val_accuracy: 0.9794\n",
                    "Epoch 387/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1401 - accuracy: 0.9776 - val_loss: 0.1633 - val_accuracy: 0.9850\n",
                    "Epoch 388/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1361 - accuracy: 0.9775 - val_loss: 0.1636 - val_accuracy: 0.9850\n",
                    "Epoch 389/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1247 - accuracy: 0.9778 - val_loss: 0.1722 - val_accuracy: 0.9813\n",
                    "Epoch 390/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1226 - accuracy: 0.9791 - val_loss: 0.1752 - val_accuracy: 0.9831\n",
                    "Epoch 391/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1491 - accuracy: 0.9753 - val_loss: 0.1663 - val_accuracy: 0.9850\n",
                    "Epoch 392/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1151 - accuracy: 0.9819 - val_loss: 0.1572 - val_accuracy: 0.9813\n",
                    "Epoch 393/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1162 - accuracy: 0.9784 - val_loss: 0.1750 - val_accuracy: 0.9813\n",
                    "Epoch 394/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1188 - accuracy: 0.9790 - val_loss: 0.1612 - val_accuracy: 0.9794\n",
                    "Epoch 395/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1298 - accuracy: 0.9762 - val_loss: 0.1667 - val_accuracy: 0.9794\n",
                    "Epoch 396/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1322 - accuracy: 0.9788 - val_loss: 0.1668 - val_accuracy: 0.9813\n",
                    "Epoch 397/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1288 - accuracy: 0.9769 - val_loss: 0.1718 - val_accuracy: 0.9813\n",
                    "Epoch 398/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1308 - accuracy: 0.9784 - val_loss: 0.1533 - val_accuracy: 0.9831\n",
                    "Epoch 399/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1253 - accuracy: 0.9788 - val_loss: 0.1711 - val_accuracy: 0.9794\n",
                    "Epoch 400/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1145 - accuracy: 0.9814 - val_loss: 0.1521 - val_accuracy: 0.9831\n",
                    "Epoch 401/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1018 - accuracy: 0.9804 - val_loss: 0.1892 - val_accuracy: 0.9794\n",
                    "Epoch 402/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1553 - accuracy: 0.9722 - val_loss: 0.1759 - val_accuracy: 0.9813\n",
                    "Epoch 403/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1204 - accuracy: 0.9798 - val_loss: 0.1906 - val_accuracy: 0.9813\n",
                    "Epoch 404/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1332 - accuracy: 0.9757 - val_loss: 0.1646 - val_accuracy: 0.9813\n",
                    "Epoch 405/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1074 - accuracy: 0.9828 - val_loss: 0.1431 - val_accuracy: 0.9831\n",
                    "Epoch 406/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1087 - accuracy: 0.9804 - val_loss: 0.1461 - val_accuracy: 0.9813\n",
                    "Epoch 407/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1344 - accuracy: 0.9763 - val_loss: 0.1582 - val_accuracy: 0.9813\n",
                    "Epoch 408/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1148 - accuracy: 0.9807 - val_loss: 0.1430 - val_accuracy: 0.9850\n",
                    "Epoch 409/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1040 - accuracy: 0.9806 - val_loss: 0.1461 - val_accuracy: 0.9813\n",
                    "Epoch 410/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1060 - accuracy: 0.9811 - val_loss: 0.1232 - val_accuracy: 0.9831\n",
                    "Epoch 411/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1269 - accuracy: 0.9772 - val_loss: 0.1581 - val_accuracy: 0.9831\n",
                    "Epoch 412/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1280 - accuracy: 0.9792 - val_loss: 0.1465 - val_accuracy: 0.9831\n",
                    "Epoch 413/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1231 - accuracy: 0.9788 - val_loss: 0.1385 - val_accuracy: 0.9831\n",
                    "Epoch 414/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1299 - accuracy: 0.9766 - val_loss: 0.1691 - val_accuracy: 0.9831\n",
                    "Epoch 415/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1335 - accuracy: 0.9747 - val_loss: 0.1770 - val_accuracy: 0.9813\n",
                    "Epoch 416/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1170 - accuracy: 0.9791 - val_loss: 0.1387 - val_accuracy: 0.9850\n",
                    "Epoch 417/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1097 - accuracy: 0.9804 - val_loss: 0.1432 - val_accuracy: 0.9813\n",
                    "Epoch 418/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1426 - accuracy: 0.9743 - val_loss: 0.1889 - val_accuracy: 0.9813\n",
                    "Epoch 419/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1308 - accuracy: 0.9797 - val_loss: 0.1603 - val_accuracy: 0.9831\n",
                    "Epoch 420/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1182 - accuracy: 0.9797 - val_loss: 0.1644 - val_accuracy: 0.9831\n",
                    "Epoch 421/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1384 - accuracy: 0.9754 - val_loss: 0.2270 - val_accuracy: 0.9794\n",
                    "Epoch 422/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1560 - accuracy: 0.9771 - val_loss: 0.1775 - val_accuracy: 0.9831\n",
                    "Epoch 423/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1089 - accuracy: 0.9832 - val_loss: 0.1336 - val_accuracy: 0.9869\n",
                    "Epoch 424/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1138 - accuracy: 0.9788 - val_loss: 0.1554 - val_accuracy: 0.9831\n",
                    "Epoch 425/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1208 - accuracy: 0.9779 - val_loss: 0.1701 - val_accuracy: 0.9831\n",
                    "Epoch 426/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1205 - accuracy: 0.9784 - val_loss: 0.1542 - val_accuracy: 0.9794\n",
                    "Epoch 427/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1086 - accuracy: 0.9826 - val_loss: 0.1366 - val_accuracy: 0.9850\n",
                    "Epoch 428/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1136 - accuracy: 0.9810 - val_loss: 0.1554 - val_accuracy: 0.9813\n",
                    "Epoch 429/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1303 - accuracy: 0.9769 - val_loss: 0.1694 - val_accuracy: 0.9869\n",
                    "Epoch 430/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1251 - accuracy: 0.9791 - val_loss: 0.1606 - val_accuracy: 0.9831\n",
                    "Epoch 431/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1104 - accuracy: 0.9804 - val_loss: 0.1341 - val_accuracy: 0.9850\n",
                    "Epoch 432/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1145 - accuracy: 0.9797 - val_loss: 0.1840 - val_accuracy: 0.9775\n",
                    "Epoch 433/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1363 - accuracy: 0.9753 - val_loss: 0.1856 - val_accuracy: 0.9813\n",
                    "Epoch 434/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1311 - accuracy: 0.9773 - val_loss: 0.1487 - val_accuracy: 0.9850\n",
                    "Epoch 435/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1050 - accuracy: 0.9826 - val_loss: 0.1379 - val_accuracy: 0.9831\n",
                    "Epoch 436/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1212 - accuracy: 0.9797 - val_loss: 0.1744 - val_accuracy: 0.9869\n",
                    "Epoch 437/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1276 - accuracy: 0.9787 - val_loss: 0.2148 - val_accuracy: 0.9757\n",
                    "Epoch 438/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1169 - accuracy: 0.9823 - val_loss: 0.1515 - val_accuracy: 0.9831\n",
                    "Epoch 439/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1277 - accuracy: 0.9763 - val_loss: 0.1581 - val_accuracy: 0.9794\n",
                    "Epoch 440/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1333 - accuracy: 0.9776 - val_loss: 0.1477 - val_accuracy: 0.9869\n",
                    "Epoch 441/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1098 - accuracy: 0.9816 - val_loss: 0.1445 - val_accuracy: 0.9850\n",
                    "Epoch 442/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1113 - accuracy: 0.9801 - val_loss: 0.1492 - val_accuracy: 0.9850\n",
                    "Epoch 443/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1392 - accuracy: 0.9772 - val_loss: 0.1566 - val_accuracy: 0.9813\n",
                    "Epoch 444/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1612 - accuracy: 0.9714 - val_loss: 0.1795 - val_accuracy: 0.9850\n",
                    "Epoch 445/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1177 - accuracy: 0.9825 - val_loss: 0.1559 - val_accuracy: 0.9831\n",
                    "Epoch 446/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1221 - accuracy: 0.9795 - val_loss: 0.1993 - val_accuracy: 0.9831\n",
                    "Epoch 447/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1216 - accuracy: 0.9800 - val_loss: 0.1609 - val_accuracy: 0.9831\n",
                    "Epoch 448/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1017 - accuracy: 0.9828 - val_loss: 0.1466 - val_accuracy: 0.9831\n",
                    "Epoch 449/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1262 - accuracy: 0.9779 - val_loss: 0.2062 - val_accuracy: 0.9738\n",
                    "Epoch 450/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1275 - accuracy: 0.9778 - val_loss: 0.1819 - val_accuracy: 0.9794\n",
                    "Epoch 451/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1064 - accuracy: 0.9813 - val_loss: 0.1681 - val_accuracy: 0.9813\n",
                    "Epoch 452/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1052 - accuracy: 0.9811 - val_loss: 0.1824 - val_accuracy: 0.9775\n",
                    "Epoch 453/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1014 - accuracy: 0.9826 - val_loss: 0.1484 - val_accuracy: 0.9850\n",
                    "Epoch 454/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1327 - accuracy: 0.9778 - val_loss: 0.1552 - val_accuracy: 0.9850\n",
                    "Epoch 455/1000\n",
                    "274/274 [==============================] - 22s 80ms/step - loss: 0.1198 - accuracy: 0.9792 - val_loss: 0.1698 - val_accuracy: 0.9813\n",
                    "Epoch 456/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1194 - accuracy: 0.9784 - val_loss: 0.1660 - val_accuracy: 0.9794\n",
                    "Epoch 457/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1014 - accuracy: 0.9806 - val_loss: 0.1553 - val_accuracy: 0.9813\n",
                    "Epoch 458/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.0992 - accuracy: 0.9829 - val_loss: 0.1411 - val_accuracy: 0.9831\n",
                    "Epoch 459/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1120 - accuracy: 0.9797 - val_loss: 0.1475 - val_accuracy: 0.9850\n",
                    "Epoch 460/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1095 - accuracy: 0.9806 - val_loss: 0.1653 - val_accuracy: 0.9813\n",
                    "Epoch 461/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1262 - accuracy: 0.9773 - val_loss: 0.1573 - val_accuracy: 0.9831\n",
                    "Epoch 462/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1190 - accuracy: 0.9800 - val_loss: 0.1660 - val_accuracy: 0.9813\n",
                    "Epoch 463/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1297 - accuracy: 0.9776 - val_loss: 0.1719 - val_accuracy: 0.9813\n",
                    "Epoch 464/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1212 - accuracy: 0.9790 - val_loss: 0.1670 - val_accuracy: 0.9775\n",
                    "Epoch 465/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1085 - accuracy: 0.9823 - val_loss: 0.1481 - val_accuracy: 0.9850\n",
                    "Epoch 466/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0960 - accuracy: 0.9820 - val_loss: 0.1664 - val_accuracy: 0.9794\n",
                    "Epoch 467/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1312 - accuracy: 0.9797 - val_loss: 0.1467 - val_accuracy: 0.9850\n",
                    "Epoch 468/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1115 - accuracy: 0.9795 - val_loss: 0.1474 - val_accuracy: 0.9850\n",
                    "Epoch 469/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1009 - accuracy: 0.9810 - val_loss: 0.1568 - val_accuracy: 0.9831\n",
                    "Epoch 470/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1077 - accuracy: 0.9813 - val_loss: 0.1357 - val_accuracy: 0.9831\n",
                    "Epoch 471/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1021 - accuracy: 0.9801 - val_loss: 0.1529 - val_accuracy: 0.9813\n",
                    "Epoch 472/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1070 - accuracy: 0.9809 - val_loss: 0.1531 - val_accuracy: 0.9794\n",
                    "Epoch 473/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1194 - accuracy: 0.9791 - val_loss: 0.1687 - val_accuracy: 0.9813\n",
                    "Epoch 474/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1055 - accuracy: 0.9803 - val_loss: 0.1817 - val_accuracy: 0.9813\n",
                    "Epoch 475/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1056 - accuracy: 0.9816 - val_loss: 0.1563 - val_accuracy: 0.9813\n",
                    "Epoch 476/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1039 - accuracy: 0.9801 - val_loss: 0.1708 - val_accuracy: 0.9813\n",
                    "Epoch 477/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1205 - accuracy: 0.9801 - val_loss: 0.1548 - val_accuracy: 0.9869\n",
                    "Epoch 478/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.0945 - accuracy: 0.9828 - val_loss: 0.1421 - val_accuracy: 0.9831\n",
                    "Epoch 479/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1308 - accuracy: 0.9766 - val_loss: 0.2019 - val_accuracy: 0.9813\n",
                    "Epoch 480/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1236 - accuracy: 0.9813 - val_loss: 0.1668 - val_accuracy: 0.9775\n",
                    "Epoch 481/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1048 - accuracy: 0.9835 - val_loss: 0.1507 - val_accuracy: 0.9813\n",
                    "Epoch 482/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1084 - accuracy: 0.9807 - val_loss: 0.1457 - val_accuracy: 0.9794\n",
                    "Epoch 483/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1140 - accuracy: 0.9800 - val_loss: 0.1576 - val_accuracy: 0.9831\n",
                    "Epoch 484/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1093 - accuracy: 0.9825 - val_loss: 0.1544 - val_accuracy: 0.9831\n",
                    "Epoch 485/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1039 - accuracy: 0.9826 - val_loss: 0.1511 - val_accuracy: 0.9831\n",
                    "Epoch 486/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1034 - accuracy: 0.9822 - val_loss: 0.1766 - val_accuracy: 0.9775\n",
                    "Epoch 487/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1205 - accuracy: 0.9768 - val_loss: 0.1631 - val_accuracy: 0.9831\n",
                    "Epoch 488/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1043 - accuracy: 0.9830 - val_loss: 0.1576 - val_accuracy: 0.9831\n",
                    "Epoch 489/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1180 - accuracy: 0.9781 - val_loss: 0.1763 - val_accuracy: 0.9813\n",
                    "Epoch 490/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1322 - accuracy: 0.9768 - val_loss: 0.1567 - val_accuracy: 0.9831\n",
                    "Epoch 491/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1245 - accuracy: 0.9787 - val_loss: 0.1759 - val_accuracy: 0.9794\n",
                    "Epoch 492/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1018 - accuracy: 0.9820 - val_loss: 0.1575 - val_accuracy: 0.9775\n",
                    "Epoch 493/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.0926 - accuracy: 0.9830 - val_loss: 0.1501 - val_accuracy: 0.9831\n",
                    "Epoch 494/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1246 - accuracy: 0.9788 - val_loss: 0.1681 - val_accuracy: 0.9813\n",
                    "Epoch 495/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1082 - accuracy: 0.9822 - val_loss: 0.1454 - val_accuracy: 0.9794\n",
                    "Epoch 496/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0920 - accuracy: 0.9844 - val_loss: 0.1298 - val_accuracy: 0.9850\n",
                    "Epoch 497/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1221 - accuracy: 0.9785 - val_loss: 0.2864 - val_accuracy: 0.9700\n",
                    "Epoch 498/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1160 - accuracy: 0.9809 - val_loss: 0.1595 - val_accuracy: 0.9831\n",
                    "Epoch 499/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1007 - accuracy: 0.9810 - val_loss: 0.1417 - val_accuracy: 0.9831\n",
                    "Epoch 500/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1020 - accuracy: 0.9813 - val_loss: 0.1557 - val_accuracy: 0.9813\n",
                    "Epoch 501/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1102 - accuracy: 0.9811 - val_loss: 0.1761 - val_accuracy: 0.9813\n",
                    "Epoch 502/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1009 - accuracy: 0.9832 - val_loss: 0.1269 - val_accuracy: 0.9869\n",
                    "Epoch 503/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0985 - accuracy: 0.9807 - val_loss: 0.1489 - val_accuracy: 0.9813\n",
                    "Epoch 504/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 0.1228 - accuracy: 0.9779 - val_loss: 0.1694 - val_accuracy: 0.9794\n",
                    "Epoch 505/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1139 - accuracy: 0.9806 - val_loss: 0.1559 - val_accuracy: 0.9888\n",
                    "Epoch 506/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0906 - accuracy: 0.9851 - val_loss: 0.1519 - val_accuracy: 0.9831\n",
                    "Epoch 507/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.0921 - accuracy: 0.9828 - val_loss: 0.1265 - val_accuracy: 0.9813\n",
                    "Epoch 508/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1156 - accuracy: 0.9778 - val_loss: 0.1670 - val_accuracy: 0.9813\n",
                    "Epoch 509/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1165 - accuracy: 0.9788 - val_loss: 0.1807 - val_accuracy: 0.9813\n",
                    "Epoch 510/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0981 - accuracy: 0.9848 - val_loss: 0.1581 - val_accuracy: 0.9813\n",
                    "Epoch 511/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1045 - accuracy: 0.9819 - val_loss: 0.1791 - val_accuracy: 0.9831\n",
                    "Epoch 512/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1119 - accuracy: 0.9804 - val_loss: 0.1665 - val_accuracy: 0.9794\n",
                    "Epoch 513/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1089 - accuracy: 0.9800 - val_loss: 0.1553 - val_accuracy: 0.9794\n",
                    "Epoch 514/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1120 - accuracy: 0.9798 - val_loss: 0.1561 - val_accuracy: 0.9831\n",
                    "Epoch 515/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1384 - accuracy: 0.9779 - val_loss: 0.1736 - val_accuracy: 0.9794\n",
                    "Epoch 516/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0983 - accuracy: 0.9819 - val_loss: 0.1478 - val_accuracy: 0.9813\n",
                    "Epoch 517/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1147 - accuracy: 0.9784 - val_loss: 0.1591 - val_accuracy: 0.9831\n",
                    "Epoch 518/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1042 - accuracy: 0.9816 - val_loss: 0.1712 - val_accuracy: 0.9813\n",
                    "Epoch 519/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1175 - accuracy: 0.9803 - val_loss: 0.1628 - val_accuracy: 0.9794\n",
                    "Epoch 520/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0942 - accuracy: 0.9835 - val_loss: 0.1492 - val_accuracy: 0.9794\n",
                    "Epoch 521/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1071 - accuracy: 0.9797 - val_loss: 0.1588 - val_accuracy: 0.9775\n",
                    "Epoch 522/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0971 - accuracy: 0.9813 - val_loss: 0.1465 - val_accuracy: 0.9850\n",
                    "Epoch 523/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1009 - accuracy: 0.9814 - val_loss: 0.1524 - val_accuracy: 0.9813\n",
                    "Epoch 524/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0988 - accuracy: 0.9825 - val_loss: 0.1575 - val_accuracy: 0.9850\n",
                    "Epoch 525/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0914 - accuracy: 0.9842 - val_loss: 0.1494 - val_accuracy: 0.9794\n",
                    "Epoch 526/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1543 - accuracy: 0.9724 - val_loss: 0.2064 - val_accuracy: 0.9813\n",
                    "Epoch 527/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.1105 - accuracy: 0.9839 - val_loss: 0.1740 - val_accuracy: 0.9794\n",
                    "Epoch 528/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.1134 - accuracy: 0.9810 - val_loss: 0.1772 - val_accuracy: 0.9794\n",
                    "Epoch 529/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1081 - accuracy: 0.9816 - val_loss: 0.1605 - val_accuracy: 0.9813\n",
                    "Epoch 530/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0868 - accuracy: 0.9852 - val_loss: 0.1536 - val_accuracy: 0.9813\n",
                    "Epoch 531/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0879 - accuracy: 0.9835 - val_loss: 0.1582 - val_accuracy: 0.9794\n",
                    "Epoch 532/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.0949 - accuracy: 0.9839 - val_loss: 0.1598 - val_accuracy: 0.9757\n",
                    "Epoch 533/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1020 - accuracy: 0.9814 - val_loss: 0.1424 - val_accuracy: 0.9813\n",
                    "Epoch 534/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1062 - accuracy: 0.9803 - val_loss: 0.1637 - val_accuracy: 0.9813\n",
                    "Epoch 535/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0891 - accuracy: 0.9839 - val_loss: 0.1485 - val_accuracy: 0.9794\n",
                    "Epoch 536/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1175 - accuracy: 0.9801 - val_loss: 0.1837 - val_accuracy: 0.9831\n",
                    "Epoch 537/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0980 - accuracy: 0.9830 - val_loss: 0.1729 - val_accuracy: 0.9775\n",
                    "Epoch 538/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.1008 - accuracy: 0.9828 - val_loss: 0.1655 - val_accuracy: 0.9813\n",
                    "Epoch 539/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1141 - accuracy: 0.9813 - val_loss: 0.1435 - val_accuracy: 0.9831\n",
                    "Epoch 540/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1077 - accuracy: 0.9804 - val_loss: 0.1673 - val_accuracy: 0.9794\n",
                    "Epoch 541/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1288 - accuracy: 0.9778 - val_loss: 0.1710 - val_accuracy: 0.9831\n",
                    "Epoch 542/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1016 - accuracy: 0.9820 - val_loss: 0.1547 - val_accuracy: 0.9813\n",
                    "Epoch 543/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1391 - accuracy: 0.9762 - val_loss: 0.1605 - val_accuracy: 0.9813\n",
                    "Epoch 544/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0897 - accuracy: 0.9852 - val_loss: 0.1562 - val_accuracy: 0.9831\n",
                    "Epoch 545/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1108 - accuracy: 0.9797 - val_loss: 0.1674 - val_accuracy: 0.9831\n",
                    "Epoch 546/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0925 - accuracy: 0.9828 - val_loss: 0.1488 - val_accuracy: 0.9813\n",
                    "Epoch 547/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0954 - accuracy: 0.9811 - val_loss: 0.1618 - val_accuracy: 0.9831\n",
                    "Epoch 548/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1240 - accuracy: 0.9778 - val_loss: 0.2006 - val_accuracy: 0.9775\n",
                    "Epoch 549/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0895 - accuracy: 0.9839 - val_loss: 0.1592 - val_accuracy: 0.9850\n",
                    "Epoch 550/1000\n",
                    "274/274 [==============================] - 22s 81ms/step - loss: 0.1033 - accuracy: 0.9823 - val_loss: 0.1703 - val_accuracy: 0.9775\n",
                    "Epoch 551/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0831 - accuracy: 0.9848 - val_loss: 0.1518 - val_accuracy: 0.9794\n",
                    "Epoch 552/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0920 - accuracy: 0.9814 - val_loss: 0.1745 - val_accuracy: 0.9850\n",
                    "Epoch 553/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0841 - accuracy: 0.9836 - val_loss: 0.1595 - val_accuracy: 0.9794\n",
                    "Epoch 554/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1280 - accuracy: 0.9781 - val_loss: 0.1787 - val_accuracy: 0.9813\n",
                    "Epoch 555/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0913 - accuracy: 0.9848 - val_loss: 0.1664 - val_accuracy: 0.9813\n",
                    "Epoch 556/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1187 - accuracy: 0.9792 - val_loss: 0.1657 - val_accuracy: 0.9831\n",
                    "Epoch 557/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1011 - accuracy: 0.9810 - val_loss: 0.1608 - val_accuracy: 0.9850\n",
                    "Epoch 558/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0889 - accuracy: 0.9849 - val_loss: 0.1593 - val_accuracy: 0.9831\n",
                    "Epoch 559/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0893 - accuracy: 0.9825 - val_loss: 0.1839 - val_accuracy: 0.9794\n",
                    "Epoch 560/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1046 - accuracy: 0.9814 - val_loss: 0.1881 - val_accuracy: 0.9775\n",
                    "Epoch 561/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1231 - accuracy: 0.9781 - val_loss: 0.1663 - val_accuracy: 0.9813\n",
                    "Epoch 562/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0971 - accuracy: 0.9826 - val_loss: 0.1575 - val_accuracy: 0.9850\n",
                    "Epoch 563/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0966 - accuracy: 0.9819 - val_loss: 0.1296 - val_accuracy: 0.9831\n",
                    "Epoch 564/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0998 - accuracy: 0.9814 - val_loss: 0.1454 - val_accuracy: 0.9831\n",
                    "Epoch 565/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1118 - accuracy: 0.9794 - val_loss: 0.1820 - val_accuracy: 0.9813\n",
                    "Epoch 566/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1011 - accuracy: 0.9836 - val_loss: 0.1437 - val_accuracy: 0.9850\n",
                    "Epoch 567/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0937 - accuracy: 0.9820 - val_loss: 0.1429 - val_accuracy: 0.9831\n",
                    "Epoch 568/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1331 - accuracy: 0.9768 - val_loss: 0.2049 - val_accuracy: 0.9850\n",
                    "Epoch 569/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1226 - accuracy: 0.9810 - val_loss: 0.1651 - val_accuracy: 0.9850\n",
                    "Epoch 570/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0946 - accuracy: 0.9830 - val_loss: 0.1509 - val_accuracy: 0.9850\n",
                    "Epoch 571/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0900 - accuracy: 0.9839 - val_loss: 0.1497 - val_accuracy: 0.9813\n",
                    "Epoch 572/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0809 - accuracy: 0.9854 - val_loss: 0.1405 - val_accuracy: 0.9794\n",
                    "Epoch 573/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1007 - accuracy: 0.9809 - val_loss: 0.1554 - val_accuracy: 0.9813\n",
                    "Epoch 574/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1145 - accuracy: 0.9801 - val_loss: 0.1709 - val_accuracy: 0.9813\n",
                    "Epoch 575/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.0958 - accuracy: 0.9836 - val_loss: 0.1854 - val_accuracy: 0.9813\n",
                    "Epoch 576/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0858 - accuracy: 0.9839 - val_loss: 0.1652 - val_accuracy: 0.9850\n",
                    "Epoch 577/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1076 - accuracy: 0.9804 - val_loss: 0.1609 - val_accuracy: 0.9794\n",
                    "Epoch 578/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1000 - accuracy: 0.9804 - val_loss: 0.1582 - val_accuracy: 0.9850\n",
                    "Epoch 579/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0865 - accuracy: 0.9828 - val_loss: 0.1523 - val_accuracy: 0.9794\n",
                    "Epoch 580/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1264 - accuracy: 0.9797 - val_loss: 0.1483 - val_accuracy: 0.9831\n",
                    "Epoch 581/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0798 - accuracy: 0.9855 - val_loss: 0.1510 - val_accuracy: 0.9813\n",
                    "Epoch 582/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0847 - accuracy: 0.9830 - val_loss: 0.1564 - val_accuracy: 0.9813\n",
                    "Epoch 583/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0925 - accuracy: 0.9822 - val_loss: 0.1438 - val_accuracy: 0.9850\n",
                    "Epoch 584/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0943 - accuracy: 0.9829 - val_loss: 0.1552 - val_accuracy: 0.9757\n",
                    "Epoch 585/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0850 - accuracy: 0.9829 - val_loss: 0.1339 - val_accuracy: 0.9831\n",
                    "Epoch 586/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0939 - accuracy: 0.9830 - val_loss: 0.1364 - val_accuracy: 0.9794\n",
                    "Epoch 587/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0884 - accuracy: 0.9822 - val_loss: 0.1338 - val_accuracy: 0.9850\n",
                    "Epoch 588/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0869 - accuracy: 0.9830 - val_loss: 0.1507 - val_accuracy: 0.9757\n",
                    "Epoch 589/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1129 - accuracy: 0.9798 - val_loss: 0.1611 - val_accuracy: 0.9831\n",
                    "Epoch 590/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1017 - accuracy: 0.9836 - val_loss: 0.1393 - val_accuracy: 0.9831\n",
                    "Epoch 591/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1185 - accuracy: 0.9785 - val_loss: 0.1526 - val_accuracy: 0.9794\n",
                    "Epoch 592/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1088 - accuracy: 0.9825 - val_loss: 0.1466 - val_accuracy: 0.9850\n",
                    "Epoch 593/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0849 - accuracy: 0.9841 - val_loss: 0.1495 - val_accuracy: 0.9831\n",
                    "Epoch 594/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1021 - accuracy: 0.9811 - val_loss: 0.1653 - val_accuracy: 0.9850\n",
                    "Epoch 595/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1089 - accuracy: 0.9806 - val_loss: 0.1519 - val_accuracy: 0.9831\n",
                    "Epoch 596/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0937 - accuracy: 0.9829 - val_loss: 0.1545 - val_accuracy: 0.9850\n",
                    "Epoch 597/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0891 - accuracy: 0.9848 - val_loss: 0.1467 - val_accuracy: 0.9850\n",
                    "Epoch 598/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1089 - accuracy: 0.9811 - val_loss: 0.1513 - val_accuracy: 0.9831\n",
                    "Epoch 599/1000\n",
                    "274/274 [==============================] - 22s 82ms/step - loss: 0.0953 - accuracy: 0.9816 - val_loss: 0.1627 - val_accuracy: 0.9831\n",
                    "Epoch 600/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1205 - accuracy: 0.9804 - val_loss: 0.1755 - val_accuracy: 0.9831\n",
                    "Epoch 601/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0905 - accuracy: 0.9836 - val_loss: 0.1574 - val_accuracy: 0.9775\n",
                    "Epoch 602/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.0802 - accuracy: 0.9841 - val_loss: 0.1536 - val_accuracy: 0.9813\n",
                    "Epoch 603/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1087 - accuracy: 0.9807 - val_loss: 0.1738 - val_accuracy: 0.9813\n",
                    "Epoch 604/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1042 - accuracy: 0.9829 - val_loss: 0.1650 - val_accuracy: 0.9831\n",
                    "Epoch 605/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0799 - accuracy: 0.9851 - val_loss: 0.1344 - val_accuracy: 0.9869\n",
                    "Epoch 606/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1077 - accuracy: 0.9804 - val_loss: 0.1879 - val_accuracy: 0.9794\n",
                    "Epoch 607/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1137 - accuracy: 0.9809 - val_loss: 0.1743 - val_accuracy: 0.9831\n",
                    "Epoch 608/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0893 - accuracy: 0.9826 - val_loss: 0.1553 - val_accuracy: 0.9850\n",
                    "Epoch 609/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0939 - accuracy: 0.9825 - val_loss: 0.1560 - val_accuracy: 0.9831\n",
                    "Epoch 610/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0882 - accuracy: 0.9839 - val_loss: 0.1642 - val_accuracy: 0.9813\n",
                    "Epoch 611/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0891 - accuracy: 0.9830 - val_loss: 0.1537 - val_accuracy: 0.9831\n",
                    "Epoch 612/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1093 - accuracy: 0.9811 - val_loss: 0.1742 - val_accuracy: 0.9794\n",
                    "Epoch 613/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1045 - accuracy: 0.9829 - val_loss: 0.1519 - val_accuracy: 0.9850\n",
                    "Epoch 614/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.0889 - accuracy: 0.9820 - val_loss: 0.1438 - val_accuracy: 0.9831\n",
                    "Epoch 615/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0982 - accuracy: 0.9813 - val_loss: 0.1560 - val_accuracy: 0.9813\n",
                    "Epoch 616/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0857 - accuracy: 0.9851 - val_loss: 0.1459 - val_accuracy: 0.9850\n",
                    "Epoch 617/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0908 - accuracy: 0.9833 - val_loss: 0.1429 - val_accuracy: 0.9813\n",
                    "Epoch 618/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0995 - accuracy: 0.9819 - val_loss: 0.1690 - val_accuracy: 0.9813\n",
                    "Epoch 619/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1228 - accuracy: 0.9791 - val_loss: 0.1417 - val_accuracy: 0.9850\n",
                    "Epoch 620/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0932 - accuracy: 0.9833 - val_loss: 0.1728 - val_accuracy: 0.9831\n",
                    "Epoch 621/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1064 - accuracy: 0.9816 - val_loss: 0.1560 - val_accuracy: 0.9794\n",
                    "Epoch 622/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0892 - accuracy: 0.9841 - val_loss: 0.1512 - val_accuracy: 0.9831\n",
                    "Epoch 623/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0838 - accuracy: 0.9847 - val_loss: 0.1507 - val_accuracy: 0.9813\n",
                    "Epoch 624/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0963 - accuracy: 0.9828 - val_loss: 0.1502 - val_accuracy: 0.9813\n",
                    "Epoch 625/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1040 - accuracy: 0.9804 - val_loss: 0.1523 - val_accuracy: 0.9850\n",
                    "Epoch 626/1000\n",
                    "274/274 [==============================] - 23s 82ms/step - loss: 0.1115 - accuracy: 0.9792 - val_loss: 0.1417 - val_accuracy: 0.9850\n",
                    "Epoch 627/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0933 - accuracy: 0.9835 - val_loss: 0.1861 - val_accuracy: 0.9831\n",
                    "Epoch 628/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1065 - accuracy: 0.9828 - val_loss: 0.1581 - val_accuracy: 0.9813\n",
                    "Epoch 629/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0893 - accuracy: 0.9851 - val_loss: 0.1543 - val_accuracy: 0.9813\n",
                    "Epoch 630/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0866 - accuracy: 0.9854 - val_loss: 0.1404 - val_accuracy: 0.9831\n",
                    "Epoch 631/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0805 - accuracy: 0.9855 - val_loss: 0.1255 - val_accuracy: 0.9869\n",
                    "Epoch 632/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0951 - accuracy: 0.9814 - val_loss: 0.1760 - val_accuracy: 0.9831\n",
                    "Epoch 633/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0963 - accuracy: 0.9826 - val_loss: 0.1468 - val_accuracy: 0.9850\n",
                    "Epoch 634/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0843 - accuracy: 0.9826 - val_loss: 0.1364 - val_accuracy: 0.9888\n",
                    "Epoch 635/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1009 - accuracy: 0.9830 - val_loss: 0.1654 - val_accuracy: 0.9813\n",
                    "Epoch 636/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0829 - accuracy: 0.9842 - val_loss: 0.1518 - val_accuracy: 0.9850\n",
                    "Epoch 637/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0855 - accuracy: 0.9832 - val_loss: 0.1556 - val_accuracy: 0.9813\n",
                    "Epoch 638/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 0.0721 - accuracy: 0.9849 - val_loss: 0.1596 - val_accuracy: 0.9813\n",
                    "Epoch 639/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1006 - accuracy: 0.9825 - val_loss: 0.1629 - val_accuracy: 0.9775\n",
                    "Epoch 640/1000\n",
                    "274/274 [==============================] - 25s 89ms/step - loss: 0.0911 - accuracy: 0.9838 - val_loss: 0.1214 - val_accuracy: 0.9850\n",
                    "Epoch 641/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1269 - accuracy: 0.9779 - val_loss: 0.1862 - val_accuracy: 0.9794\n",
                    "Epoch 642/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.0914 - accuracy: 0.9851 - val_loss: 0.1488 - val_accuracy: 0.9813\n",
                    "Epoch 643/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0770 - accuracy: 0.9848 - val_loss: 0.1692 - val_accuracy: 0.9813\n",
                    "Epoch 644/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0938 - accuracy: 0.9832 - val_loss: 0.1753 - val_accuracy: 0.9813\n",
                    "Epoch 645/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.0945 - accuracy: 0.9833 - val_loss: 0.1704 - val_accuracy: 0.9850\n",
                    "Epoch 646/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0968 - accuracy: 0.9807 - val_loss: 0.1556 - val_accuracy: 0.9813\n",
                    "Epoch 647/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.1544 - accuracy: 0.9753 - val_loss: 0.2247 - val_accuracy: 0.9757\n",
                    "Epoch 648/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1147 - accuracy: 0.9832 - val_loss: 0.1441 - val_accuracy: 0.9831\n",
                    "Epoch 649/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0826 - accuracy: 0.9854 - val_loss: 0.1452 - val_accuracy: 0.9831\n",
                    "Epoch 650/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0827 - accuracy: 0.9855 - val_loss: 0.1627 - val_accuracy: 0.9794\n",
                    "Epoch 651/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0836 - accuracy: 0.9842 - val_loss: 0.1476 - val_accuracy: 0.9850\n",
                    "Epoch 652/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0815 - accuracy: 0.9836 - val_loss: 0.1414 - val_accuracy: 0.9813\n",
                    "Epoch 653/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0773 - accuracy: 0.9845 - val_loss: 0.1349 - val_accuracy: 0.9794\n",
                    "Epoch 654/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.0988 - accuracy: 0.9819 - val_loss: 0.1887 - val_accuracy: 0.9813\n",
                    "Epoch 655/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0995 - accuracy: 0.9829 - val_loss: 0.1861 - val_accuracy: 0.9794\n",
                    "Epoch 656/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1033 - accuracy: 0.9830 - val_loss: 0.1664 - val_accuracy: 0.9775\n",
                    "Epoch 657/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0915 - accuracy: 0.9842 - val_loss: 0.1567 - val_accuracy: 0.9831\n",
                    "Epoch 658/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0787 - accuracy: 0.9841 - val_loss: 0.1363 - val_accuracy: 0.9850\n",
                    "Epoch 659/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0798 - accuracy: 0.9832 - val_loss: 0.1581 - val_accuracy: 0.9813\n",
                    "Epoch 660/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1105 - accuracy: 0.9806 - val_loss: 0.1395 - val_accuracy: 0.9850\n",
                    "Epoch 661/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0899 - accuracy: 0.9841 - val_loss: 0.1488 - val_accuracy: 0.9850\n",
                    "Epoch 662/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0853 - accuracy: 0.9845 - val_loss: 0.1540 - val_accuracy: 0.9850\n",
                    "Epoch 663/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0907 - accuracy: 0.9822 - val_loss: 0.1667 - val_accuracy: 0.9794\n",
                    "Epoch 664/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0888 - accuracy: 0.9836 - val_loss: 0.1543 - val_accuracy: 0.9831\n",
                    "Epoch 665/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1449 - accuracy: 0.9773 - val_loss: 0.1676 - val_accuracy: 0.9813\n",
                    "Epoch 666/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1019 - accuracy: 0.9830 - val_loss: 0.1519 - val_accuracy: 0.9831\n",
                    "Epoch 667/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0917 - accuracy: 0.9833 - val_loss: 0.1543 - val_accuracy: 0.9813\n",
                    "Epoch 668/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0948 - accuracy: 0.9829 - val_loss: 0.2195 - val_accuracy: 0.9719\n",
                    "Epoch 669/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.1073 - accuracy: 0.9836 - val_loss: 0.1552 - val_accuracy: 0.9850\n",
                    "Epoch 670/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0989 - accuracy: 0.9822 - val_loss: 0.1536 - val_accuracy: 0.9850\n",
                    "Epoch 671/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0852 - accuracy: 0.9851 - val_loss: 0.1499 - val_accuracy: 0.9850\n",
                    "Epoch 672/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0828 - accuracy: 0.9849 - val_loss: 0.1490 - val_accuracy: 0.9831\n",
                    "Epoch 673/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0738 - accuracy: 0.9854 - val_loss: 0.1846 - val_accuracy: 0.9757\n",
                    "Epoch 674/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0926 - accuracy: 0.9845 - val_loss: 0.1658 - val_accuracy: 0.9831\n",
                    "Epoch 675/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.1081 - accuracy: 0.9798 - val_loss: 0.1705 - val_accuracy: 0.9813\n",
                    "Epoch 676/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1034 - accuracy: 0.9820 - val_loss: 0.1566 - val_accuracy: 0.9813\n",
                    "Epoch 677/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0789 - accuracy: 0.9848 - val_loss: 0.1504 - val_accuracy: 0.9850\n",
                    "Epoch 678/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0807 - accuracy: 0.9836 - val_loss: 0.1388 - val_accuracy: 0.9831\n",
                    "Epoch 679/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0793 - accuracy: 0.9863 - val_loss: 0.1446 - val_accuracy: 0.9850\n",
                    "Epoch 680/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0918 - accuracy: 0.9830 - val_loss: 0.1524 - val_accuracy: 0.9850\n",
                    "Epoch 681/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0734 - accuracy: 0.9867 - val_loss: 0.1348 - val_accuracy: 0.9831\n",
                    "Epoch 682/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0779 - accuracy: 0.9835 - val_loss: 0.1609 - val_accuracy: 0.9813\n",
                    "Epoch 683/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.1063 - accuracy: 0.9807 - val_loss: 0.1556 - val_accuracy: 0.9869\n",
                    "Epoch 684/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0800 - accuracy: 0.9866 - val_loss: 0.1411 - val_accuracy: 0.9850\n",
                    "Epoch 685/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0889 - accuracy: 0.9823 - val_loss: 0.1865 - val_accuracy: 0.9813\n",
                    "Epoch 686/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1000 - accuracy: 0.9825 - val_loss: 0.1459 - val_accuracy: 0.9831\n",
                    "Epoch 687/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0825 - accuracy: 0.9861 - val_loss: 0.1683 - val_accuracy: 0.9794\n",
                    "Epoch 688/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0862 - accuracy: 0.9838 - val_loss: 0.1665 - val_accuracy: 0.9831\n",
                    "Epoch 689/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1046 - accuracy: 0.9823 - val_loss: 0.1638 - val_accuracy: 0.9794\n",
                    "Epoch 690/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1222 - accuracy: 0.9791 - val_loss: 0.2008 - val_accuracy: 0.9794\n",
                    "Epoch 691/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0886 - accuracy: 0.9842 - val_loss: 0.1478 - val_accuracy: 0.9831\n",
                    "Epoch 692/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0748 - accuracy: 0.9851 - val_loss: 0.1436 - val_accuracy: 0.9831\n",
                    "Epoch 693/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.0889 - accuracy: 0.9825 - val_loss: 0.1833 - val_accuracy: 0.9831\n",
                    "Epoch 694/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0970 - accuracy: 0.9828 - val_loss: 0.1609 - val_accuracy: 0.9831\n",
                    "Epoch 695/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0689 - accuracy: 0.9861 - val_loss: 0.1415 - val_accuracy: 0.9831\n",
                    "Epoch 696/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0574 - accuracy: 0.9868 - val_loss: 0.1488 - val_accuracy: 0.9813\n",
                    "Epoch 697/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1175 - accuracy: 0.9779 - val_loss: 0.1744 - val_accuracy: 0.9869\n",
                    "Epoch 698/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0798 - accuracy: 0.9863 - val_loss: 0.1448 - val_accuracy: 0.9850\n",
                    "Epoch 699/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0747 - accuracy: 0.9860 - val_loss: 0.1482 - val_accuracy: 0.9831\n",
                    "Epoch 700/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0978 - accuracy: 0.9803 - val_loss: 0.1729 - val_accuracy: 0.9794\n",
                    "Epoch 701/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1006 - accuracy: 0.9816 - val_loss: 0.1502 - val_accuracy: 0.9869\n",
                    "Epoch 702/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.0881 - accuracy: 0.9848 - val_loss: 0.1481 - val_accuracy: 0.9869\n",
                    "Epoch 703/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0896 - accuracy: 0.9849 - val_loss: 0.1427 - val_accuracy: 0.9888\n",
                    "Epoch 704/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0898 - accuracy: 0.9822 - val_loss: 0.1580 - val_accuracy: 0.9813\n",
                    "Epoch 705/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1008 - accuracy: 0.9820 - val_loss: 0.1934 - val_accuracy: 0.9813\n",
                    "Epoch 706/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0965 - accuracy: 0.9830 - val_loss: 0.1471 - val_accuracy: 0.9813\n",
                    "Epoch 707/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1111 - accuracy: 0.9825 - val_loss: 0.1549 - val_accuracy: 0.9831\n",
                    "Epoch 708/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.0907 - accuracy: 0.9833 - val_loss: 0.1446 - val_accuracy: 0.9850\n",
                    "Epoch 709/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.1146 - accuracy: 0.9810 - val_loss: 0.1741 - val_accuracy: 0.9831\n",
                    "Epoch 710/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1039 - accuracy: 0.9828 - val_loss: 0.1389 - val_accuracy: 0.9869\n",
                    "Epoch 711/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.0928 - accuracy: 0.9845 - val_loss: 0.1436 - val_accuracy: 0.9813\n",
                    "Epoch 712/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0678 - accuracy: 0.9868 - val_loss: 0.1346 - val_accuracy: 0.9850\n",
                    "Epoch 713/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0839 - accuracy: 0.9838 - val_loss: 0.1358 - val_accuracy: 0.9813\n",
                    "Epoch 714/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0933 - accuracy: 0.9825 - val_loss: 0.1347 - val_accuracy: 0.9850\n",
                    "Epoch 715/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0928 - accuracy: 0.9839 - val_loss: 0.1609 - val_accuracy: 0.9813\n",
                    "Epoch 716/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0862 - accuracy: 0.9838 - val_loss: 0.1599 - val_accuracy: 0.9813\n",
                    "Epoch 717/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0852 - accuracy: 0.9847 - val_loss: 0.1438 - val_accuracy: 0.9831\n",
                    "Epoch 718/1000\n",
                    "274/274 [==============================] - 23s 83ms/step - loss: 0.0688 - accuracy: 0.9855 - val_loss: 0.1525 - val_accuracy: 0.9813\n",
                    "Epoch 719/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1007 - accuracy: 0.9806 - val_loss: 0.1600 - val_accuracy: 0.9831\n",
                    "Epoch 720/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0957 - accuracy: 0.9819 - val_loss: 0.1523 - val_accuracy: 0.9869\n",
                    "Epoch 721/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0758 - accuracy: 0.9867 - val_loss: 0.1418 - val_accuracy: 0.9813\n",
                    "Epoch 722/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0862 - accuracy: 0.9830 - val_loss: 0.1538 - val_accuracy: 0.9813\n",
                    "Epoch 723/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0915 - accuracy: 0.9822 - val_loss: 0.1444 - val_accuracy: 0.9869\n",
                    "Epoch 724/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0821 - accuracy: 0.9855 - val_loss: 0.1480 - val_accuracy: 0.9831\n",
                    "Epoch 725/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.1047 - accuracy: 0.9800 - val_loss: 0.1641 - val_accuracy: 0.9831\n",
                    "Epoch 726/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0792 - accuracy: 0.9852 - val_loss: 0.1663 - val_accuracy: 0.9831\n",
                    "Epoch 727/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0866 - accuracy: 0.9829 - val_loss: 0.1893 - val_accuracy: 0.9794\n",
                    "Epoch 728/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.0797 - accuracy: 0.9844 - val_loss: 0.1548 - val_accuracy: 0.9813\n",
                    "Epoch 729/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0701 - accuracy: 0.9861 - val_loss: 0.1535 - val_accuracy: 0.9831\n",
                    "Epoch 730/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0802 - accuracy: 0.9848 - val_loss: 0.1835 - val_accuracy: 0.9775\n",
                    "Epoch 731/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1094 - accuracy: 0.9811 - val_loss: 0.1679 - val_accuracy: 0.9813\n",
                    "Epoch 732/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0752 - accuracy: 0.9852 - val_loss: 0.1506 - val_accuracy: 0.9831\n",
                    "Epoch 733/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0911 - accuracy: 0.9825 - val_loss: 0.1841 - val_accuracy: 0.9757\n",
                    "Epoch 734/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0928 - accuracy: 0.9832 - val_loss: 0.1952 - val_accuracy: 0.9850\n",
                    "Epoch 735/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1206 - accuracy: 0.9803 - val_loss: 0.1531 - val_accuracy: 0.9813\n",
                    "Epoch 736/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0752 - accuracy: 0.9852 - val_loss: 0.1348 - val_accuracy: 0.9813\n",
                    "Epoch 737/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0746 - accuracy: 0.9836 - val_loss: 0.1293 - val_accuracy: 0.9869\n",
                    "Epoch 738/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0682 - accuracy: 0.9844 - val_loss: 0.1471 - val_accuracy: 0.9813\n",
                    "Epoch 739/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0787 - accuracy: 0.9854 - val_loss: 0.1860 - val_accuracy: 0.9757\n",
                    "Epoch 740/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0864 - accuracy: 0.9826 - val_loss: 0.1446 - val_accuracy: 0.9869\n",
                    "Epoch 741/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0866 - accuracy: 0.9832 - val_loss: 0.1473 - val_accuracy: 0.9831\n",
                    "Epoch 742/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0978 - accuracy: 0.9835 - val_loss: 0.1596 - val_accuracy: 0.9831\n",
                    "Epoch 743/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0766 - accuracy: 0.9861 - val_loss: 0.1418 - val_accuracy: 0.9831\n",
                    "Epoch 744/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.0705 - accuracy: 0.9868 - val_loss: 0.1461 - val_accuracy: 0.9794\n",
                    "Epoch 745/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0700 - accuracy: 0.9848 - val_loss: 0.1359 - val_accuracy: 0.9831\n",
                    "Epoch 746/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0915 - accuracy: 0.9823 - val_loss: 0.1539 - val_accuracy: 0.9813\n",
                    "Epoch 747/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0723 - accuracy: 0.9851 - val_loss: 0.1768 - val_accuracy: 0.9794\n",
                    "Epoch 748/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0839 - accuracy: 0.9845 - val_loss: 0.1570 - val_accuracy: 0.9775\n",
                    "Epoch 749/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.1027 - accuracy: 0.9809 - val_loss: 0.1763 - val_accuracy: 0.9813\n",
                    "Epoch 750/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.0954 - accuracy: 0.9835 - val_loss: 0.1899 - val_accuracy: 0.9831\n",
                    "Epoch 751/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0786 - accuracy: 0.9851 - val_loss: 0.1608 - val_accuracy: 0.9813\n",
                    "Epoch 752/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.1000 - accuracy: 0.9803 - val_loss: 0.1801 - val_accuracy: 0.9794\n",
                    "Epoch 753/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0873 - accuracy: 0.9841 - val_loss: 0.1462 - val_accuracy: 0.9831\n",
                    "Epoch 754/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.0809 - accuracy: 0.9842 - val_loss: 0.1541 - val_accuracy: 0.9794\n",
                    "Epoch 755/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0789 - accuracy: 0.9852 - val_loss: 0.1620 - val_accuracy: 0.9813\n",
                    "Epoch 756/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0835 - accuracy: 0.9830 - val_loss: 0.1691 - val_accuracy: 0.9794\n",
                    "Epoch 757/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.1038 - accuracy: 0.9816 - val_loss: 0.1618 - val_accuracy: 0.9831\n",
                    "Epoch 758/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0748 - accuracy: 0.9849 - val_loss: 0.1642 - val_accuracy: 0.9831\n",
                    "Epoch 759/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0616 - accuracy: 0.9876 - val_loss: 0.1433 - val_accuracy: 0.9813\n",
                    "Epoch 760/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1008 - accuracy: 0.9806 - val_loss: 0.2043 - val_accuracy: 0.9831\n",
                    "Epoch 761/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0873 - accuracy: 0.9854 - val_loss: 0.1441 - val_accuracy: 0.9831\n",
                    "Epoch 762/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0818 - accuracy: 0.9848 - val_loss: 0.1614 - val_accuracy: 0.9813\n",
                    "Epoch 763/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0877 - accuracy: 0.9839 - val_loss: 0.1727 - val_accuracy: 0.9813\n",
                    "Epoch 764/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0927 - accuracy: 0.9842 - val_loss: 0.1715 - val_accuracy: 0.9831\n",
                    "Epoch 765/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0937 - accuracy: 0.9845 - val_loss: 0.1690 - val_accuracy: 0.9775\n",
                    "Epoch 766/1000\n",
                    "274/274 [==============================] - 25s 89ms/step - loss: 0.0719 - accuracy: 0.9851 - val_loss: 0.1363 - val_accuracy: 0.9850\n",
                    "Epoch 767/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.0892 - accuracy: 0.9842 - val_loss: 0.1521 - val_accuracy: 0.9850\n",
                    "Epoch 768/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0798 - accuracy: 0.9836 - val_loss: 0.1445 - val_accuracy: 0.9813\n",
                    "Epoch 769/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0877 - accuracy: 0.9848 - val_loss: 0.1497 - val_accuracy: 0.9831\n",
                    "Epoch 770/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0837 - accuracy: 0.9842 - val_loss: 0.1546 - val_accuracy: 0.9813\n",
                    "Epoch 771/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0707 - accuracy: 0.9855 - val_loss: 0.1531 - val_accuracy: 0.9813\n",
                    "Epoch 772/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0852 - accuracy: 0.9835 - val_loss: 0.1461 - val_accuracy: 0.9850\n",
                    "Epoch 773/1000\n",
                    "274/274 [==============================] - 23s 84ms/step - loss: 0.0836 - accuracy: 0.9836 - val_loss: 0.1781 - val_accuracy: 0.9831\n",
                    "Epoch 774/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0894 - accuracy: 0.9832 - val_loss: 0.1899 - val_accuracy: 0.9794\n",
                    "Epoch 775/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0812 - accuracy: 0.9849 - val_loss: 0.1763 - val_accuracy: 0.9831\n",
                    "Epoch 776/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.0805 - accuracy: 0.9839 - val_loss: 0.1537 - val_accuracy: 0.9813\n",
                    "Epoch 777/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.0839 - accuracy: 0.9861 - val_loss: 0.1502 - val_accuracy: 0.9850\n",
                    "Epoch 778/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0660 - accuracy: 0.9857 - val_loss: 0.1386 - val_accuracy: 0.9850\n",
                    "Epoch 779/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0857 - accuracy: 0.9811 - val_loss: 0.1724 - val_accuracy: 0.9831\n",
                    "Epoch 780/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.0841 - accuracy: 0.9839 - val_loss: 0.1777 - val_accuracy: 0.9794\n",
                    "Epoch 781/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0805 - accuracy: 0.9838 - val_loss: 0.1753 - val_accuracy: 0.9794\n",
                    "Epoch 782/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0860 - accuracy: 0.9847 - val_loss: 0.1946 - val_accuracy: 0.9813\n",
                    "Epoch 783/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0720 - accuracy: 0.9857 - val_loss: 0.1831 - val_accuracy: 0.9831\n",
                    "Epoch 784/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0773 - accuracy: 0.9858 - val_loss: 0.1560 - val_accuracy: 0.9831\n",
                    "Epoch 785/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0797 - accuracy: 0.9852 - val_loss: 0.1758 - val_accuracy: 0.9794\n",
                    "Epoch 786/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.0868 - accuracy: 0.9836 - val_loss: 0.1782 - val_accuracy: 0.9813\n",
                    "Epoch 787/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0758 - accuracy: 0.9851 - val_loss: 0.1608 - val_accuracy: 0.9813\n",
                    "Epoch 788/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0908 - accuracy: 0.9829 - val_loss: 0.1853 - val_accuracy: 0.9813\n",
                    "Epoch 789/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.1174 - accuracy: 0.9813 - val_loss: 0.1888 - val_accuracy: 0.9831\n",
                    "Epoch 790/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0957 - accuracy: 0.9832 - val_loss: 0.1754 - val_accuracy: 0.9850\n",
                    "Epoch 791/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0769 - accuracy: 0.9855 - val_loss: 0.1602 - val_accuracy: 0.9850\n",
                    "Epoch 792/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0741 - accuracy: 0.9858 - val_loss: 0.1489 - val_accuracy: 0.9850\n",
                    "Epoch 793/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.1099 - accuracy: 0.9810 - val_loss: 0.1920 - val_accuracy: 0.9831\n",
                    "Epoch 794/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0730 - accuracy: 0.9870 - val_loss: 0.1639 - val_accuracy: 0.9831\n",
                    "Epoch 795/1000\n",
                    "274/274 [==============================] - 25s 93ms/step - loss: 0.0664 - accuracy: 0.9861 - val_loss: 0.1265 - val_accuracy: 0.9850\n",
                    "Epoch 796/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.0943 - accuracy: 0.9810 - val_loss: 0.1635 - val_accuracy: 0.9831\n",
                    "Epoch 797/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0835 - accuracy: 0.9851 - val_loss: 0.1695 - val_accuracy: 0.9831\n",
                    "Epoch 798/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0883 - accuracy: 0.9842 - val_loss: 0.1522 - val_accuracy: 0.9850\n",
                    "Epoch 799/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.0825 - accuracy: 0.9854 - val_loss: 0.1712 - val_accuracy: 0.9831\n",
                    "Epoch 800/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0809 - accuracy: 0.9836 - val_loss: 0.2001 - val_accuracy: 0.9794\n",
                    "Epoch 801/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0861 - accuracy: 0.9857 - val_loss: 0.1402 - val_accuracy: 0.9813\n",
                    "Epoch 802/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0997 - accuracy: 0.9814 - val_loss: 0.1914 - val_accuracy: 0.9831\n",
                    "Epoch 803/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0778 - accuracy: 0.9855 - val_loss: 0.1411 - val_accuracy: 0.9869\n",
                    "Epoch 804/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0753 - accuracy: 0.9860 - val_loss: 0.1453 - val_accuracy: 0.9869\n",
                    "Epoch 805/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0713 - accuracy: 0.9861 - val_loss: 0.1471 - val_accuracy: 0.9831\n",
                    "Epoch 806/1000\n",
                    "274/274 [==============================] - 23s 85ms/step - loss: 0.0786 - accuracy: 0.9841 - val_loss: 0.2010 - val_accuracy: 0.9775\n",
                    "Epoch 807/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.1210 - accuracy: 0.9800 - val_loss: 0.2134 - val_accuracy: 0.9794\n",
                    "Epoch 808/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0843 - accuracy: 0.9852 - val_loss: 0.1659 - val_accuracy: 0.9813\n",
                    "Epoch 809/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.0702 - accuracy: 0.9867 - val_loss: 0.1587 - val_accuracy: 0.9775\n",
                    "Epoch 810/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0937 - accuracy: 0.9817 - val_loss: 0.1948 - val_accuracy: 0.9813\n",
                    "Epoch 811/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0777 - accuracy: 0.9861 - val_loss: 0.1483 - val_accuracy: 0.9813\n",
                    "Epoch 812/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0815 - accuracy: 0.9836 - val_loss: 0.1796 - val_accuracy: 0.9831\n",
                    "Epoch 813/1000\n",
                    "274/274 [==============================] - 23s 86ms/step - loss: 0.0757 - accuracy: 0.9854 - val_loss: 0.1781 - val_accuracy: 0.9813\n",
                    "Epoch 814/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0668 - accuracy: 0.9863 - val_loss: 0.1684 - val_accuracy: 0.9794\n",
                    "Epoch 815/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0956 - accuracy: 0.9838 - val_loss: 0.1828 - val_accuracy: 0.9813\n",
                    "Epoch 816/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0838 - accuracy: 0.9844 - val_loss: 0.1556 - val_accuracy: 0.9813\n",
                    "Epoch 817/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0780 - accuracy: 0.9857 - val_loss: 0.1424 - val_accuracy: 0.9831\n",
                    "Epoch 818/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0675 - accuracy: 0.9860 - val_loss: 0.1477 - val_accuracy: 0.9831\n",
                    "Epoch 819/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0828 - accuracy: 0.9835 - val_loss: 0.1614 - val_accuracy: 0.9775\n",
                    "Epoch 820/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.0896 - accuracy: 0.9841 - val_loss: 0.1445 - val_accuracy: 0.9850\n",
                    "Epoch 821/1000\n",
                    "274/274 [==============================] - 26s 93ms/step - loss: 0.0859 - accuracy: 0.9836 - val_loss: 0.1648 - val_accuracy: 0.9831\n",
                    "Epoch 822/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0711 - accuracy: 0.9852 - val_loss: 0.1566 - val_accuracy: 0.9813\n",
                    "Epoch 823/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0780 - accuracy: 0.9836 - val_loss: 0.1624 - val_accuracy: 0.9831\n",
                    "Epoch 824/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0744 - accuracy: 0.9849 - val_loss: 0.1473 - val_accuracy: 0.9831\n",
                    "Epoch 825/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0669 - accuracy: 0.9868 - val_loss: 0.1489 - val_accuracy: 0.9813\n",
                    "Epoch 826/1000\n",
                    "274/274 [==============================] - 25s 93ms/step - loss: 0.0954 - accuracy: 0.9816 - val_loss: 0.1862 - val_accuracy: 0.9831\n",
                    "Epoch 827/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.1008 - accuracy: 0.9832 - val_loss: 0.1592 - val_accuracy: 0.9869\n",
                    "Epoch 828/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0821 - accuracy: 0.9851 - val_loss: 0.1567 - val_accuracy: 0.9831\n",
                    "Epoch 829/1000\n",
                    "274/274 [==============================] - 26s 93ms/step - loss: 0.1108 - accuracy: 0.9807 - val_loss: 0.1868 - val_accuracy: 0.9831\n",
                    "Epoch 830/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0785 - accuracy: 0.9864 - val_loss: 0.1487 - val_accuracy: 0.9831\n",
                    "Epoch 831/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0633 - accuracy: 0.9863 - val_loss: 0.1411 - val_accuracy: 0.9813\n",
                    "Epoch 832/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0816 - accuracy: 0.9852 - val_loss: 0.1648 - val_accuracy: 0.9813\n",
                    "Epoch 833/1000\n",
                    "274/274 [==============================] - 25s 93ms/step - loss: 0.0762 - accuracy: 0.9854 - val_loss: 0.1369 - val_accuracy: 0.9850\n",
                    "Epoch 834/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.0630 - accuracy: 0.9861 - val_loss: 0.1472 - val_accuracy: 0.9794\n",
                    "Epoch 835/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0854 - accuracy: 0.9826 - val_loss: 0.1493 - val_accuracy: 0.9850\n",
                    "Epoch 836/1000\n",
                    "274/274 [==============================] - 26s 93ms/step - loss: 0.0898 - accuracy: 0.9828 - val_loss: 0.1749 - val_accuracy: 0.9831\n",
                    "Epoch 837/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0818 - accuracy: 0.9838 - val_loss: 0.1397 - val_accuracy: 0.9850\n",
                    "Epoch 838/1000\n",
                    "274/274 [==============================] - 25s 93ms/step - loss: 0.0761 - accuracy: 0.9848 - val_loss: 0.1598 - val_accuracy: 0.9831\n",
                    "Epoch 839/1000\n",
                    "274/274 [==============================] - 26s 93ms/step - loss: 0.0949 - accuracy: 0.9820 - val_loss: 0.1503 - val_accuracy: 0.9794\n",
                    "Epoch 840/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0701 - accuracy: 0.9863 - val_loss: 0.1298 - val_accuracy: 0.9869\n",
                    "Epoch 841/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0738 - accuracy: 0.9826 - val_loss: 0.2341 - val_accuracy: 0.9682\n",
                    "Epoch 842/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.1112 - accuracy: 0.9848 - val_loss: 0.1784 - val_accuracy: 0.9794\n",
                    "Epoch 843/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0821 - accuracy: 0.9845 - val_loss: 0.1504 - val_accuracy: 0.9813\n",
                    "Epoch 844/1000\n",
                    "274/274 [==============================] - 26s 93ms/step - loss: 0.0753 - accuracy: 0.9851 - val_loss: 0.1989 - val_accuracy: 0.9738\n",
                    "Epoch 845/1000\n",
                    "274/274 [==============================] - 26s 93ms/step - loss: 0.0770 - accuracy: 0.9848 - val_loss: 0.1495 - val_accuracy: 0.9813\n",
                    "Epoch 846/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.0935 - accuracy: 0.9819 - val_loss: 0.1527 - val_accuracy: 0.9794\n",
                    "Epoch 847/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0778 - accuracy: 0.9852 - val_loss: 0.1464 - val_accuracy: 0.9831\n",
                    "Epoch 848/1000\n",
                    "274/274 [==============================] - 25s 93ms/step - loss: 0.0721 - accuracy: 0.9851 - val_loss: 0.1497 - val_accuracy: 0.9831\n",
                    "Epoch 849/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0778 - accuracy: 0.9854 - val_loss: 0.1509 - val_accuracy: 0.9794\n",
                    "Epoch 850/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0776 - accuracy: 0.9854 - val_loss: 0.1432 - val_accuracy: 0.9850\n",
                    "Epoch 851/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0859 - accuracy: 0.9836 - val_loss: 0.1929 - val_accuracy: 0.9831\n",
                    "Epoch 852/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0918 - accuracy: 0.9844 - val_loss: 0.2025 - val_accuracy: 0.9757\n",
                    "Epoch 853/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.1001 - accuracy: 0.9839 - val_loss: 0.1379 - val_accuracy: 0.9813\n",
                    "Epoch 854/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0750 - accuracy: 0.9857 - val_loss: 0.1372 - val_accuracy: 0.9831\n",
                    "Epoch 855/1000\n",
                    "274/274 [==============================] - 25s 93ms/step - loss: 0.0794 - accuracy: 0.9848 - val_loss: 0.1367 - val_accuracy: 0.9850\n",
                    "Epoch 856/1000\n",
                    "274/274 [==============================] - 26s 93ms/step - loss: 0.0864 - accuracy: 0.9835 - val_loss: 0.1480 - val_accuracy: 0.9850\n",
                    "Epoch 857/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0739 - accuracy: 0.9858 - val_loss: 0.1442 - val_accuracy: 0.9813\n",
                    "Epoch 858/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0747 - accuracy: 0.9844 - val_loss: 0.1567 - val_accuracy: 0.9831\n",
                    "Epoch 859/1000\n",
                    "274/274 [==============================] - 26s 93ms/step - loss: 0.0754 - accuracy: 0.9841 - val_loss: 0.1592 - val_accuracy: 0.9831\n",
                    "Epoch 860/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0736 - accuracy: 0.9847 - val_loss: 0.1463 - val_accuracy: 0.9831\n",
                    "Epoch 861/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0805 - accuracy: 0.9826 - val_loss: 0.1804 - val_accuracy: 0.9831\n",
                    "Epoch 862/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0798 - accuracy: 0.9845 - val_loss: 0.1666 - val_accuracy: 0.9850\n",
                    "Epoch 863/1000\n",
                    "274/274 [==============================] - 26s 93ms/step - loss: 0.0809 - accuracy: 0.9838 - val_loss: 0.1736 - val_accuracy: 0.9813\n",
                    "Epoch 864/1000\n",
                    "274/274 [==============================] - 25s 93ms/step - loss: 0.0833 - accuracy: 0.9836 - val_loss: 0.1393 - val_accuracy: 0.9794\n",
                    "Epoch 865/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.0667 - accuracy: 0.9860 - val_loss: 0.1445 - val_accuracy: 0.9831\n",
                    "Epoch 866/1000\n",
                    "274/274 [==============================] - 26s 93ms/step - loss: 0.0715 - accuracy: 0.9841 - val_loss: 0.1968 - val_accuracy: 0.9813\n",
                    "Epoch 867/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.1075 - accuracy: 0.9811 - val_loss: 0.2032 - val_accuracy: 0.9794\n",
                    "Epoch 868/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0918 - accuracy: 0.9855 - val_loss: 0.1598 - val_accuracy: 0.9831\n",
                    "Epoch 869/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0755 - accuracy: 0.9848 - val_loss: 0.1634 - val_accuracy: 0.9813\n",
                    "Epoch 870/1000\n",
                    "274/274 [==============================] - 25s 93ms/step - loss: 0.0804 - accuracy: 0.9835 - val_loss: 0.1534 - val_accuracy: 0.9831\n",
                    "Epoch 871/1000\n",
                    "274/274 [==============================] - 26s 93ms/step - loss: 0.0960 - accuracy: 0.9816 - val_loss: 0.1651 - val_accuracy: 0.9850\n",
                    "Epoch 872/1000\n",
                    "274/274 [==============================] - 24s 86ms/step - loss: 0.0633 - accuracy: 0.9876 - val_loss: 0.1383 - val_accuracy: 0.9831\n",
                    "Epoch 873/1000\n",
                    "274/274 [==============================] - 26s 93ms/step - loss: 0.0681 - accuracy: 0.9861 - val_loss: 0.1405 - val_accuracy: 0.9831\n",
                    "Epoch 874/1000\n",
                    "274/274 [==============================] - 25s 93ms/step - loss: 0.0715 - accuracy: 0.9847 - val_loss: 0.1573 - val_accuracy: 0.9813\n",
                    "Epoch 875/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0724 - accuracy: 0.9848 - val_loss: 0.1487 - val_accuracy: 0.9813\n",
                    "Epoch 876/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0898 - accuracy: 0.9841 - val_loss: 0.1562 - val_accuracy: 0.9831\n",
                    "Epoch 877/1000\n",
                    "274/274 [==============================] - 26s 93ms/step - loss: 0.0642 - accuracy: 0.9863 - val_loss: 0.1664 - val_accuracy: 0.9794\n",
                    "Epoch 878/1000\n",
                    "274/274 [==============================] - 25s 93ms/step - loss: 0.0920 - accuracy: 0.9841 - val_loss: 0.1663 - val_accuracy: 0.9813\n",
                    "Epoch 879/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0669 - accuracy: 0.9860 - val_loss: 0.1448 - val_accuracy: 0.9850\n",
                    "Epoch 880/1000\n",
                    "274/274 [==============================] - 26s 93ms/step - loss: 0.0719 - accuracy: 0.9858 - val_loss: 0.1778 - val_accuracy: 0.9850\n",
                    "Epoch 881/1000\n",
                    "274/274 [==============================] - 26s 95ms/step - loss: 0.1051 - accuracy: 0.9807 - val_loss: 0.2014 - val_accuracy: 0.9813\n",
                    "Epoch 882/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.1078 - accuracy: 0.9828 - val_loss: 0.2617 - val_accuracy: 0.9738\n",
                    "Epoch 883/1000\n",
                    "274/274 [==============================] - 24s 89ms/step - loss: 0.1167 - accuracy: 0.9817 - val_loss: 0.1601 - val_accuracy: 0.9850\n",
                    "Epoch 884/1000\n",
                    "274/274 [==============================] - 25s 93ms/step - loss: 0.0709 - accuracy: 0.9876 - val_loss: 0.1554 - val_accuracy: 0.9813\n",
                    "Epoch 885/1000\n",
                    "274/274 [==============================] - 26s 95ms/step - loss: 0.0607 - accuracy: 0.9868 - val_loss: 0.1364 - val_accuracy: 0.9869\n",
                    "Epoch 886/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0677 - accuracy: 0.9877 - val_loss: 0.1527 - val_accuracy: 0.9869\n",
                    "Epoch 887/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0789 - accuracy: 0.9847 - val_loss: 0.1940 - val_accuracy: 0.9850\n",
                    "Epoch 888/1000\n",
                    "274/274 [==============================] - 25s 93ms/step - loss: 0.0725 - accuracy: 0.9866 - val_loss: 0.1653 - val_accuracy: 0.9813\n",
                    "Epoch 889/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0827 - accuracy: 0.9833 - val_loss: 0.2179 - val_accuracy: 0.9757\n",
                    "Epoch 890/1000\n",
                    "274/274 [==============================] - 29s 105ms/step - loss: 0.0969 - accuracy: 0.9836 - val_loss: 0.1506 - val_accuracy: 0.9850\n",
                    "Epoch 891/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0621 - accuracy: 0.9876 - val_loss: 0.1375 - val_accuracy: 0.9869\n",
                    "Epoch 892/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0650 - accuracy: 0.9855 - val_loss: 0.1309 - val_accuracy: 0.9869\n",
                    "Epoch 893/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0617 - accuracy: 0.9874 - val_loss: 0.1173 - val_accuracy: 0.9869\n",
                    "Epoch 894/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0733 - accuracy: 0.9858 - val_loss: 0.1648 - val_accuracy: 0.9831\n",
                    "Epoch 895/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.1277 - accuracy: 0.9795 - val_loss: 0.1947 - val_accuracy: 0.9813\n",
                    "Epoch 896/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0928 - accuracy: 0.9845 - val_loss: 0.1659 - val_accuracy: 0.9813\n",
                    "Epoch 897/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0706 - accuracy: 0.9864 - val_loss: 0.1741 - val_accuracy: 0.9850\n",
                    "Epoch 898/1000\n",
                    "274/274 [==============================] - 25s 91ms/step - loss: 0.0785 - accuracy: 0.9848 - val_loss: 0.1528 - val_accuracy: 0.9850\n",
                    "Epoch 899/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0707 - accuracy: 0.9851 - val_loss: 0.1366 - val_accuracy: 0.9850\n",
                    "Epoch 900/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0684 - accuracy: 0.9873 - val_loss: 0.1443 - val_accuracy: 0.9850\n",
                    "Epoch 901/1000\n",
                    "274/274 [==============================] - 26s 95ms/step - loss: 0.0616 - accuracy: 0.9863 - val_loss: 0.1383 - val_accuracy: 0.9850\n",
                    "Epoch 902/1000\n",
                    "274/274 [==============================] - 24s 87ms/step - loss: 0.0716 - accuracy: 0.9848 - val_loss: 0.1513 - val_accuracy: 0.9850\n",
                    "Epoch 903/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0698 - accuracy: 0.9860 - val_loss: 0.1482 - val_accuracy: 0.9757\n",
                    "Epoch 904/1000\n",
                    "274/274 [==============================] - 25s 93ms/step - loss: 0.0956 - accuracy: 0.9807 - val_loss: 0.1984 - val_accuracy: 0.9813\n",
                    "Epoch 905/1000\n",
                    "274/274 [==============================] - 26s 95ms/step - loss: 0.0824 - accuracy: 0.9861 - val_loss: 0.1770 - val_accuracy: 0.9794\n",
                    "Epoch 906/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0664 - accuracy: 0.9871 - val_loss: 0.1602 - val_accuracy: 0.9794\n",
                    "Epoch 907/1000\n",
                    "274/274 [==============================] - 26s 95ms/step - loss: 0.0768 - accuracy: 0.9842 - val_loss: 0.1496 - val_accuracy: 0.9813\n",
                    "Epoch 908/1000\n",
                    "274/274 [==============================] - 25s 93ms/step - loss: 0.0832 - accuracy: 0.9848 - val_loss: 0.1665 - val_accuracy: 0.9813\n",
                    "Epoch 909/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0611 - accuracy: 0.9864 - val_loss: 0.1394 - val_accuracy: 0.9850\n",
                    "Epoch 910/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0789 - accuracy: 0.9841 - val_loss: 0.1632 - val_accuracy: 0.9813\n",
                    "Epoch 911/1000\n",
                    "274/274 [==============================] - 26s 96ms/step - loss: 0.0834 - accuracy: 0.9835 - val_loss: 0.1532 - val_accuracy: 0.9831\n",
                    "Epoch 912/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0697 - accuracy: 0.9855 - val_loss: 0.1629 - val_accuracy: 0.9831\n",
                    "Epoch 913/1000\n",
                    "274/274 [==============================] - 26s 95ms/step - loss: 0.0886 - accuracy: 0.9842 - val_loss: 0.1598 - val_accuracy: 0.9813\n",
                    "Epoch 914/1000\n",
                    "274/274 [==============================] - 24s 88ms/step - loss: 0.0805 - accuracy: 0.9838 - val_loss: 0.1833 - val_accuracy: 0.9813\n",
                    "Epoch 915/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0796 - accuracy: 0.9844 - val_loss: 0.1541 - val_accuracy: 0.9813\n",
                    "Epoch 916/1000\n",
                    "274/274 [==============================] - 26s 93ms/step - loss: 0.0778 - accuracy: 0.9847 - val_loss: 0.1905 - val_accuracy: 0.9813\n",
                    "Epoch 917/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.1044 - accuracy: 0.9848 - val_loss: 0.1629 - val_accuracy: 0.9813\n",
                    "Epoch 918/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0667 - accuracy: 0.9876 - val_loss: 0.1421 - val_accuracy: 0.9831\n",
                    "Epoch 919/1000\n",
                    "274/274 [==============================] - 26s 95ms/step - loss: 0.1134 - accuracy: 0.9811 - val_loss: 0.1741 - val_accuracy: 0.9794\n",
                    "Epoch 920/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0822 - accuracy: 0.9857 - val_loss: 0.1644 - val_accuracy: 0.9813\n",
                    "Epoch 921/1000\n",
                    "274/274 [==============================] - 25s 90ms/step - loss: 0.0848 - accuracy: 0.9841 - val_loss: 0.1614 - val_accuracy: 0.9850\n",
                    "Epoch 922/1000\n",
                    "274/274 [==============================] - 25s 92ms/step - loss: 0.0627 - accuracy: 0.9877 - val_loss: 0.1458 - val_accuracy: 0.9831\n",
                    "Epoch 923/1000\n",
                    "274/274 [==============================] - 26s 94ms/step - loss: 0.0606 - accuracy: 0.9873 - val_loss: 0.1577 - val_accuracy: 0.9813\n",
                    "Epoch 924/1000\n",
                    "274/274 [==============================] - 26s 95ms/step - loss: 0.0762 - accuracy: 0.9829 - val_loss: 0.1812 - val_accuracy: 0.9813\n",
                    "Epoch 925/1000\n",
                    "183/274 [===================>..........] - ETA: 8s - loss: 0.0780 - accuracy: 0.9838"
                    ]
                }
            ],
        "source":
            [
            "# Define the Model\n",
            "# Embedding layer\n",
            "# LSTM layer\n",
            "# Dense layer with softmax activation function\n",
            "'''\n",
            "model = tf.keras.models.Sequential([\n",
            "tf.keras.layers.Input(shape=(MAXLEN,)),\n",
            "tf.keras.layers.Embedding(input_dim=len(documents), output_dim=100, input_length=MAXLEN),\n",
            "tf.keras.layers.LSTM(units=len(documents)),\n",
            "tf.keras.layers.Dropout(0.2),\n",
            "tf.keras.layers.Dense(units=len(documents), activation='softmax')])\n",
            "'''\n",
            "'''\n",
            "model = Sequential()\n",
            "model.add(Embedding(input_dim=len(documents), output_dim=64, input_length=MAXLEN))\n",
            "model.add(SpatialDropout1D(0.1))\n",
            "model.add(LSTM(62, dropout=0.1, recurrent_dropout=0.1)) #\n",
            "model.add(Dense(int(len(documents)/4), activation='softmax'))\n",
            "'''\n",
            "model = Sequential()\n",
            "model.add(Embedding(input_dim=len(train_padded_sequences), output_dim=100, input_length=MAXLEN))\n",
            "model.add(SpatialDropout1D(0.2))\n",
            "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=l2(0.01)))\n",
            "model.add(Dense(FEATURES_NUM, activation='softmax'))\n",
            "\n",
            "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
            "history = model.fit(x=np.array(train_padded_sequences), y=np.array(train_features), epochs=1000, validation_data=(np.array(val_padded_sequences), np.array(val_features)), batch_size=25)\n",
            "\n",
            "model.save('/content/drive/MyDrive/_PTYXIAKI/MODEL_KATHARO/chatbot_model_4.h5')\n",
            "pickle.dump(history, open('/content/drive/MyDrive/_PTYXIAKI/MODEL_KATHARO/history_4.pkl', 'wb'))\n",
            "model.summary()"
            ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "metadata":
            {
            "id": "GWYacR4_7XYM"
            },
        "outputs":
            [
            ],
        "source":
            [
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# Plot training & validation accuracy values\n",
            "plt.plot(history.history['accuracy'])\n",
            "plt.plot(history.history['val_accuracy'])\n",
            "plt.title('Model accuracy')\n",
            "plt.ylabel('Accuracy')\n",
            "plt.xlabel('Epoch')\n",
            "plt.legend(['Train', 'Validation'], loc='upper left')\n",
            "plt.savefig('/content/drive/MyDrive/_PTYXIAKI/model_4_accuracy.png')\n",
            "plt.show()\n",
            "\n",
            "# Plot training & validation loss values\n",
            "plt.plot(history.history['loss'])\n",
            "plt.plot(history.history['val_loss'])\n",
            "plt.title('Model loss')\n",
            "plt.ylabel('Loss')\n",
            "plt.xlabel('Epoch')\n",
            "plt.legend(['Train', 'Validation'], loc='upper left')\n",
            "plt.savefig('/content/drive/MyDrive/_PTYXIAKI/model_4_loss.png')\n",
            "plt.show()\n"
            ]
        },
        {
        "cell_type": "code",
        "execution_count": 2,
        "metadata":
            {
            "colab":
                {
                "base_uri": "https://localhost:8080/",
                "height": 927
                },
            "executionInfo":
                {
                "elapsed": 758,
                "status": "ok",
                "timestamp": 1683907166936
                },
            "id": "iNwJ1SelabNO",
            "outputId": "b7d3f73b-a8f8-4139-c78b-b31708eee824"
            },
        "outputs":
            [
                {
                "data":
                    {
                    "image/png": "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\n",
                    "text/plain":
                        [
                        "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                "metadata":
                    {
                    },
                "output_type": "display_data"
                },
                {
                "data":
                    {
                    "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQkElEQVR4nO3deXxU9b3/8deZfbJDyMaOiIILqCAYtXWjRfSiINaK2KJS+alIxaVVrkvVVtHaVmtFvLYK9V4tFStWbZEiInUBQSwKLogKsiYsIXsy2/n+/kgyZCQgJCFnEt7Px2MekznnzJnP5Kh5+13O1zLGGERERETaIZfTBYiIiIg0l4KMiIiItFsKMiIiItJuKciIiIhIu6UgIyIiIu2WgoyIiIi0WwoyIiIi0m4pyIiIiEi7pSAjIiIi7ZaCjIgkFcuyuPvuuw/6fRs2bMCyLGbPnr3f4958800sy+LNN99sVn0iklwUZERkL7Nnz8ayLCzL4u23395rvzGGHj16YFkW//Vf/+VAhSIidRRkRGSfAoEAzz333F7blyxZwubNm/H7/Q5UJSKyh4KMiOzTeeedx9y5c4lGownbn3vuOQYPHkx+fr5DlYmI1FGQEZF9GjduHLt27WLhwoXxbeFwmBdeeIHLLrusyfdUVVVx880306NHD/x+P0cffTS/+c1vMMYkHBcKhbjxxhvJyckhPT2dCy64gM2bNzd5zi1btnDVVVeRl5eH3+/n2GOP5emnn269LwrMnTuXwYMHEwwG6dKlC5dffjlbtmxJOKaoqIgrr7yS7t274/f7KSgo4MILL2TDhg3xY95//31GjBhBly5dCAaD9OnTh6uuuqpVaxWRPTxOFyAiyat3794UFhbyl7/8hZEjRwIwf/58ysrKuPTSS3n00UcTjjfGcMEFF7B48WImTpzICSecwIIFC/jZz37Gli1bePjhh+PH/uQnP+H//u//uOyyyzj11FN54403OP/88/eqobi4mFNOOQXLsrj++uvJyclh/vz5TJw4kfLycqZOndri7zl79myuvPJKTj75ZKZPn05xcTG///3veeedd/jPf/5DVlYWAGPHjuXjjz9mypQp9O7dm+3bt7Nw4UI2btwYf/3973+fnJwcbrvtNrKystiwYQMvvvhii2sUkX0wIiLfMGvWLAOYFStWmMcee8ykp6eb6upqY4wxP/jBD8xZZ51ljDGmV69e5vzzz4+/76WXXjKA+dWvfpVwvosvvthYlmW++OILY4wxq1atMoC57rrrEo677LLLDGB+8YtfxLdNnDjRFBQUmJ07dyYce+mll5rMzMx4XevXrzeAmTVr1n6/2+LFiw1gFi9ebIwxJhwOm9zcXHPccceZmpqa+HGvvvqqAcxdd91ljDFm9+7dBjAPPfTQPs89b968+O9NRNqGupZEZL8uueQSampqePXVV6moqODVV1/dZ7fSP//5T9xuNz/96U8Ttt98880YY5g/f378OGCv477ZumKM4W9/+xujRo3CGMPOnTvjjxEjRlBWVsYHH3zQou/3/vvvs337dq677joCgUB8+/nnn0///v35xz/+AUAwGMTn8/Hmm2+ye/fuJs/V0HLz6quvEolEWlSXiBwYBRkR2a+cnByGDx/Oc889x4svvkgsFuPiiy9u8tivv/6arl27kp6enrB9wIAB8f0Nzy6Xi759+yYcd/TRRye83rFjB6WlpTz55JPk5OQkPK688koAtm/f3qLv11DTNz8boH///vH9fr+fBx98kPnz55OXl8d3v/tdfv3rX1NUVBQ//owzzmDs2LHcc889dOnShQsvvJBZs2YRCoVaVKOI7JvGyIjIt7rsssu4+uqrKSoqYuTIkfGWh0PNtm0ALr/8ciZMmNDkMQMHDmyTWqCuxWjUqFG89NJLLFiwgDvvvJPp06fzxhtvcOKJJ2JZFi+88ALLli3jlVdeYcGCBVx11VX89re/ZdmyZaSlpbVZrSKHC7XIiMi3GjNmDC6Xi2XLlu2zWwmgV69ebN26lYqKioTtn332WXx/w7Nt23z55ZcJx61duzbhdcOMplgsxvDhw5t85Obmtui7NdT0zc9u2Nawv0Hfvn25+eab+de//sWaNWsIh8P89re/TTjmlFNO4b777uP999/n2Wef5eOPP2bOnDktqlNEmqYgIyLfKi0tjZkzZ3L33XczatSofR533nnnEYvFeOyxxxK2P/zww1iWFZ/51PD8zVlPjzzySMJrt9vN2LFj+dvf/saaNWv2+rwdO3Y05+skGDJkCLm5uTzxxBMJXUDz58/n008/jc+kqq6upra2NuG9ffv2JT09Pf6+3bt37zXN/IQTTgBQ95LIIaKuJRE5IPvq2mls1KhRnHXWWdx+++1s2LCBQYMG8a9//Yu///3vTJ06NT4m5oQTTmDcuHE8/vjjlJWVceqpp7Jo0SK++OKLvc75wAMPsHjxYoYNG8bVV1/NMcccQ0lJCR988AGvv/46JSUlLfpeXq+XBx98kCuvvJIzzjiDcePGxadf9+7dmxtvvBGAzz//nHPOOYdLLrmEY445Bo/Hw7x58yguLubSSy8F4M9//jOPP/44Y8aMoW/fvlRUVPDHP/6RjIwMzjvvvBbVKSJNU5ARkVbjcrl4+eWXueuuu/jrX//KrFmz6N27Nw899BA333xzwrFPP/00OTk5PPvss7z00kucffbZ/OMf/6BHjx4Jx+Xl5bF8+XLuvfdeXnzxRR5//HGys7M59thjefDBB1ul7iuuuIKUlBQeeOABbr31VlJTUxkzZgwPPvhgfDxQjx49GDduHIsWLeJ///d/8Xg89O/fn+eff56xY8cCdYN9ly9fzpw5cyguLiYzM5OhQ4fy7LPP0qdPn1apVUQSWeab7aAiIiIi7YTGyIiIiEi7pSAjIiIi7ZaCjIiIiLRbCjIiIiLSbinIiIiISLulICMiIiLtVoe/j4xt22zdupX09HQsy3K6HBERETkAxhgqKiro2rUrLtd+2l2MwzZv3mzGjx9vOnfubAKBgDnuuOPMihUr4vtt2zZ33nmnyc/PN4FAwJxzzjnm888/P+Dzb9q0yQB66KGHHnrooUc7fGzatGm/f+cdbZHZvXs3p512GmeddRbz588nJyeHdevW0alTp/gxv/71r3n00Uf585//TJ8+fbjzzjsZMWIEn3zyCYFA4Fs/Iz09HYBNmzaRkZFxyL6LiIiItJ7y8nJ69OgR/zu+L47e2fe2227jnXfe4a233mpyvzGGrl27cvPNN3PLLbcAUFZWRl5eHrNnz46vb7I/5eXlZGZmUlZWpiAjIiLSThzo329HB/u+/PLLDBkyhB/84Afk5uZy4okn8sc//jG+f/369RQVFTF8+PD4tszMTIYNG8bSpUubPGcoFKK8vDzhISIiIh2To0Hmq6++YubMmfTr148FCxZw7bXX8tOf/pQ///nPABQVFQF1i8Y1lpeXF9/3TdOnTyczMzP++OYCdCIiItJxOBpkbNvmpJNO4v777+fEE09k0qRJXH311TzxxBPNPue0adMoKyuLPzZt2tSKFYuIiEgycXSwb0FBAcccc0zCtgEDBvC3v/0NgPz8fACKi4spKCiIH1NcXMwJJ5zQ5Dn9fj9+v/+ga4nFYkQikYN+nyQfr9eL2+12ugwREWkDjgaZ0047jbVr1yZs+/zzz+nVqxcAffr0IT8/n0WLFsWDS3l5Oe+99x7XXnttq9RgjKGoqIjS0tJWOZ8kh6ysLPLz83XvIBGRDs7RIHPjjTdy6qmncv/993PJJZewfPlynnzySZ588kkALMti6tSp/OpXv6Jfv37x6dddu3Zl9OjRrVJDQ4jJzc0lJSVFf/jaOWMM1dXVbN++HSChJU9ERDoeR4PMySefzLx585g2bRr33nsvffr04ZFHHmH8+PHxY37+859TVVXFpEmTKC0t5fTTT+e11147oHvIfJtYLBYPMdnZ2S0+nySHYDAIwPbt28nNzVU3k4hIB+bofWTawv7modfW1rJ+/Xp69+4d/+MnHUNNTQ0bNmygT58+rRJ6RUSkbbWL+8gkC3UndTy6piIihwcFGREREWm3FGQEgN69e/PII484XYaIiMhBUZBpZyzL2u/j7rvvbtZ5V6xYwaRJk1q3WBERkUPM0VlL7Vk0ZmMbg8tl4XG1XR7ctm1b/Oe//vWv3HXXXQn34klLS4v/bIwhFovh8Xz7Zc7JyWndQkVERNqAWmSaqai8ls+KKthVGW7Tz83Pz48/MjMzsSwr/vqzzz4jPT2d+fPnM3jwYPx+P2+//TZffvklF154IXl5eaSlpXHyySfz+uuvJ5z3m11LlmXxpz/9iTFjxpCSkkK/fv14+eWX2/S7ioiIfBsFmUaMMVSHowf0qA3HqI3EqAnHDvg9+3u05iz42267jQceeIBPP/2UgQMHUllZyXnnnceiRYv4z3/+w7nnnsuoUaPYuHHjfs9zzz33cMkll/DRRx9x3nnnMX78eEpKSlqtThERkZZS11IjNZEYx9y1wJHP/uTeEaT4Wudy3HvvvXzve9+Lv+7cuTODBg2Kv/7lL3/JvHnzePnll7n++uv3eZ4rrriCcePGAXD//ffz6KOPsnz5cs4999xWqVNERKSl1CLTAQ0ZMiThdWVlJbfccgsDBgwgKyuLtLQ0Pv30029tkRk4cGD859TUVDIyMuK3/hcREUkGapFpJOh188m9Iw7o2G2lteyqCpGT5icvs+V3jg16W+82+qmpqQmvb7nlFhYuXMhvfvMbjjzySILBIBdffDHh8P7H93i93oTXlmVh23ar1SkiItJSCjKNWJZ1wN07QZ+bQNhNwOdutS6hQ+Wdd97hiiuuYMyYMUBdC82GDRucLUpERKQVqGupmdrTDfD79evHiy++yKpVq/jwww+57LLL1LIiIiIdgoJMc7WjJPO73/2OTp06ceqppzJq1ChGjBjBSSed5HRZIiIiLabVr9evb9YKydvKathREaJLmp+uWVo5O9m05NqKiIjztPr1IdaOGmREREQ6LAWZZnKZGD6iWCbmdCkiIiKHreSebpPE0sI7yHWVUh7tAqR96/EiIiLS+tQi01IdeoSRiIhIclOQabaGUTJKMiIiIk5RkGkujfYVERFxnIKMiIiItFsKMs2mJhkRERGnKci0VMe+n6CIiEhSU5A5DJ155plMnTo1/rp379488sgj+32PZVm89NJLLf7s1jqPiIgIKMg0n+VM19KoUaM499xzm9z31ltvYVkWH3300UGdc8WKFUyaNKk1you7++67OeGEE/bavm3bNkaOHNmqnyUiIocvBZlm2hNj2rZraeLEiSxcuJDNmzfvtW/WrFkMGTKEgQMHHtQ5c3JySElJaa0S9ys/Px+/398mnyUiIh2fgkw781//9V/k5OQwe/bshO2VlZXMnTuX0aNHM27cOLp160ZKSgrHH388f/nLX/Z7zm92La1bt47vfve7BAIBjjnmGBYuXLjXe2699VaOOuooUlJSOOKII7jzzjuJRCIAzJ49m3vuuYcPP/wQy7KwLCte7ze7llavXs3ZZ59NMBgkOzubSZMmUVlZGd9/xRVXMHr0aH7zm99QUFBAdnY2kydPjn+WiIgc3rREQWPGQKT6wI6NVEOkBmw/hKta/tnelAPqrvJ4PPz4xz9m9uzZ3H777Vj175k7dy6xWIzLL7+cuXPncuutt5KRkcE//vEPfvSjH9G3b1+GDh36ree3bZuLLrqIvLw83nvvPcrKyhLG0zRIT09n9uzZdO3aldWrV3P11VeTnp7Oz3/+c374wx+yZs0aXnvtNV5//XUAMjMz9zpHVVUVI0aMoLCwkBUrVrB9+3Z+8pOfcP311ycEtcWLF1NQUMDixYv54osv+OEPf8gJJ5zA1Vdf/a3fR0REOjYFmcYi1XB/1wM6NFj/vO+FxQ/Sf28FX+oBHXrVVVfx0EMPsWTJEs4880ygrltp7Nix9OrVi1tuuSV+7JQpU1iwYAHPP//8AQWZ119/nc8++4wFCxbQtWvd7+L+++/fa1zLHXfcEf+5d+/e3HLLLcyZM4ef//znBINB0tLS8Hg85Ofn7/OznnvuOWpra3nmmWdITa377o899hijRo3iwQcfJC8vD4BOnTrx2GOP4Xa76d+/P+effz6LFi1SkBEREXUttUf9+/fn1FNP5emnnwbgiy++4K233mLixInEYjF++ctfcvzxx9O5c2fS0tJYsGABGzduPKBzf/rpp/To0SMeYgAKCwv3Ou6vf/0rp512Gvn5+aSlpXHHHXcc8Gc0/qxBgwbFQwzAaaedhm3brF27Nr7t2GOPxe12x18XFBSwffv2g/osERHpmNQi05g3pa5l5ADUlGwmGNpFuSuLjLxerfPZB2HixIlMmTKFGTNmMGvWLPr27csZZ5zBgw8+yO9//3seeeQRjj/+eFJTU5k6dSrhcLjlNdZbunQp48eP55577mHEiBFkZmYyZ84cfvvb37baZzTm9XoTXluWhW3bh+SzRESkfVGQacyyDrh7B28q2NXgChz4e1rRJZdcwg033MBzzz3HM888w7XXXotlWbzzzjtceOGFXH755UDdmJfPP/+cY4455oDOO2DAADZt2sS2bdsoKCgAYNmyZQnHvPvuu/Tq1Yvbb789vu3rr79OOMbn8xGLxb71s2bPnk1VVVW8Veadd97B5XJx9NFHH1C9IiJyeFPXUjuVlpbGD3/4Q6ZNm8a2bdu44oorAOjXrx8LFy7k3Xff5dNPP+X//b//R3Fx8QGfd/jw4Rx11FFMmDCBDz/8kLfeeishsDR8xsaNG5kzZw5ffvkljz76KPPmzUs4pnfv3qxfv55Vq1axc+dOQqHQXp81fvx4AoEAEyZMYM2aNSxevJgpU6bwox/9KD4+RkREZH8UZNqxiRMnsnv3bkaMGBEf03LHHXdw0kknMWLECM4880zy8/MZPXr0AZ/T5XIxb948ampqGDp0KD/5yU+47777Eo654IILuPHGG7n++us54YQTePfdd7nzzjsTjhk7diznnnsuZ511Fjk5OU1OAU9JSWHBggWUlJRw8sknc/HFF3POOefw2GOPHfwvQ0REDkuWMR17saDy8nIyMzMpKysjIyNxjlFtbS3r16+nT58+BAKBgzpvTckWgrXbKXdlkpF/RGuWLK2gJddWRESct7+/342pRabFOnQOFBERSWoKMs0Uv3WdcoyIiIhjFGRERESk3VKQaTZnVr8WERGRPRRkgGaNd1bfUlLr4GPYRUSk3mEdZBruGFtdfYALRSZQi0wya7im37wrsIiIdCyH9Z193W43WVlZ8XV7UlJS4qtJf5tQOAJRQ5gYtbW1h7JMOQjGGKqrq9m+fTtZWVkJazSJiEjHc1gHGSC+OvPBLkIYqS7DGy6j1qogUKV1f5JNVlbWflfeFhGRjuGwDzKWZVFQUEBubi6RSOSA37fxjT/R85MnWO45mQHXzDyEFcrB8nq9aokRETlMHPZBpoHb7T6oP36eWA2Byk3YXt05VkRExCmH9WDfFrEafnXqVhIREXGKgkwzWfVBxtI0XxEREcc4GmTuvvtuLMtKePTv3z++v7a2lsmTJ5OdnU1aWhpjx46luLjYwYr3sFxqkREREXGa4y0yxx57LNu2bYs/3n777fi+G2+8kVdeeYW5c+eyZMkStm7dykUXXeRgtXs0TNNWi4yIiIhzHB/s6/F4mpwmW1ZWxlNPPcVzzz3H2WefDcCsWbMYMGAAy5Yt45RTTmnrUhM1dC3pzr4iIiKOcbxFZt26dXTt2pUjjjiC8ePHs3HjRgBWrlxJJBJh+PDh8WP79+9Pz549Wbp06T7PFwqFKC8vT3gcCntunKeuJREREac4GmSGDRvG7Nmzee2115g5cybr16/nO9/5DhUVFRQVFeHz+cjKykp4T15eHkVFRfs85/Tp08nMzIw/evTocUhqbxgjY6lBRkRExDGOdi2NHDky/vPAgQMZNmwYvXr14vnnnycYDDbrnNOmTeOmm26Kvy4vLz8kYcZy1d1zxlKLjIiIiGMc71pqLCsri6OOOoovvviC/Px8wuEwpaWlCccUFxfv99bzfr+fjIyMhMehEO9a0mBfERERxyRVkKmsrOTLL7+koKCAwYMH4/V6WbRoUXz/2rVr2bhxI4WFhQ5WWcfSYF8RERHHOdq1dMsttzBq1Ch69erF1q1b+cUvfoHb7WbcuHFkZmYyceJEbrrpJjp37kxGRgZTpkyhsLDQ+RlL7AkyKMiIiIg4xtEgs3nzZsaNG8euXbvIycnh9NNPZ9myZeTk5ADw8MMP43K5GDt2LKFQiBEjRvD44487WXKc5Wq4j4zGyIiIiDjF0SAzZ86c/e4PBALMmDGDGTNmtFFFB04tMiIiIs5LqjEy7Ul8+rWCjIiIiGMUZJpJSxSIiIg4T0GmmSyr4T4yCjIiIiJOUZBppobBvhojIyIi4hwFmWaK30dGs5ZEREQcoyDTTK74YF8RERFxioJMM+25s69aZERERJyiINNMLveewb62rXEyIiIiTlCQaSZX/fRrF4aYpmCLiIg4QkGmmRpuiAeGmFpkREREHKEg00wNg31dGGy1yIiIiDhCQaaZGgcZtciIiIg4Q0GmmVyN1lqyNXFJRETEEQoyzeSqX6LAhSGqJCMiIuIIBZlmcjVaokCzlkRERJyhINNcVqPBvmqQERERcYSCTHPV30fGUouMiIiIYxRkmiuhRUZBRkRExAkKMs3WqEVGQUZERMQRCjLNZe2Zfq2uJREREWcoyDSX1fg+MgoyIiIiTlCQaS4tGikiIuI4BZnmaty1pBYZERERRyjINFt9i4yl+8iIiIg4RUGmuSwr/qOWKBAREXGGgkxzxcfI2NgaIyMiIuIIBZnmio+RgZgaZERERByhINNse1pkNNhXRETEGQoyzdWoRUZdSyIiIs5QkGmu+KKRapERERFxioJMczVaNFI3xBMREXGGgkyz7Vk0UksUiIiIOENBprkat8goyIiIiDhCQaa5rEYtMupaEhERcYSCTHM1WmspqhYZERERRyjINFe8RQZ1LYmIiDhEQabZtESBiIiI0xRkmktLFIiIiDhOQaa5Gi8aqa4lERERRyjINFfjFhl1LYmIiDhCQaa54veR0RIFIiIiTlGQaS6XFwCvFcO2NUhGRETECQoyzeX2xn80sYiDhYiIiBy+FGSay+2L/2iiYQcLEREROXwpyDRXoyBj2QoyIiIiTlCQaS6XG7v+pni2WmREREQcoSDTXJaFbdWNkwmHQg4XIyIicnhKmiDzwAMPYFkWU6dOjW+rra1l8uTJZGdnk5aWxtixYykuLnauyG+IuTwAhEO1DlciIiJyeEqKILNixQr+53/+h4EDByZsv/HGG3nllVeYO3cuS5YsYevWrVx00UUOVbm3hhaZaERBRkRExAmOB5nKykrGjx/PH//4Rzp16hTfXlZWxlNPPcXvfvc7zj77bAYPHsysWbN49913WbZsmYMV72HX30smElbXkoiIiBMcDzKTJ0/m/PPPZ/jw4QnbV65cSSQSSdjev39/evbsydKlS/d5vlAoRHl5ecLjUDH1M5ciYQ32FRERcYLHyQ+fM2cOH3zwAStWrNhrX1FRET6fj6ysrITteXl5FBUV7fOc06dP55577mntUpvmUteSiIiIkxxrkdm0aRM33HADzz77LIFAoNXOO23aNMrKyuKPTZs2tdq5v6mhRSYWUdeSiIiIExwLMitXrmT79u2cdNJJeDwePB4PS5Ys4dFHH8Xj8ZCXl0c4HKa0tDThfcXFxeTn5+/zvH6/n4yMjITHoWLVL1MQi6hrSURExAmOdS2dc845rF69OmHblVdeSf/+/bn11lvp0aMHXq+XRYsWMXbsWADWrl3Lxo0bKSwsdKLkvXnqWmRsBRkRERFHOBZk0tPTOe644xK2paamkp2dHd8+ceJEbrrpJjp37kxGRgZTpkyhsLCQU045xYmS9+Kq71rSnX1FRESc4ehg32/z8MMP43K5GDt2LKFQiBEjRvD44487XVacVd8iY6IaIyMiIuKEpAoyb775ZsLrQCDAjBkzmDFjhjMFfQuX1w+AHQtjjMGyLIcrEhERObw4fh+Z9sxd3yLjJUooajtcjYiIyOFHQaYFGlpkvESpCkUdrkZEROTwoyDTAg2Dfb1EqQ7HHK5GRETk8KMg0xKeuhaZABFqIgoyIiIibU1BpiX8dTfbS7dq1CIjIiLiAAWZlghkApBBFdVhjZERERFpawoyLRFoaJGppkYtMiIiIm1OQaYlGrqWUNeSiIiIExRkWkItMiIiIo5SkGmJeItMNVUaIyMiItLmFGRaomGwr1VNSZUWjhQREWlrCjItEdjTIlNcXutwMSIiIocfBZmW8Ne1yKRaIXaUVTtcjIiIyOFHQaYl6ltkACrKShwsRERE5PCkINMSbi+2JwBAdbmCjIiISFtTkGkhq757iVA5uypDzhYjIiJymFGQaSEruGfm0rrtlQ5XIyIicnhRkGmpRveSUZARERFpWwoyLdVoCvYXxRUOFyMiInJ4UZBpqfoWGXUtiYiItD0FmZYKdgIgkyoFGRERkTamINNSKZ0ByLIq2VERorRaSxWIiIi0FQWZlqpvkenmqwHgC7XKiIiItBkFmZYK1rXI5PvqlihQ95KIiEjbUZBpqfqupS6uKgDWFSvIiIiItBUFmZaqb5HJoG7q9brtmoItIiLSVhRkWiolG4BgeDcA72/YzfaKWicrEhEROWwoyLRURgEA7mgVQ/Jc1ERiLFhT5HBRIiIihwcFmZbypcZbZUb2iAAa8CsiItJWFGRaQ2Z3AI53bwQ0BVtERKStKMi0hk69ARi0/kkAPti4m91VujGeiIjIoaYg0xqGTgLAV76RE/J91EZsFn5S7HBRIiIiHZ+CTGvofToEO2NhuKBb3f1kPtpS6mxNIiIihwEFmdaSczQAJwS3A/CfjaUOFiMiInJ4UJBpLV2OAuBoz1ZcFny8tZwNO6scLkpERKRjU5BpLfUtMqllX/KdfjkAPLponZMViYiIdHgKMq2lS12QYefnXHV6HwBWfF3iYEEiIiIdn4JMa8mp61pi15cc0dkPQHF5CGOMg0WJiIh0bAoyrSWjO3hTwI6QF9sKQDhqU1YTcbgwERGRjktBprW4XNClHwC+knV0SvECda0yIiIicmgoyLSmhnEyO9aSlxEA4OOtZQ4WJCIi0rEpyLSmhnEyOz/njKPrZi49+95GBwsSERHp2BRkWlO8ReYzxpzYDdACkiIiIoeSgkxr6npi3XPRanoFagEoq4lQWq0FJEVERA4FBZnWlNUD8o8HYxPc+Cb59eNk1usOvyIiIoeEgkxr63V63fPmFfTLSwPqlisQERGR1tesILNp0yY2b94cf718+XKmTp3Kk08+2WqFtVvdh9Q9b1zGST07AXDHS2t48t9fOliUiIhIx9SsIHPZZZexePFiAIqKivje977H8uXLuf3227n33nsP+DwzZ85k4MCBZGRkkJGRQWFhIfPnz4/vr62tZfLkyWRnZ5OWlsbYsWMpLi5uTsltp893wXJD0UeclrUrvvn+f37mYFEiIiIdU7OCzJo1axg6dCgAzz//PMcddxzvvvsuzz77LLNnzz7g83Tv3p0HHniAlStX8v7773P22Wdz4YUX8vHHHwNw44038sorrzB37lyWLFnC1q1bueiii5pTcttJy4V+3wfg2A1/drgYERGRjs3TnDdFIhH8/rr1hF5//XUuuOACAPr378+2bdsO+DyjRo1KeH3fffcxc+ZMli1bRvfu3Xnqqad47rnnOPvsswGYNWsWAwYMYNmyZZxyyinNKb1tDJsEn88nZcs7wKhvPVxERESap1ktMsceeyxPPPEEb731FgsXLuTcc88FYOvWrWRnZzerkFgsxpw5c6iqqqKwsJCVK1cSiUQYPnx4/Jj+/fvTs2dPli5d2qzPaDPZdUsVWOXb6NnJH99cHY46VZGIiEiH1Kwg8+CDD/I///M/nHnmmYwbN45BgwYB8PLLL8e7nA7U6tWrSUtLw+/3c8011zBv3jyOOeYYioqK8Pl8ZGVlJRyfl5dHUVHRPs8XCoUoLy9PeLS59AKwXGBHePXKoznK2sQFrnfYVaF1l0RERFpTs7qWzjzzTHbu3El5eTmdOnWKb580aRIpKSkHda6jjz6aVatWUVZWxgsvvMCECRNYsmRJc8oCYPr06dxzzz3Nfn+rcHvqwkz5FjLCxfzLfysA69YOhVNHO1ubiIhIB9KsFpmamhpCoVA8xHz99dc88sgjrF27ltzc3IM6l8/n48gjj2Tw4MFMnz6dQYMG8fvf/578/HzC4TClpaUJxxcXF5Ofn7/P802bNo2ysrL4Y9OmTQf9/VpF5yPqntfumYVV/fUHztQiIiLSQTUryFx44YU888wzAJSWljJs2DB++9vfMnr0aGbOnNmigmzbJhQKMXjwYLxeL4sWLYrvW7t2LRs3bqSwsHCf7/f7/fHp3A0PRwy5qu75w7/EN20r11IFIiIiralZQeaDDz7gO9/5DgAvvPACeXl5fP311zzzzDM8+uijB3yeadOm8e9//5sNGzawevVqpk2bxptvvsn48ePJzMxk4sSJ3HTTTSxevJiVK1dy5ZVXUlhYmNwzlhocdS64fVC+Jb5pV1XEwYJEREQ6nmaNkamuriY9PR2Af/3rX1x00UW4XC5OOeUUvv766wM+z/bt2/nxj3/Mtm3byMzMZODAgSxYsIDvfe97ADz88MO4XC7Gjh1LKBRixIgRPP74480pue35UqDnKbD+3/FNtTEH6xEREemAmhVkjjzySF566SXGjBnDggULuPHGG4G6YHIwXTlPPfXUfvcHAgFmzJjBjBkzmlOm8444MzHIRI1ztYiIiHRAzepauuuuu7jlllvo3bs3Q4cOjY9Z+de//sWJJ57YqgW2a/kDE16GdBsZERGRVtWsFpmLL76Y008/nW3btsXvIQNwzjnnMGbMmFYrrt3zJ7ZO1cbUIiMiItKamhVkAPLz88nPz4+vgt29e/eDvhlehxfITHgZiUYxxmBZlkMFiYiIdCzN6lqybZt7772XzMxMevXqRa9evcjKyuKXv/wltm23do3tVyCxRcZjotRG9PsRERFpLc1qkbn99tt56qmneOCBBzjttNMAePvtt7n77rupra3lvvvua9Ui261vdC15iVIZihL0uR0qSEREpGNpVpD585//zJ/+9Kf4qtcAAwcOpFu3blx33XUKMg18qYkvrWj9wpH+po8XERGRg9KsrqWSkhL69++/1/b+/ftTUlLS4qI6jG+MhWlokREREZHW0awgM2jQIB577LG9tj/22GMMHDiwiXcIgJcYpdW6u6+IiEhraVbX0q9//WvOP/98Xn/99fg9ZJYuXcqmTZv45z//2aoFdiReouysDDldhoiISIfRrBaZM844g88//5wxY8ZQWlpKaWkpF110ER9//DH/+7//29o1tm8Fe+6z4yPKzkotHCkiItJaLGNMq92l7cMPP+Skk04iFkueRYXKy8vJzMykrKzMmZWwd34Bjw0GYG70u3x1+kPceu7e44tERERkjwP9+92sFhk5CF2OhO/XzeLyWlF2qWtJRESk1SjItAVP3XRrL1GKyhVkREREWouCTFtwewHwEePL7ZUOFyMiItJxHNSspYsuumi/+0tLS1tSS8fl9gHgI8KW0hoqaiOkB7wOFyUiItL+HVSQyczM/Nb9P/7xj1tUUIfkTQEg0xOBCHy1o4pBPbKcrUlERKQDOKggM2vWrENVR8fmTwcgy10LwKbd1QoyIiIirUBjZNpCoK4lK8OqAWBTSY2T1YiIiHQYCjJtoX4V7BRTDcDm3dVOViMiItJhKMi0hfquJV+sCjBs2l3fImPHYPP7ENXdfkVERJpDQaYtBOpaZFwmRjd2srmkvkVm8X3wp3Pg5esdLE5ERKT9UpBpC/WzlgD+6Psdm0trsG0Db/22buNHf3WoMBERkfZNQaYtWFb8x2NcXxOO2uzQUgUiIiItpiDjkC936A6/IiIiLaUg01b+6xEAdrlzAVhXrCAjIiLSUgoybaX3dwBIowqAz4srnKxGRESkQ1CQaSv1N8XzxaqxsNlYonvJiIiItJSCTFupn4JtYUinhi27dXdfERGRllKQaSseP3gCAGRY1WwuVZARERFpKQWZtlTfvZRpVRGO2g4XIyIi0v4pyLSl+iDTKzXqcCEiIiIdg4JMW6oPMr1TIw4XIiIi0jEoyLSl+lWwuwUVZERERFqDgkxbqm+RyfdreQIREZHWoCDTluqDTK5XQUZERKQ1KMi0pfog08mlm+GJiIi0BgWZtlR/U7ycXe87XIiIiEjHoCDTllLrFoz071xDPrscLkZERKT9U5BpS8eOif94fGqZg4WIiIh0DAoybcmfBt2HAnBUugb8ioiItJSCTFtLyQagd7DW4UJERETaPwWZtpZaF2S6+6sSNlfU6iZ5IiIiB0tBpq3Vt8jkuhODTFmNgoyIiMjBUpBpayldAOhMacLm2ohWwxYRETlYCjJtrXMfADIqvkrYXBOOOVGNiIhIu6Yg09byjgPAvWttwubqcNSJakRERNo1BZm2ltULvCkQCydsro6oRUZERORgKci0NZcLgp332qyuJRERkYPnaJCZPn06J598Munp6eTm5jJ69GjWrk3scqmtrWXy5MlkZ2eTlpbG2LFjKS4udqjiVlK/eGRj1QoyIiIiB83RILNkyRImT57MsmXLWLhwIZFIhO9///tUVe2ZmnzjjTfyyiuvMHfuXJYsWcLWrVu56KKLHKy6FdQvHtlYjcbIiIiIHDSPkx/+2muvJbyePXs2ubm5rFy5ku9+97uUlZXx1FNP8dxzz3H22WcDMGvWLAYMGMCyZcs45ZRTnCi75fx7Bxm1yIiIiBy8pBojU1ZWt5Bi5851Y0hWrlxJJBJh+PDh8WP69+9Pz549Wbp0aZPnCIVClJeXJzySjrqWREREWkXSBBnbtpk6dSqnnXYaxx1XN0W5qKgIn89HVlZWwrF5eXkUFRU1eZ7p06eTmZkZf/To0eNQl37wmupa0qwlERGRg5Y0QWby5MmsWbOGOXPmtOg806ZNo6ysLP7YtGlTK1XYiproWvpye6UDhYiIiLRvjo6RaXD99dfz6quv8u9//5vu3bvHt+fn5xMOhyktLU1olSkuLiY/P7/Jc/n9fvx+/6EuuWWCnfbatGJDCbZtcLksBwoSERFpnxxtkTHGcP311zNv3jzeeOMN+vTpk7B/8ODBeL1eFi1aFN+2du1aNm7cSGFhYVuX23qOHL7XpvLaKGuLKxwoRkREpP1ytEVm8uTJPPfcc/z9738nPT09Pu4lMzOTYDBIZmYmEydO5KabbqJz585kZGQwZcoUCgsL2++MJYC8YyC9K1RsTdi8YkMJAwr27nYSERGRpjkaZGbOnAnAmWeembB91qxZXHHFFQA8/PDDuFwuxo4dSygUYsSIETz++ONtXOkhkLF3kPlcLTIiIiIHxdEgY4z51mMCgQAzZsxgxowZbVBRG8roClsSNxWVhZypRUREpJ1KmllLh52MrnttKiqvcaAQERGR9ktBxinZR+61SS0yIiIiB0dBximDLt1r087KELW6MZ6IiMgBU5Bxij8drn8fAINFlzQfAB9vTcIlFURERJKUgoyT6m+MZ2EY1K1u/aUPN5U6WJCIiEj7oiDjJGvPr//o/FQANpZUO1WNiIhIu6Mg4yRrz3IEuWleAHZUaMCviIjIgVKQcZLljv+oICMiInLwFGSc1KhrKSe1PshUKsiIiIgcKAUZJ7n2tMh0Sa27ybJaZERERA6cgoyTGrXI5KXXTb+uDEUpq444VZGIiEi7oiDjpEZjZFK+WkBehh+A9buqnKpIRESkXVGQcVKjriVeuoZjOtf9uH5npTP1iIiItDMKMk5qNP0aYFB6BQDrd+peMiIiIgdCQcZpw++J/3ikvxSALbu1CraIiMiBUJBx2qlTIFC3PEF3VwkAW0sVZERERA6EgozTXG44/hIAcs1OALaWKciIiIgcCAWZZJDZDYCsyHagrkWmvFZTsEVERL6NgkwyyOwBQLBmG0fmphGJGV5bU+RwUSIiIslPQSYZZNS1yFhlmznjqBwAPi+qcLIiERGRdkFBJhnUdy1RvpWuGXVrLm0rr3WwIBERkfZBQSYZZHQDXzrYEY7mawC2aeaSiIjIt1KQSQYuN/QqBKB35X8A+GBjKeGo7WRVIiIiSU9BJlnkHQtATmwHAW/dZXlh5WYnKxIREUl6CjLJIli30JI/UsqFg+rGzHxerAG/IiIi+6MgkyxS6leMrC5hYI+6O/1u3q01l0RERPZHQSZZBDvVPdeU0L1TCgCbteaSiIjIfinIJIvgnhaZHp2CAGzYVUUkpgG/IiIi+6IgkywaupZqdtM7O5XMoJfaiM0nW8udrUtERCSJKcgki4aupdoyXNgM6VX3+j8bdztYlIiISHJTkEkWDUEGAzWl9MtLB+CrnVXO1SQiIpLkFGSShdsL/oy6n2tK6NOlbsDvegUZERGRfVKQSSYNrTLVJfTpkgbUDfgVERGRpinIJJP4FOzd9K5vkdmyu4ZQNOZgUSIiIslLQSaZxGculZCT5ifV58Y2sKlEN8YTERFpioJMMml0LxnLsujdJRWAd7/c5WBRIiIiyUtBJpmk5dY9V2wD4IQeWQDMfneDM/WIiIgkOQWZZJLdt+555zoArj2z7vVXO6qoDkedqkpERCRpKcgkk+x+dc+76oJM904pdErxArBhp8bJiIiIfJOCTDLpclTd8+6vIRoCoE/9OBndT0ZERGRvCjLJJD0ffOlgYlCyHiB+P5n1OyudrExERCQpKcgkE8uCLkfW/VzfvXRETl2LjJYqEBER2ZuCTLLpcnTd89dLgT1dS1/uUJARERH5JgWZZHPcRXXPq/4PjOG4rpkAfLS5lLVFFQ4WJiIiknwUZJLNEWeB5YbaMqjYRs/sFM4/vgBj4NFF65yuTkREJKkoyCQbjw869a77uf5+Mld/9wgA/r1uB8YYhwoTERFJPgoyyahL/f1kdnwGwDEFGXhcFhW1UbaV1TpYmIiISHJxNMj8+9//ZtSoUXTt2hXLsnjppZcS9htjuOuuuygoKCAYDDJ8+HDWrTsMule6Da57Xv9vAHweF31z6qZhf1ZU7lRVIiIiScfRIFNVVcWgQYOYMWNGk/t//etf8+ijj/LEE0/w3nvvkZqayogRI6it7eCtEn3PrnveuDS+6ej8dAA+04BfERGROI+THz5y5EhGjhzZ5D5jDI888gh33HEHF154IQDPPPMMeXl5vPTSS1x66aVtWWrbCnaqe46G656/XMzJmfAy8Nk2BRkREZEGSTtGZv369RQVFTF8+PD4tszMTIYNG8bSpUv3+b5QKER5eXnCo91xueuejQ2bVsD/juZHy0cD8OUO3eFXRESkQdIGmaKiIgDy8vIStufl5cX3NWX69OlkZmbGHz169DikdR4SVv1lMTZseT9hVzhqO1CQiIhIckraINNc06ZNo6ysLP7YtGmT0yUdvHiQie21y9b0axERkbikDTL5+fkAFBcXJ2wvLi6O72uK3+8nIyMj4dHuNG6R+QZbOUZERCQuaYNMnz59yM/PZ9GiRfFt5eXlvPfeexQWFjpYWRuwGo2R+Qa1yIiIiOzh6KylyspKvvjii/jr9evXs2rVKjp37kzPnj2ZOnUqv/rVr+jXrx99+vThzjvvpGvXrowePdq5ottC4xaZbwQXBRkREZE9HA0y77//PmeddVb89U033QTAhAkTmD17Nj//+c+pqqpi0qRJlJaWcvrpp/Paa68RCAScKrltWI0byr4RZDTWV0REJM7RIHPmmWfud+0gy7K49957uffee9uwqiRgWXt+/kb3klpkRERE9kjaMTKHtYb7yICCjIiIyH4oyCSjxl1LduIU7Ji6lkREROIUZJJR4yDzjRaZ/XXFiYiIHG4UZJKR1bhrKTG4xBRkRERE4hRkktF+WmRs3RFPREQkTkEmGe23a6mNaxEREUliCjLJKCHIfGOwr5KMiIhInIJMMnLtq0XGaPq1iIhIIwoyyaqhVabR9GsXRotGioiINKIgk6yaWAHbja3BviIiIo0oyCSrJlbAdmGra0lERKQRBZlkFW+R2RNcLHUtiYiIJFCQSVbxIJM4RgZ0LxkREZEGCjLJah9jZEALR4qIiDRQkElWrr2DjFUfZHQvGRERkToKMsmqiRaZhq4l5RgREZE6CjLJqon7yKhrSUREJJGCTLJqmH5t7z3YN6bBviIiIoCCTPKKt8hE92xqmLWkHCMiIgIoyCSveJCJxDc1dC0ZdS2JiIgACjLJqyHIxPYEGVfDrCU1yYiIiAAKMsnL1cSikZa6lkRERBpTkElWTXQteay6Z3UtiYiI1FGQSVZNdC15LN0QT0REpDEFmWQVn369Z9aSR11LIiIiCRRkklUT06/d9V1LWjRSRESkjoJMstpP15Lu7CsiIlJHQSZZNdUi41LXkoiISGMKMsmqienXnvpn3UdGRESkjoJMsmpq+rVLd/YVERFpTEEmWTU5RkZdSyIiIo0pyCSrJqZfN1wsdS2JiIjUUZBJVk0M9t3TIqMgIyIiAgoyyavJINMwRsaJgkRERJKPgkyyamr16/oWGS1RICIiUkdBJlm5mlqioH6TgoyIiAigIJO8rIbUsnfXkpYoEBERqaMgk6ya6Fpyo+nXIiIijSnIJKv49OtGQcalWUsiIiKNKcgkK2vvS+NGXUsiIiKNKcgkq6aCTHywbxvXIiIikqQUZJJVky0y6loSERFpTEEmWTVMv27EXT9rSfeRERERqaMgk6wapl830rBEgVa/FhERqaMgk6ya6Fpy1XctvbamSGFGREQE8DhdgOxDas5em1K8da00z7+/mbfW7WTCqb05u38uPTunYAwEfXt3R4mIiHRklung/2tfXl5OZmYmZWVlZGRkOF3OgSvfCi9PgS9eT9i809uNx2q+xyvRYewiM7493e9h0nePoCArSL/cNLp1CpKd6sNq1EUVidl43WqEExGR5Hegf7/bRZCZMWMGDz30EEVFRQwaNIg//OEPDB069IDe226DTIOP58HcK/babFtutrm7URm2eSVWyFemgMX2CdQQAAxeYuRkpuF2W2QFfRwdXctnuw2dew/k1L7Z9M9PJ9XvoXunIJ1SfERtQ5r/2xvoPti4m2jMMLRP59b/riIiIvU6TJD561//yo9//GOeeOIJhg0bxiOPPMLcuXNZu3Ytubm53/r+dh9kGkRDsOZvsOZFqCyGoo/2OiRieakhiGVHSKWW9SafdaY7+VYJJ7i+BOBvsdNZbR9BicmgBh8ubDzYVBKk1gqQn2IIuGw+dh2F5Q3SKWDRt6ALVriSosoIOV+9RAwXW3pfRJoXMtLTGNSzC6FIDGNZZAa9pJtK/MF0PD4/4ZiNbcDrtvB76lqDfG43oWiMUNQmPeChJhwjPzNAis9DesCDy7L4ckclNeU7yemSR4/sVCpqI0RjhkjMxudxsXl3DUfmpmEbg8flIhyz40GsJhwj4HUltEY1pTIUJeh143ZZxDb/ByuzO670HDBmr8HWxhi+3LKDLp0yyUr1t8YV3SMaAs9+ztlEPa2i4V/9Q3HufQlXgTdl359p2+Bqw1ZDY+CTv0PBQOh8RNt9bntj22Bi4PYewLGxJmddHpZa49+xUAV4U1vn3wtjYNuHkNEN0uqHL7TW9ToE/53qMEFm2LBhnHzyyTz22GMA2LZNjx49mDJlCrfddtu3vr/DBJlv2vkFbFoGn/0DyjZBdQmUb2nzMqqNnyhuMqxqKk2AFEK4LMNuk0YVAfxEsOoHKTf8g1Zq0vERIYyXoBUiZLxU4yeNGgwW5aSSTTk9XDvYYrqwga5kmHK8xEizatht0si3drPdZBEgjGVB2HjY5iogQA259k52ePLpbJfgs6K4MYSsujrL7ABp1OAmRmdTyg4rm85WJQWmmGrjp9qTSUashGqClLqyCFgxQjFDJSkcy5eUmxR2+7uRYldigFp3GlmRHbhNhK89vXFbFh4i5IY3U2WlEvZlErL8mEgIl9tNZ3s3OwK9MLXl+E2IoNuQG/6aIndXqt3ppER2Y7v9+N0QsXyEjYdu4S/Z7cmlhEyM5SaLCqLeNGzLQ2q0hECknKjloYoUYnaM0tS+eLx+0iM7iHrTIFJDSm0R3lgNW319SHNH8RIhq2YzPlNDjScTb6yWMl8uUVcALBdVvmxi0SjpVi2+0E5SY2UEohVsTj2OqMuP5XLj8bgxlhuDi0BNEd5wGTWBPMoDXfG6DAHCgCFYu50Qfvy1O8gObWKXvzuhYB613ky8oVL8djXuaDVRbwY5VWvZmH4itjcVq3oH2dHtuCxw22HC/s6Ue7rgMjFcviB2LIIVrcXtdmFbXqpdqaRYYaxINb5oJcabgidUSsxAeWpvvJZNILyLqPFgYhHwp9K1ZDkAMcvDjqwTMJaLqHFh/OmkVG7CZRmqrRRitsH2Z+IhiuVyg9tLasnHWEAorQdufyoGg6tqBzFPgKjlIxwOY+MmxecmUFOE8QSw3QFi4Rpi7gA+jwtvzU7K0vridYEvtJNA5RZ2Zx2D5c8gUP4Vxu2j1tuJEF5SUlKJuvzYlgdXVTHVVgpuYniw8RAjFovhIoYvWkEwtJOoy080JRdXpJqIJxXbmwKxCK6aEow/HRPoRCxURTC8i3Bad4Lh3UQsDy5jg9tN2q7VVAfycXt9BHavI+bysrvb2Vh2hLTSzwj7sogGcwjFINWuoLKmlk6RYlJDO9jVZQgpsQpqjZeYPxOXLwViYYjWEqjeStTy4YrW4MLG+NOpDHajxvjINOXYto3bRPGGdlPtTgMDtZl98LnAX7UVj2UTicZwWQYLcEVr8LggFothDHhCpezOHkSaqcbYMWp8nQl43bgqi3BHqnCHygj5OxFLycVn1+IvX48VqSKU1p0YbqKBztiWF0+sBne0GneshnAwB0+oFHekCjDE3Cm4otXEYjEsfyrEohhfKsa2CdQUEazeyu68U0kpW4cVCxEJ5uCJ1RBKySPkycTjC2CH6/47ZLm9mMrtRC0PeFNx+4K4XRamugTLjpBRshqDi1BqV8qzBuB1WwR2fUIMNzV5g+v+/QiV4q3dhXF5634fFZuozjqKqDsIGd2IxGJgx0gvW0tayccYLKryh5KyYxW25aHsiPMx5dvwVGzGlVFALL0HrtBufDs/wQ5mE/Wmg8uFiYbwVRdjDFS5M3D7gnj8KQR3fULVeX+g0/EjWvXvS4cIMuFwmJSUFF544QVGjx4d3z5hwgRKS0v5+9//vtd7QqEQoVAo/rq8vJwePXp0vCDzTcZAyVdQUVT3HwxvEErW14WccBVsWblnJlT5VgiVg8ePqS3H9mdh3F6MHSNi+fDU7sJXs8PZ7yMiIu3G6qOncPy4X7XqOQ80yCT1rKWdO3cSi8XIy8tL2J6Xl8dnn33W5HumT5/OPffc0xblJRfLguy+dY8GPU/59rcBjRsVvVAXimpLweUBl7fuZ7cPfKl1rzF1Qcjtg7LNEKmG9K4QrqxbrbumpC40uTx1gcpy1XWfhCrAjhKrLcPl8WMbcKV0xrKjRGvLsdw+amw3oYpdpKWlY2X1YPe6ZViWRUanHGLhEITK2Vljk53fi2hVCSFPBiZSja+2hMrqKkxtBcGsPHaVV+HJ6obtCRKt3o0/VIIrkEHQY4h6UqkI2dS4UojVlONxu0nL7U3Nrk1UlO4kLa8vgUgpZbVR7HANu00q+WYHOUcNpaisltrNH+JyufAQo8b24ErPI8Xnwl2zi6pQhKgrQGqnPFJqt1O5cxMRXya2HcPrC1BidSJY9mVdiHT7cJkYeAIE0zKpLt2BNxAkza5gQ3WAdLuMNJ+b3Wl9icViVJWXkO2LEfZnEancjT9WRYU3m4Dfhy9WjeX24vH4MKVfE4vZVFppeKIV1Lgz8XXuRobPIlSyCQtDjfFjTAyTmoddvRtfIAVftIIQXjw1JQRiFdS4ghAJE00vwBsuI8uUUe7vRk04QjQaBRPDhY1lbELGww4rm27WdgKxKrBjuKI1hFwp1HrSSLMr8QTTKfdkE6zaQsSdQmqsjLA/C2+0iogvC1Nd11LgN2Gqo6bu/+KNIRYJEfJnU10bJpMK8KUSqq3G73Fh+zMIh2pxhSsIuKHK+HH7U4kai1i4hkCskvTUFOxYjNqoTY07jYAL3B43VVWVBAiREd1NLNAJOxbGjkaI+jvhry6mLLUXVbafzgGDN1pNKFRLxPKCscGOEQqFqArkETS1xCK1+E0tUXcqLq+PVCtE2BWgvDZK0IriMhFqrRRc2LgCGcRsm1iklpA/h+61n+OPVRG2vAR9PspIIVZTQWVaL0LhMKl2JTU11WR4YqS4IviIUONOJ9NnE8FH2HZRHQWXx0M0BmHLizdaRdTfCROtIeZJIy1WhmVH8BPG9gQJGQ+Eq8jwGSLGhYmGcUerCbtTKCUDt2XTM7aJsO1iredoOntqSYmWgh0j6IpS7s2hFh/poWLcbi+7SKc2Cql+DxmuGiK2RVmtjTeYjitShSdWQ8AVJeYOkGkq2eXrit9l464toTQWwIpUURWKkpqSit/vJ2JcpFtVVLszSbdCeCPlRKMRdrhzKQm7yQj6sLEIRcHYNj67mqjLh9eycftS6FTzNR4TIeoOYFxeQqEQlVYaYU86tsdPLrsJR2NU2n6saC3dPaVs8fTEY9mkRXfXtYTho5IUwsZDntlOyBVkpzuXFFOL14piLDdRTyruyiKCkd343TahQB4+u4Zu9lZK7FS2kUOpCdKdHZRYnai1XeR6qiFSg9/rppIgVqSaFL8HCxdWpArLjlBh+wkG/LhjNQSjlRR5ulJgiqlyZxKJxfB4AwStELFQNRE8VBMgjBfLGFKoYbe/gKBdjc+uISVahtfjoSZqqCTI1i6n063iI6KhSmxvGtlWOYHaHXhcLjZ6emLFQnQ2ZdQQIORNpyrqpjNlRI2LWk8aeIK4IlWkWCFqjB9XtIqt/r5878jvc3wr/kk7GEkdZJpj2rRp3HTTTfHXDS0ychAsC4Kd9rz25u99TMP+nKMbbczb+7gmuL/xDHv+QUyrf8TPmD9gr/en7uO8jYcff/voqaYUfusRdf8kndmssx+MEw/5J4jI4Wu00wW0qqQOMl26dMHtdlNcXJywvbi4mPz8Jv64An6/H7+/lQdjioiISFJK6puK+Hw+Bg8ezKJFi+LbbNtm0aJFFBZ++/89i4iISMeW1C0yADfddBMTJkxgyJAhDB06lEceeYSqqiquvPJKp0sTERERhyV9kPnhD3/Ijh07uOuuuygqKuKEE07gtdde22sAsIiIiBx+knr6dWvosPeRERER6cAO9O93Uo+REREREdkfBRkRERFptxRkREREpN1SkBEREZF2S0FGRERE2i0FGREREWm3FGRERESk3VKQERERkXZLQUZERETaraRfoqClGm5cXF5e7nAlIiIicqAa/m5/2wIEHT7IVFRUANCjRw+HKxEREZGDVVFRQWZm5j73d/i1lmzbZuvWraSnp2NZVqudt7y8nB49erBp0yat4eQgXYfkoOuQHHQdnKdr0HqMMVRUVNC1a1dcrn2PhOnwLTIul4vu3bsfsvNnZGToH9YkoOuQHHQdkoOug/N0DVrH/lpiGmiwr4iIiLRbCjIiIiLSbinINJPf7+cXv/gFfr/f6VIOa7oOyUHXITnoOjhP16DtdfjBviIiItJxqUVGRERE2i0FGREREWm3FGRERESk3VKQERERkXZLQaaZZsyYQe/evQkEAgwbNozly5c7XVKHMX36dE4++WTS09PJzc1l9OjRrF27NuGY2tpaJk+eTHZ2NmlpaYwdO5bi4uKEYzZu3Mj5559PSkoKubm5/OxnPyMajbblV+kwHnjgASzLYurUqfFtugZtY8uWLVx++eVkZ2cTDAY5/vjjef/99+P7jTHcddddFBQUEAwGGT58OOvWrUs4R0lJCePHjycjI4OsrCwmTpxIZWVlW3+VdisWi3HnnXfSp08fgsEgffv25Ze//GXCGkC6Dg4yctDmzJljfD6fefrpp83HH39srr76apOVlWWKi4udLq1DGDFihJk1a5ZZs2aNWbVqlTnvvPNMz549TWVlZfyYa665xvTo0cMsWrTIvP/+++aUU04xp556anx/NBo1xx13nBk+fLj5z3/+Y/75z3+aLl26mGnTpjnxldq15cuXm969e5uBAweaG264Ib5d1+DQKykpMb169TJXXHGFee+998xXX31lFixYYL744ov4MQ888IDJzMw0L730kvnwww/NBRdcYPr06WNqamrix5x77rlm0KBBZtmyZeatt94yRx55pBk3bpwTX6lduu+++0x2drZ59dVXzfr1683cuXNNWlqa+f3vfx8/RtfBOQoyzTB06FAzefLk+OtYLGa6du1qpk+f7mBVHdf27dsNYJYsWWKMMaa0tNR4vV4zd+7c+DGffvqpAczSpUuNMcb885//NC6XyxQVFcWPmTlzpsnIyDChUKhtv0A7VlFRYfr162cWLlxozjjjjHiQ0TVoG7feeqs5/fTT97nftm2Tn59vHnroofi20tJS4/f7zV/+8hdjjDGffPKJAcyKFSvix8yfP99YlmW2bNly6IrvQM4//3xz1VVXJWy76KKLzPjx440xug5OU9fSQQqHw6xcuZLhw4fHt7lcLoYPH87SpUsdrKzjKisrA6Bz584ArFy5kkgkknAN+vfvT8+ePePXYOnSpRx//PHk5eXFjxkxYgTl5eV8/PHHbVh9+zZ58mTOP//8hN816Bq0lZdffpkhQ4bwgx/8gNzcXE488UT++Mc/xvevX7+eoqKihOuQmZnJsGHDEq5DVlYWQ4YMiR8zfPhwXC4X7733Xtt9mXbs1FNPZdGiRXz++ecAfPjhh7z99tuMHDkS0HVwWodfNLK17dy5k1gslvAfZ4C8vDw+++wzh6rquGzbZurUqZx22mkcd9xxABQVFeHz+cjKyko4Ni8vj6KiovgxTV2jhn3y7ebMmcMHH3zAihUr9tqna9A2vvrqK2bOnMlNN93Ef//3f7NixQp++tOf4vP5mDBhQvz32NTvufF1yM3NTdjv8Xjo3LmzrsMBuu222ygvL6d///643W5isRj33Xcf48ePB9B1cJiCjCS1yZMns2bNGt5++22nSzmsbNq0iRtuuIGFCxcSCAScLuewZds2Q4YM4f777wfgxBNPZM2aNTzxxBNMmDDB4eoOH88//zzPPvsszz33HMceeyyrVq1i6tSpdO3aVdchCahr6SB16dIFt9u91+yM4uJi8vPzHaqqY7r++ut59dVXWbx4Md27d49vz8/PJxwOU1pamnB842uQn5/f5DVq2Cf7t3LlSrZv385JJ52Ex+PB4/GwZMkSHn30UTweD3l5eboGbaCgoIBjjjkmYduAAQPYuHEjsOf3uL//HuXn57N9+/aE/dFolJKSEl2HA/Szn/2M2267jUsvvZTjjz+eH/3oR9x4441Mnz4d0HVwmoLMQfL5fAwePJhFixbFt9m2zaJFiygsLHSwso7DGMP111/PvHnzeOONN+jTp0/C/sGDB+P1ehOuwdq1a9m4cWP8GhQWFrJ69eqE/3AsXLiQjIyMvf4wyN7OOeccVq9ezapVq+KPIUOGMH78+PjPugaH3mmnnbbXrQc+//xzevXqBUCfPn3Iz89PuA7l5eW89957CdehtLSUlStXxo954403sG2bYcOGtcG3aP+qq6txuRL/XLrdbmzbBnQdHOf0aOP2aM6cOcbv95vZs2ebTz75xEyaNMlkZWUlzM6Q5rv22mtNZmamefPNN822bdvij+rq6vgx11xzjenZs6d54403zPvvv28KCwtNYWFhfH/D1N/vf//7ZtWqVea1114zOTk5mvrbAo1nLRmja9AWli9fbjwej7nvvvvMunXrzLPPPmtSUlLM//3f/8WPeeCBB0xWVpb5+9//bj766CNz4YUXNjnt98QTTzTvvfeeefvtt02/fv007fcgTJgwwXTr1i0+/frFF180Xbp0MT//+c/jx+g6OEdBppn+8Ic/mJ49exqfz2eGDh1qli1b5nRJHQbQ5GPWrFnxY2pqasx1111nOnXqZFJSUsyYMWPMtm3bEs6zYcMGM3LkSBMMBk2XLl3MzTffbCKRSBt/m47jm0FG16BtvPLKK+a4444zfr/f9O/f3zz55JMJ+23bNnfeeafJy8szfr/fnHPOOWbt2rUJx+zatcuMGzfOpKWlmYyMDHPllVeaioqKtvwa7Vp5ebm54YYbTM+ePU0gEDBHHHGEuf322xNuI6Dr4BzLmEa3JhQRERFpRzRGRkRERNotBRkRERFptxRkREREpN1SkBEREZF2S0FGRERE2i0FGREREWm3FGRERESk3VKQEZHDjmVZvPTSS06XISKtQEFGRNrUFVdcgWVZez3OPfdcp0sTkXbI43QBInL4Offcc5k1a1bCNr/f71A1ItKeqUVGRNqc3+8nPz8/4dGpUyegrttn5syZjBw5kmAwyBFHHMELL7yQ8P7Vq1dz9tlnEwwGyc7OZtKkSVRWViYc8/TTT3Psscfi9/spKCjg+uuvT9i/c+dOxowZQ0pKCv369ePll18+tF9aRA4JBRkRSTp33nknY8eO5cMPP2T8+PFceumlfPrppwBUVVUxYsQIOnXqxIoVK5g7dy6vv/56QlCZOXMmkydPZtKkSaxevZqXX36ZI488MuEz7rnnHi655BI++ugjzjvvPMaPH09JSUmbfk8RaQVOr1opIoeXCRMmGLfbbVJTUxMe9913nzGmbvXza665JuE9w4YNM9dee60xxpgnn3zSdOrUyVRWVsb3/+Mf/zAul8sUFRUZY4zp2rWruf322/dZA2DuuOOO+OvKykoDmPnz57fa9xSRtqExMiLS5s466yxmzpyZsK1z587xnwsLCxP2FRYWsmrVKgA+/fRTBg0aRGpqanz/aaedhm3brF27Fsuy2Lp1K+ecc85+axg4cGD859TUVDIyMti+fXtzv5KIOERBRkTaXGpq6l5dPa0lGAwe0HFerzfhtWVZ2LZ9KEoSkUNIY2REJOksW7Zsr9cDBgwAYMCAAXz44YdUVVXF97/zzju4XC6OPvpo0tPT6d27N4sWLWrTmkXEGWqREZE2FwqFKCoqStjm8Xjo0qULAHPnzmXIkCGcfvrpPPvssyxfvpynnnoKgPHjx/OLX/yCCRMmcPfdd7Njxw6mTJnCj370I/Ly8gC4++67ueaaa8jNzWXkyJFUVFTwzjvvMGXKlLb9oiJyyCnIiEibe+211ygoKEjYdvTRR/PZZ58BdTOK5syZw3XXXUdBQQF/+ctfOOaYYwBISUlhwYIF3HDDDZx88smkpKQwduxYfve738XPNWHCBGpra3n44Ye55ZZb6NKlCxdffHHbfUERaTOWMcY4XYSISAPLspg3bx6jR492uhQRaQc0RkZERETaLQUZERERabc0RkZEkop6u0XkYKhFRkRERNotBRkRERFptxRkREREpN1SkBEREZF2S0FGRERE2i0FGREREWm3FGRERESk3VKQERERkXZLQUZERETarf8PUEp6IhnMLmoAAAAASUVORK5CYII=\n",
                    "text/plain":
                        [
                        "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                "metadata":
                    {
                    },
                "output_type": "display_data"
                }
            ],
        "source":
            [
            "import matplotlib.pyplot as plt\n",
            "'''\n",
            "history = {\"loss\": [60.049, 55.584, 50.394, 46.084, 43.635, 41.551, 40.186, 39.520, 38.751, 37.617, 37.232, 36.509, 35.945, 36.021, 35.233, 34.683, 34.297, 33.997, 33.572, 33.366, 32.856, 33.527, 32.331, 32.024, 31.841, 31.298, 31.341, 31.204, 30.603, 30.082, 30.064, 29.584, 29.313, 29.340, 29.050, 28.650, 28.220, 27.753, 27.805, 27.185, 26.721, 26.484, 25.533, 25.642, 25.237, 24.607, 24.227, 23.584, 23.265, 23.045, 22.731, 22.193, 21.621, 21.294, 21.350, 20.760, 19.974, 20.119, 19.512, 19.155, 19.153, 18.550, 17.684, 17.681, 17.122, 16.999, 16.324, 16.013, 15.577, 15.663, 14.983, 14.752, 14.592, 13.990, 13.553, 13.742, 13.303, 13.797, 13.046, 12.206, 12.688, 11.872, 11.830, 10.950, 10.954, 10.852, 10.939, 10.280, 10.000, 0.9937, 0.9607, 0.9794, 0.9616, 0.9317, 0.9734, 0.8813, 0.8404, 0.8562, 0.8129, 0.9270, 0.8395, 0.7847, 0.7281, 0.7495, 0.7399, 0.7381, 0.7263, 0.7245, 0.6764, 0.6359, 0.6507, 0.6138, 0.5996, 0.5949, 0.5792, 0.5567, 0.5481, 0.5657, 0.5391, 0.5415, 0.6375, 0.5757, 0.5161, 0.4919, 0.4998, 0.5480, 0.4902, 0.4881, 0.4574, 0.4342, 0.4446, 0.4477, 0.4439, 0.4303, 0.4498, 0.4096, 0.3833, 0.3991, 0.4005, 0.3941, 0.3716, 0.4195, 0.3543, 0.3539, 0.3579, 0.3206, 0.3258, 0.3260, 0.3048, 0.3380, 0.3294, 0.3320, 0.3290, 0.3305, 0.3344, 0.3187, 0.3115, 0.3049, 0.2998, 0.4663, 0.4457, 0.3632, 0.3232, 0.2925, 0.2898, 0.2687, 0.2731, 0.2585, 0.2667, 0.2922, 0.2605, 0.2388, 0.2506, 0.2507, 0.2533, 0.2716, 0.2214, 0.2429, 0.2646, 0.2185, 0.2772, 0.2306, 0.2292, 0.2519, 0.2844, 0.2093, 0.2076, 0.2223, 0.2285, 0.2104, 0.2136, 0.1808, 0.1907, 0.2157, 0.1981, 0.2181, 0.2070, 0.2665, 0.4256, 0.3625, 0.2640, 0.2428, 0.2537, 0.2020, 0.1957, 0.2046, 0.2106, 0.1929, 0.1976, 0.2341, 0.1811, 0.2153, 0.1512, 0.1881, 0.2178, 0.1946, 0.1687, 0.2707, 0.1851, 0.2033, 0.1549, 0.1754, 0.1499, 0.1805, 0.1655, 0.1955, 0.2288, 0.2176, 0.1752, 0.2175, 0.1833, 0.1736, 0.1755, 0.1446, 0.1765, 0.1607, 0.1549, 0.1487, 0.1449, 0.1753, 0.2184, 0.1815, 0.1721, 0.1312, 0.1418, 0.1447, 0.1325, 0.1433, 0.1610, 0.1770, 0.1990, 0.1914, 0.1658, 0.1363, 0.1377, 0.1676, 0.1668, 0.3071, 0.2467, 0.1865, 0.1735, 0.1725, 0.1443, 0.1307, 0.1374, 0.1521, 0.1462, 0.1440, 0.1126, 0.1366, 0.1510, 0.1441, 0.1033, 0.1131, 0.1484, 0.1478, 0.1457, 0.1398, 0.1552, 0.1355, 0.1584, 0.1289, 0.1383, 0.1196, 0.1573, 0.1121, 0.1136, 0.1872, 0.1496, 0.1433, 0.1433, 0.1205, 0.1051, 0.1245, 0.1453, 0.1017, 0.1071, 0.2045, 0.1431, 0.1454, 0.1353, 0.1110, 0.0967, 0.1339, 0.3612, 0.4259, 0.1886, 0.1942, 0.1469, 0.1404, 0.1179, 0.1232, 0.1284, 0.1131, 0.1041, 0.1185, 0.1569, 0.1342, 0.1176, 0.1056, 0.2202, 0.1420, 0.1243, 0.1304, 0.1066, 0.1179, 0.1380, 0.1020, 0.1043, 0.1270, 0.1150, 0.1146, 0.1226, 0.1027, 0.0971, 0.0990, 0.1776, 0.1387, 0.1192, 0.1174, 0.0896, 0.0832, 0.0988, 0.1021, 0.1239, 0.1494, 0.1653, 0.1174, 0.1200, 0.1042, 0.1107, 0.0994, 0.0715, 0.0686, 0.0859, 0.0946, 0.1318, 0.1019, 0.1050, 0.1298, 0.0876, 0.1175, 0.1529, 0.1527, 0.1260, 0.0994, 0.2006, 0.1104, 0.1209, 0.1010, 0.0951, 0.0909, 0.1125, 0.0960, 0.1005, 0.0841, 0.0750, 0.0809, 0.0839, 0.0856, 0.0813, 0.1060, 0.1767, 0.1540, 0.1273, 0.1147, 0.0923, 0.0811, 0.0896, 0.0871, 0.1100, 0.1326, 0.1082, 0.0925, 0.1209, 0.0914, 0.0922, 0.0719, 0.0882, 0.0739, 0.0821, 0.1045, 0.0956, 0.0926, 0.0979, 0.1563, 0.0877, 0.0823, 0.0971, 0.0814, 0.1159, 0.1288, 0.1330, 0.0786, 0.0697, 0.0866, 0.1047, 0.1261, 0.1061, 0.0802, 0.1023, 0.0932, 0.1086, 0.0861, 0.0907, 0.0882, 0.1160, 0.0742, 0.0673, 0.0595, 0.1071, 0.2093, 0.1134, 0.0735, 0.0675, 0.0838, 0.0881, 0.0770, 0.0937, 0.0871, 0.0671, 0.1064, 0.0952, 0.1090, 0.1039, 0.0837, 0.0961, 0.0949, 0.0861, 0.0679, 0.0715, 0.0883, 0.0730, 0.1052, 0.0704, 0.0622, 0.0813, 0.2864, 0.1626, 0.1054, 0.0917, 0.0890, 0.0874, 0.0977, 0.0868, 0.0993, 0.0732, 0.0933, 0.0935, 0.0899, 0.0751, 0.1130, 0.0774, 0.0793, 0.1038, 0.0753, 0.0609, 0.1610, 0.1146, 0.1078, 0.1114, 0.0809, 0.0728, 0.0798, 0.0666, 0.0857, 0.0626, 0.1272, 0.1097, 0.0842, 0.0751, 0.0762, 0.0995, 0.0587, 0.0513, 0.0788, 0.0888, 0.0567, 0.0869, 0.0733, 0.0830, 0.0805, 0.0615, 0.0644, 0.0692, 0.0712, 0.1631, 0.1309, 0.0950, 0.0597, 0.0624, 0.0554, 0.0721, 0.1005, 0.0675, 0.0794, 0.0913, 0.1087, 0.0674, 0.0635, 0.0700, 0.0909, 0.0743, 0.0771, 0.0792, 0.0591, 0.0688, 0.0720, 0.0583, 0.0747, 0.1547, 0.1031, 0.0987, 0.0873, 0.0671, 0.0612, 0.0647, 0.0742, 0.0644, 0.0659, 0.0556, 0.0956, 0.0748, 0.0772, 0.0741, 0.1291, 0.1418, 0.0980, 0.0723, 0.0653, 0.0632, 0.0578, 0.0547, 0.0567, 0.0609, 0.0489, 0.1008, 0.0986, 0.0789, 0.0746, 0.0636, 0.0597, 0.0507, 0.0556, 0.0741, 0.0778, 0.0776, 0.0553, 0.0670, 0.0861, 0.0824, 0.0664, 0.0630, 0.0818, 0.0520, 0.0597, 0.0772, 0.0729, 0.0500, 0.0884, 0.1213, 0.0800, 0.0859, 0.0715, 0.0594, 0.0486, 0.0420, 0.0521, 0.1149, 0.0849, 0.0735, 0.0652, 0.0720, 0.0759, 0.0553, 0.0540, 0.0484, 0.0469, 0.0676, 0.1173, 0.1077, 0.0728, 0.0583, 0.0607, 0.0704, 0.0452, 0.0474, 0.0556, 0.0485, 0.0808, 0.0721, 0.0850, 0.0877, 0.0960, 0.0702, 0.0614, 0.0447, 0.0706, 0.0759, 0.0659, 0.0763, 0.0749, 0.0554, 0.0758, 0.0639, 0.0452, 0.0420, 0.0655, 0.0656, 0.1006, 0.0475, 0.0706, 0.0679, 0.0431, 0.0425, 0.0478, 0.0647, 0.0812, 0.1193, 0.0887, 0.0950, 0.0795, 0.0679, 0.0500, 0.0566, 0.0684, 0.1370, 0.0693, 0.0968, 0.0793, 0.0648, 0.0547, 0.0486, 0.0423, 0.0490, 0.0788, 0.0601, 0.0427, 0.0385, 0.0597, 0.0464, 0.0921, 0.0907, 0.0577, 0.0979, 0.0556, 0.0822, 0.0723, 0.0589, 0.0523, 0.0754, 0.0629, 0.0454, 0.0413, 0.0357, 0.0528, 0.0677, 0.0837, 0.0696, 0.0510, 0.0603, 0.0586, 0.0670, 0.0631, 0.0830, 0.0644, 0.0837, 0.0832, 0.0594, 0.0495, 0.0514, 0.0582, 0.0491, 0.0566, 0.0600, 0.0494, 0.1013, 0.0731, 0.0578, 0.0421, 0.1229, 0.0747, 0.0458, 0.0552, 0.0626, 0.0673, 0.0538, 0.0348, 0.0412, 0.0475, 0.0773, 0.1001, 0.0707, 0.0701, 0.0530, 0.0477, 0.0730, 0.0797, 0.0656, 0.0673, 0.0487, 0.0502, 0.0437, 0.0369, 0.0327, 0.0414, 0.0515, 0.0311, 0.0301, 0.0559, 0.0361, 0.0882, 0.0965, 0.0684, 0.0585, 0.0624, 0.0781, 0.0508, 0.0653, 0.0483, 0.0377, 0.0693, 0.0895, 0.1605, 0.1487, 0.1036, 0.0708, 0.0777, 0.0695, 0.0593, 0.0620, 0.0694, 0.0535, 0.0546, 0.0479, 0.0441, 0.0559, 0.0583, 0.0709, 0.0541, 0.0524, 0.0354, 0.0377, 0.0557, 0.0633, 0.0639, 0.0402, 0.0457, 0.0393, 0.0358, 0.0339, 0.0402, 0.0438, 0.1167, 0.0986, 0.0915, 0.0597, 0.0651, 0.1455, 0.0863, 0.0548, 0.0699, 0.0469, 0.0459, 0.0348, 0.0315, 0.0337, 0.0335, 0.0338, 0.0856, 0.0617, 0.0623, 0.0479, 0.0402, 0.0567, 0.0430, 0.0398, 0.0898, 0.2343, 0.0820],\n",
            "           \"accuracy\": [0.0020, 0.0043, 0.0089, 0.0142, 0.0198, 0.0244, 0.0327, 0.0336, 0.0383, 0.0458, 0.0449, 0.0475, 0.0491, 0.0449, 0.0498, 0.0610, 0.0610, 0.0567, 0.0617, 0.0689, 0.0709, 0.0627, 0.0825, 0.0828, 0.0818, 0.0864, 0.0897, 0.0877, 0.0963, 0.1092, 0.1046, 0.1105, 0.1201, 0.1234, 0.1174, 0.1332, 0.1412, 0.1527, 0.1504, 0.1557, 0.1827, 0.1765, 0.2098, 0.1953, 0.2124, 0.2272, 0.2345, 0.2427, 0.2658, 0.2803, 0.2843, 0.2807, 0.3028, 0.3176, 0.3120, 0.3391, 0.3572, 0.3635, 0.3740, 0.3750, 0.3948, 0.3968, 0.4225, 0.4208, 0.4472, 0.4575, 0.4822, 0.4819, 0.5030, 0.5188, 0.5257, 0.5228, 0.5399, 0.5478, 0.5782, 0.5604, 0.5851, 0.5607, 0.5914, 0.6085, 0.6006, 0.6306, 0.6293, 0.6633, 0.6547, 0.6606, 0.6616, 0.6811, 0.6913, 0.7058, 0.7203, 0.7048, 0.7078, 0.7249, 0.7068, 0.7470, 0.7516, 0.7507, 0.7744, 0.7322, 0.7612, 0.7714, 0.8018, 0.7859, 0.7949, 0.7949, 0.7972, 0.8008, 0.8067, 0.8245, 0.8189, 0.8371, 0.8391, 0.8503, 0.8470, 0.8559, 0.8618, 0.8480, 0.8618, 0.8592, 0.8460, 0.8559, 0.8734, 0.8826, 0.8783, 0.8569, 0.8846, 0.8803, 0.8925, 0.8991, 0.8945, 0.8931, 0.8987, 0.8964, 0.8931, 0.9063, 0.9159, 0.9109, 0.9080, 0.9113, 0.9208, 0.9017, 0.9271, 0.9225, 0.9248, 0.9340, 0.9307, 0.9307, 0.9350, 0.9241, 0.9327, 0.9307, 0.9307, 0.9245, 0.9291, 0.9354, 0.9311, 0.9393, 0.9453, 0.9040, 0.9182, 0.9347, 0.9393, 0.9502, 0.9505, 0.9561, 0.9525, 0.9548, 0.9495, 0.9433, 0.9535, 0.9568, 0.9515, 0.9515, 0.9512, 0.9416, 0.9614, 0.9535, 0.9476, 0.9581, 0.9436, 0.9581, 0.9588, 0.9472, 0.9495, 0.9654, 0.9627, 0.9548, 0.9545, 0.9650, 0.9584, 0.9723, 0.9660, 0.9604, 0.9637, 0.9575, 0.9571, 0.9522, 0.9248, 0.9436, 0.9641, 0.9644, 0.9578, 0.9700, 0.9710, 0.9664, 0.9660, 0.9673, 0.9641, 0.9565, 0.9753, 0.9611, 0.9819, 0.9660, 0.9561, 0.9634, 0.9703, 0.9462, 0.9739, 0.9644, 0.9789, 0.9680, 0.9766, 0.9660, 0.9716, 0.9621, 0.9588, 0.9594, 0.9736, 0.9591, 0.9716, 0.9706, 0.9710, 0.9769, 0.9664, 0.9726, 0.9763, 0.9766, 0.9805, 0.9680, 0.9631, 0.9736, 0.9703, 0.9832, 0.9756, 0.9769, 0.9796, 0.9769, 0.9687, 0.9637, 0.9617, 0.9667, 0.9690, 0.9828, 0.9802, 0.9697, 0.9710, 0.9446, 0.9673, 0.9789, 0.9759, 0.9746, 0.9835, 0.9832, 0.9763, 0.9753, 0.9772, 0.9756, 0.9871, 0.9805, 0.9753, 0.9756, 0.9871, 0.9842, 0.9736, 0.9733, 0.9766, 0.9782, 0.9716, 0.9782, 0.9739, 0.9815, 0.9802, 0.9832, 0.9726, 0.9852, 0.9835, 0.9670, 0.9802, 0.9776, 0.9756, 0.9802, 0.9858, 0.9792, 0.9726, 0.9904, 0.9822, 0.9575, 0.9776, 0.9789, 0.9796, 0.9832, 0.9871, 0.9753, 0.9489, 0.9307, 0.9802, 0.9743, 0.9855, 0.9832, 0.9894, 0.9861, 0.9822, 0.9875, 0.9881, 0.9842, 0.9713, 0.9796, 0.9848, 0.9871, 0.9716, 0.9845, 0.9881, 0.9858, 0.9894, 0.9828, 0.9733, 0.9875, 0.9848, 0.9812, 0.9848, 0.9825, 0.9822, 0.9868, 0.9865, 0.9858, 0.9647, 0.9766, 0.9828, 0.9812, 0.9901, 0.9901, 0.9845, 0.9858, 0.9828, 0.9749, 0.9726, 0.9855, 0.9848, 0.9875, 0.9809, 0.9858, 0.9931, 0.9921, 0.9875, 0.9852, 0.9776, 0.9838, 0.9861, 0.9782, 0.9908, 0.9799, 0.9769, 0.9802, 0.9858, 0.9904, 0.9716, 0.9885, 0.9855, 0.9911, 0.9901, 0.9885, 0.9789, 0.9881, 0.9825, 0.9901, 0.9918, 0.9894, 0.9875, 0.9885, 0.9888, 0.9812, 0.9759, 0.9763, 0.9815, 0.9868, 0.9911, 0.9898, 0.9861, 0.9871, 0.9819, 0.9776, 0.9858, 0.9885, 0.9809, 0.9885, 0.9885, 0.9921, 0.9861, 0.9921, 0.9918, 0.9828, 0.9878, 0.9871, 0.9858, 0.9713, 0.9891, 0.9924, 0.9871, 0.9927, 0.9828, 0.9832, 0.9812, 0.9944, 0.9921, 0.9881, 0.9832, 0.9759, 0.9858, 0.9921, 0.9822, 0.9881, 0.9832, 0.9891, 0.9878, 0.9878, 0.9835, 0.9927, 0.9931, 0.9941, 0.9789, 0.9614, 0.9838, 0.9941, 0.9931, 0.9908, 0.9868, 0.9881, 0.9832, 0.9852, 0.9927, 0.9885, 0.9891, 0.9832, 0.9865, 0.9914, 0.9858, 0.9875, 0.9911, 0.9944, 0.9888, 0.9835, 0.9898, 0.9815, 0.9914, 0.9921, 0.9865, 0.9660, 0.9805, 0.9937, 0.9941, 0.9914, 0.9898, 0.9878, 0.9911, 0.9875, 0.9944, 0.9871, 0.9875, 0.9898, 0.9918, 0.9805, 0.9891, 0.9898, 0.9855, 0.9904, 0.9947, 0.9766, 0.9878, 0.9858, 0.9828, 0.9914, 0.9927, 0.9904, 0.9937, 0.9875, 0.9931, 0.9799, 0.9861, 0.9901, 0.9918, 0.9908, 0.9865, 0.9954, 0.9974, 0.9924, 0.9898, 0.9960, 0.9858, 0.9898, 0.9894, 0.9881, 0.9944, 0.9937, 0.9911, 0.9901, 0.9720, 0.9796, 0.9894, 0.9967, 0.9937, 0.9954, 0.9888, 0.9888, 0.9911, 0.9894, 0.9868, 0.9838, 0.9934, 0.9927, 0.9911, 0.9842, 0.9924, 0.9894, 0.9868, 0.9937, 0.9918, 0.9908, 0.9937, 0.9908, 0.9812, 0.9908, 0.9871, 0.9908, 0.9931, 0.9944, 0.9924, 0.9891, 0.9918, 0.9904, 0.9944, 0.9878, 0.9891, 0.9901, 0.9914, 0.9802, 0.9835, 0.9911, 0.9954, 0.9947, 0.9931, 0.9934, 0.9954, 0.9927, 0.9914, 0.9960, 0.9828, 0.9828, 0.9891, 0.9901, 0.9931, 0.9927, 0.9951, 0.9927, 0.9894, 0.9931, 0.9891, 0.9951, 0.9898, 0.9881, 0.9891, 0.9921, 0.9918, 0.9881, 0.9957, 0.9921, 0.9878, 0.9914, 0.9967, 0.9871, 0.9855, 0.9908, 0.9901, 0.9914, 0.9944, 0.9960, 0.9960, 0.9941, 0.9819, 0.9894, 0.9924, 0.9960, 0.9898, 0.9898, 0.9954, 0.9941, 0.9957, 0.9947, 0.9885, 0.9802, 0.9835, 0.9921, 0.9951, 0.9947, 0.9934, 0.9964, 0.9964, 0.9941, 0.9944, 0.9881, 0.9914, 0.9881, 0.9885, 0.9875, 0.9934, 0.9931, 0.9974, 0.9914, 0.9918, 0.9924, 0.9918, 0.9914, 0.9931, 0.9888, 0.9934, 0.9967, 0.9954, 0.9898, 0.9901, 0.9858, 0.9964, 0.9904, 0.9914, 0.9977, 0.9947, 0.9944, 0.9894, 0.9871, 0.9852, 0.9927, 0.9908, 0.9894, 0.9937, 0.9960, 0.9941, 0.9911, 0.9812, 0.9954, 0.9885, 0.9947, 0.9947, 0.9960, 0.9964, 0.9964, 0.9941, 0.9875, 0.9921, 0.9964, 0.9967, 0.9924, 0.9964, 0.9838, 0.9888, 0.9944, 0.9865, 0.9947, 0.9894, 0.9947, 0.9934, 0.9947, 0.9891, 0.9924, 0.9967, 0.9970, 0.9970, 0.9908, 0.9908, 0.9888, 0.9937, 0.9960, 0.9927, 0.9934, 0.9918, 0.9924, 0.9908, 0.9924, 0.9871, 0.9898, 0.9954, 0.9954, 0.9941, 0.9947, 0.9964, 0.9947, 0.9944, 0.9934, 0.9858, 0.9901, 0.9937, 0.9977, 0.9845, 0.9911, 0.9964, 0.9951, 0.9934, 0.9904, 0.9944, 0.9984, 0.9960, 0.9934, 0.9894, 0.9868, 0.9918, 0.9931, 0.9954, 0.9960, 0.9927, 0.9934, 0.9954, 0.9924, 0.9960, 0.9937, 0.9960, 0.9977, 0.9974, 0.9964, 0.9931, 0.9980, 0.9970, 0.9927, 0.9977, 0.9848, 0.9871, 0.9934, 0.9924, 0.9934, 0.9894, 0.9957, 0.9911, 0.9967, 0.9970, 0.9875, 0.9881, 0.9792, 0.9885, 0.9937, 0.9974, 0.9941, 0.9947, 0.9954, 0.9954, 0.9921, 0.9957, 0.9944, 0.9960, 0.9960, 0.9937, 0.9921, 0.9885, 0.9957, 0.9941, 0.9984, 0.9957, 0.9921, 0.9911, 0.9937, 0.9964, 0.9944, 0.9977, 0.9970, 0.9970, 0.9947, 0.9944, 0.9822, 0.9921, 0.9868, 0.9957, 0.9911, 0.9832, 0.9927, 0.9970, 0.9901, 0.9967, 0.9957, 0.9987, 0.9980, 0.9951, 0.9977, 0.9970, 0.9852, 0.9941, 0.9927, 0.9954, 0.9967, 0.9924, 0.9951, 0.9960, 0.9865, 0.9667, 0.9957],\n",
            "           \"val_loss\": [56.080, 53.138, 47.831, 45.636, 42.522, 41.849, 39.416, 38.665, 38.137, 38.156, 36.735, 35.950, 36.776, 35.982, 35.372, 34.679, 34.048, 33.895, 33.389, 33.600, 33.087, 33.884, 32.482, 32.883, 31.403, 31.219, 30.652, 32.645, 32.598, 30.359, 29.476, 29.750, 30.682, 28.372, 28.615, 28.916, 28.801, 26.944, 27.792, 26.213, 26.368, 25.916, 26.105, 25.942, 25.668, 24.431, 23.546, 23.149, 23.663, 23.029, 21.918, 23.586, 21.701, 22.014, 21.991, 20.637, 19.648, 19.153, 19.494, 18.330, 18.591, 17.987, 17.461, 17.519, 17.081, 16.810, 15.594, 16.618, 15.287, 16.559, 14.795, 15.029, 14.356, 13.907, 13.808, 14.214, 13.062, 13.794, 13.181, 12.513, 12.141, 11.849, 11.347, 11.594, 11.639, 11.302, 11.139, 10.599, 10.126, 11.125, 10.607, 0.9709, 0.9218, 12.054, 0.9554, 0.8645, 0.8704, 0.9236, 0.8492, 0.9692, 0.8608, 0.8264, 0.7818, 0.7719, 0.8318, 0.9132, 0.7743, 0.7561, 0.7349, 0.6838, 0.6951, 0.6909, 0.6459, 0.6595, 0.6639, 0.6285, 0.6162, 0.6681, 0.6230, 0.6813, 0.7115, 0.6666, 0.6194, 0.5827, 0.6707, 0.6818, 0.5747, 0.5552, 0.5562, 0.5277, 0.5460, 0.6193, 0.5403, 0.5522, 0.4909, 0.5104, 0.4997, 0.5357, 0.5172, 0.4858, 0.5061, 0.5682, 0.4560, 0.4944, 0.4593, 0.4336, 0.4493, 0.4794, 0.4744, 0.4567, 0.4616, 0.4787, 0.4280, 0.4877, 0.4996, 0.4878, 0.4173, 0.5768, 0.4711, 0.6633, 0.4934, 0.4195, 0.3983, 0.4230, 0.3935, 0.3801, 0.3953, 0.4468, 0.4296, 0.3959, 0.4025, 0.3715, 0.3818, 0.4081, 0.4854, 0.3921, 0.4166, 0.4911, 0.4145, 0.3526, 0.4208, 0.4220, 0.3817, 0.4247, 0.3915, 0.3790, 0.4466, 0.4306, 0.3488, 0.3600, 0.3994, 0.4304, 0.4257, 0.3379, 0.3394, 0.3943, 0.3922, 0.6984, 0.6423, 0.4045, 0.4511, 0.3968, 0.3916, 0.3387, 0.3469, 0.3690, 0.3809, 0.3629, 0.3986, 0.4067, 0.4352, 0.4042, 0.3773, 0.4160, 0.3769, 0.3836, 0.3947, 0.4500, 0.3770, 0.4168, 0.3723, 0.3804, 0.3579, 0.4098, 0.3341, 0.3711, 0.4064, 0.4254, 0.3918, 0.4540, 0.3784, 0.3688, 0.3564, 0.3450, 0.4080, 0.3574, 0.3505, 0.3538, 0.3645, 0.4481, 0.3996, 0.4066, 0.4060, 0.3741, 0.3663, 0.3994, 0.3626, 0.3828, 0.3675, 0.3762, 0.3700, 0.3607, 0.3904, 0.3752, 0.3539, 0.3776, 0.3978, 0.4379, 0.3968, 0.3425, 0.3819, 0.3460, 0.3675, 0.3639, 0.3531, 0.3925, 0.3698, 0.3348, 0.3136, 0.3569, 0.4013, 0.3293, 0.3443, 0.3553, 0.3662, 0.3771, 0.4049, 0.3955, 0.3929, 0.4211, 0.3867, 0.3658, 0.3515, 0.3480, 0.3463, 0.3217, 0.3294, 0.3549, 0.3323, 0.3865, 0.3372, 0.3629, 0.3291, 0.3468, 0.3901, 0.3565, 0.3509, 0.3452, 0.3309, 0.3521, 0.3723, 0.3290, 0.3590, 0.3597, 0.7917, 0.4228, 0.3956, 0.3606, 0.3505, 0.3255, 0.3163, 0.3152, 0.3406, 0.3023, 0.3244, 0.3452, 0.3802, 0.3065, 0.3075, 0.3382, 0.3611, 0.3134, 0.3411, 0.3471, 0.3212, 0.2983, 0.3161, 0.3037, 0.3230, 0.2915, 0.2948, 0.3015, 0.3064, 0.2887, 0.3187, 0.3482, 0.3471, 0.3133, 0.3532, 0.3086, 0.2958, 0.2933, 0.3378, 0.3708, 0.3764, 0.3883, 0.3254, 0.3076, 0.3443, 0.3278, 0.3154, 0.3240, 0.3213, 0.3423, 0.2749, 0.3214, 0.3108, 0.3207, 0.3544, 0.3160, 0.2920, 0.3149, 0.3860, 0.3402, 0.3478, 0.3265, 0.3246, 0.3403, 0.3445, 0.3370, 0.3494, 0.3041, 0.3834, 0.3429, 0.3661, 0.3079, 0.2614, 0.3120, 0.2973, 0.3248, 0.3248, 0.3257, 0.2873, 0.3054, 0.4064, 0.2495, 0.2663, 0.2614, 0.2765, 0.2819, 0.3541, 0.3263, 0.2858, 0.3176, 0.2844, 0.2524, 0.2395, 0.2775, 0.2765, 0.2668, 0.2766, 0.3825, 0.2597, 0.3099, 0.3618, 0.2832, 0.3166, 0.2875, 0.2994, 0.2709, 0.2789, 0.3208, 0.3011, 0.2775, 0.2875, 0.2637, 0.2943, 0.2619, 0.2902, 0.2662, 0.3541, 0.2788, 0.3162, 0.3226, 0.3021, 0.3241, 0.2859, 0.2595, 0.2923, 0.2629, 0.4035, 0.3053, 0.2978, 0.2877, 0.2611, 0.2772, 0.2547, 0.2778, 0.2470, 0.2930, 0.2502, 0.2812, 0.2710, 0.2897, 0.2863, 0.3051, 0.2899, 0.3065, 0.2811, 0.3032, 0.2627, 0.3055, 0.2615, 0.3099, 0.3289, 0.2572, 0.2878, 0.3765, 0.2938, 0.3066, 0.2610, 0.2625, 0.2790, 0.2676, 0.2585, 0.3198, 0.2950, 0.3391, 0.3138, 0.2749, 0.2796, 0.3066, 0.2782, 0.3200, 0.2988, 0.2876, 0.3009, 0.3317, 0.2920, 0.3139, 0.3234, 0.2754, 0.2929, 0.2706, 0.3007, 0.3054, 0.3187, 0.3879, 0.2934, 0.3437, 0.2967, 0.2858, 0.3078, 0.2989, 0.3024, 0.3142, 0.3203, 0.3052, 0.3089, 0.3230, 0.2989, 0.2671, 0.2890, 0.2999, 0.2951, 0.3976, 0.4542, 0.3214, 0.2874, 0.2720, 0.2534, 0.2399, 0.3334, 0.2829, 0.2567, 0.2463, 0.2833, 0.2470, 0.2559, 0.2618, 0.2764, 0.2939, 0.2622, 0.2517, 0.2773, 0.2558, 0.2608, 0.2631, 0.2376, 0.2979, 0.3124, 0.2780, 0.3168, 0.2945, 0.2754, 0.2678, 0.2732, 0.3074, 0.2672, 0.2619, 0.2985, 0.3123, 0.2599, 0.3034, 0.3098, 0.3853, 0.3069, 0.2614, 0.2431, 0.2891, 0.2761, 0.2686, 0.2751, 0.2323, 0.2608, 0.2627, 0.3541, 0.3192, 0.2697, 0.3021, 0.2488, 0.2592, 0.2858, 0.3008, 0.3030, 0.2792, 0.2999, 0.2843, 0.2713, 0.2980, 0.2767, 0.3401, 0.2781, 0.2721, 0.2495, 0.2584, 0.3303, 0.3198, 0.2633, 0.3131, 0.2935, 0.3112, 0.3043, 0.2638, 0.2669, 0.2832, 0.2697, 0.4618, 0.2933, 0.3593, 0.3565, 0.2907, 0.3124, 0.3421, 0.2881, 0.2766, 0.2830, 0.2608, 0.3085, 0.3094, 0.2735, 0.2599, 0.2930, 0.3005, 0.2657, 0.2460, 0.2400, 0.2705, 0.2950, 0.3145, 0.3102, 0.2993, 0.3875, 0.3122, 0.2614, 0.2544, 0.2629, 0.3372, 0.3134, 0.2976, 0.2911, 0.2348, 0.2520, 0.2879, 0.2448, 0.2670, 0.2669, 0.2834, 0.2645, 0.2609, 0.2655, 0.2758, 0.2496, 0.2642, 0.2621, 0.2295, 0.2717, 0.2809, 0.2629, 0.3263, 0.2691, 0.2878, 0.2688, 0.2738, 0.2477, 0.3274, 0.3011, 0.2669, 0.2983, 0.2507, 0.2572, 0.2718, 0.2596, 0.2938, 0.2476, 0.2994, 0.2411, 0.2598, 0.3367, 0.3166, 0.3286, 0.3323, 0.3278, 0.2711, 0.2907, 0.3006, 0.2771, 0.2600, 0.2610, 0.2571, 0.2912, 0.2624, 0.2872, 0.2454, 0.2753, 0.2664, 0.2624, 0.2787, 0.2794, 0.2598, 0.2858, 0.2830, 0.2832, 0.3520, 0.2368, 0.2184, 0.3478, 0.2701, 0.2655, 0.2540, 0.2495, 0.2854, 0.2620, 0.2698, 0.2552, 0.3175, 0.2692, 0.2912, 0.3007, 0.2386, 0.2697, 0.2352, 0.2481, 0.2310, 0.2722, 0.2557, 0.2488, 0.2423, 0.2601, 0.2657, 0.2584, 0.2950, 0.2711, 0.2280, 0.2540, 0.2990, 0.2333, 0.2435, 0.2945, 0.2469, 0.2370, 0.2807, 0.2485, 0.2587, 0.2692, 0.2826, 0.2685, 0.2467, 0.2232, 0.2464, 0.2634, 0.3304, 0.2417, 0.2372, 0.2275, 0.2925, 0.2461, 0.2563, 0.2669, 0.2200, 0.2423, 0.2545, 0.2924, 0.3933, 0.2846, 0.2783, 0.2603, 0.2696, 0.2531, 0.2451, 0.2328, 0.2386, 0.2960, 0.2614, 0.2249, 0.2434, 0.2505, 0.2889, 0.2753, 0.2481, 0.2073, 0.2365, 0.2152, 0.2381, 0.2614, 0.2445, 0.2333, 0.2480, 0.2328, 0.2414, 0.2339, 0.2314, 0.2862, 0.3288, 0.2927, 0.2556, 0.3146, 0.2936, 0.3397, 0.2817, 0.2441, 0.2551, 0.2780, 0.2522, 0.2496, 0.2244, 0.2596, 0.2289, 0.2490, 0.3239, 0.2718, 0.2656, 0.2573, 0.2641, 0.2895, 0.2957, 0.3148, 0.4739, 0.3110, 0.2595],\n",
            "           \"val_accuracy\": [0.0019, 0.0056, 0.0150, 0.0169, 0.0262, 0.0243, 0.0375, 0.0356, 0.0431, 0.0375, 0.0375, 0.0412, 0.0487, 0.0356, 0.0412, 0.0506, 0.0487, 0.0543, 0.0674, 0.0581, 0.0543, 0.0524, 0.0599, 0.0880, 0.0730, 0.0918, 0.0861, 0.0805, 0.0674, 0.0899, 0.1199, 0.1330, 0.1049, 0.1479, 0.1442, 0.1742, 0.1311, 0.1910, 0.1573, 0.2453, 0.2116, 0.2210, 0.2135, 0.2079, 0.2097, 0.2584, 0.2828, 0.3315, 0.2734, 0.3052, 0.3015, 0.2472, 0.3221, 0.3146, 0.2978, 0.3933, 0.4045, 0.4382, 0.3933, 0.4476, 0.4532, 0.4345, 0.4794, 0.4663, 0.4757, 0.5000, 0.5262, 0.5094, 0.5749, 0.4963, 0.5730, 0.5506, 0.5787, 0.6086, 0.5768, 0.5843, 0.6049, 0.6180, 0.6067, 0.6367, 0.6386, 0.6648, 0.7191, 0.7154, 0.6592, 0.6966, 0.7191, 0.7416, 0.7303, 0.7172, 0.7079, 0.7547, 0.7753, 0.6742, 0.7697, 0.8165, 0.8127, 0.7790, 0.7809, 0.7678, 0.8052, 0.8296, 0.8352, 0.8652, 0.7940, 0.7547, 0.8258, 0.8352, 0.8558, 0.8652, 0.8727, 0.8708, 0.8783, 0.8745, 0.8652, 0.8801, 0.8764, 0.8689, 0.8596, 0.8596, 0.8764, 0.8876, 0.8914, 0.9082, 0.8464, 0.8820, 0.8933, 0.8970, 0.9120, 0.9176, 0.9082, 0.8801, 0.9064, 0.8989, 0.9176, 0.9307, 0.9251, 0.8933, 0.9195, 0.9157, 0.8989, 0.9120, 0.9213, 0.9251, 0.9326, 0.9401, 0.9270, 0.9307, 0.9307, 0.9232, 0.9157, 0.9345, 0.9288, 0.9288, 0.9232, 0.9101, 0.9494, 0.9195, 0.9213, 0.9026, 0.9419, 0.9419, 0.9476, 0.9382, 0.9401, 0.9532, 0.9551, 0.9401, 0.9363, 0.9401, 0.9251, 0.9551, 0.9382, 0.9438, 0.9120, 0.9382, 0.9363, 0.9213, 0.9382, 0.9476, 0.9401, 0.9476, 0.9532, 0.9419, 0.9438, 0.9419, 0.9157, 0.9288, 0.9382, 0.9476, 0.9494, 0.9438, 0.9307, 0.9494, 0.9513, 0.9438, 0.9382, 0.8914, 0.9026, 0.9513, 0.9401, 0.9457, 0.9513, 0.9551, 0.9513, 0.9457, 0.9476, 0.9532, 0.9476, 0.9401, 0.9419, 0.9457, 0.9532, 0.9401, 0.9494, 0.9382, 0.9569, 0.9419, 0.9569, 0.9401, 0.9551, 0.9401, 0.9476, 0.9419, 0.9607, 0.9438, 0.9438, 0.9345, 0.9438, 0.9270, 0.9438, 0.9532, 0.9513, 0.9532, 0.9419, 0.9569, 0.9438, 0.9569, 0.9532, 0.9438, 0.9551, 0.9532, 0.9513, 0.9532, 0.9551, 0.9457, 0.9569, 0.9457, 0.9494, 0.9569, 0.9551, 0.9588, 0.9438, 0.9438, 0.9382, 0.9494, 0.9476, 0.9438, 0.9532, 0.9513, 0.9532, 0.9607, 0.9532, 0.9532, 0.9551, 0.9401, 0.9532, 0.9588, 0.9625, 0.9551, 0.9345, 0.9551, 0.9551, 0.9476, 0.9494, 0.9476, 0.9551, 0.9419, 0.9494, 0.9476, 0.9532, 0.9513, 0.9513, 0.9532, 0.9457, 0.9569, 0.9569, 0.9513, 0.9551, 0.9476, 0.9625, 0.9513, 0.9513, 0.9569, 0.9551, 0.9569, 0.9457, 0.9532, 0.9551, 0.9532, 0.9494, 0.9569, 0.9513, 0.9551, 0.8876, 0.9457, 0.9513, 0.9476, 0.9644, 0.9569, 0.9588, 0.9569, 0.9551, 0.9588, 0.9551, 0.9607, 0.9419, 0.9588, 0.9607, 0.9532, 0.9569, 0.9607, 0.9513, 0.9532, 0.9607, 0.9625, 0.9588, 0.9569, 0.9569, 0.9588, 0.9532, 0.9663, 0.9625, 0.9607, 0.9494, 0.9438, 0.9588, 0.9644, 0.9551, 0.9644, 0.9588, 0.9644, 0.9476, 0.9438, 0.9476, 0.9457, 0.9625, 0.9625, 0.9588, 0.9513, 0.9588, 0.9569, 0.9607, 0.9607, 0.9625, 0.9607, 0.9551, 0.9569, 0.9513, 0.9607, 0.9607, 0.9569, 0.9532, 0.9551, 0.9569, 0.9551, 0.9625, 0.9588, 0.9607, 0.9588, 0.9551, 0.9588, 0.9513, 0.9569, 0.9494, 0.9588, 0.9663, 0.9607, 0.9569, 0.9607, 0.9644, 0.9607, 0.9532, 0.9682, 0.9345, 0.9663, 0.9644, 0.9607, 0.9588, 0.9588, 0.9494, 0.9569, 0.9644, 0.9513, 0.9607, 0.9644, 0.9682, 0.9625, 0.9607, 0.9607, 0.9588, 0.9569, 0.9625, 0.9532, 0.9419, 0.9607, 0.9607, 0.9588, 0.9569, 0.9644, 0.9625, 0.9551, 0.9663, 0.9663, 0.9625, 0.9607, 0.9644, 0.9700, 0.9569, 0.9625, 0.9551, 0.9588, 0.9569, 0.9588, 0.9625, 0.9476, 0.9607, 0.9663, 0.9663, 0.9625, 0.9438, 0.9625, 0.9607, 0.9644, 0.9607, 0.9644, 0.9663, 0.9644, 0.9644, 0.9588, 0.9663, 0.9682, 0.9644, 0.9644, 0.9625, 0.9532, 0.9644, 0.9700, 0.9644, 0.9644, 0.9644, 0.9644, 0.9607, 0.9644, 0.9663, 0.9700, 0.9663, 0.9644, 0.9663, 0.9682, 0.9700, 0.9682, 0.9625, 0.9682, 0.9682, 0.9663, 0.9625, 0.9551, 0.9682, 0.9682, 0.9663, 0.9625, 0.9682, 0.9588, 0.9700, 0.9682, 0.9663, 0.9532, 0.9738, 0.9682, 0.9607, 0.9738, 0.9588, 0.9682, 0.9663, 0.9644, 0.9663, 0.9494, 0.9569, 0.9476, 0.9625, 0.9625, 0.9663, 0.9663, 0.9625, 0.9625, 0.9644, 0.9663, 0.9682, 0.9663, 0.9663, 0.9700, 0.9663, 0.9551, 0.9700, 0.9513, 0.9288, 0.9644, 0.9682, 0.9719, 0.9719, 0.9700, 0.9588, 0.9700, 0.9682, 0.9719, 0.9700, 0.9719, 0.9682, 0.9663, 0.9644, 0.9644, 0.9644, 0.9738, 0.9663, 0.9700, 0.9719, 0.9700, 0.9663, 0.9644, 0.9700, 0.9700, 0.9663, 0.9700, 0.9700, 0.9719, 0.9738, 0.9663, 0.9738, 0.9719, 0.9607, 0.9588, 0.9682, 0.9700, 0.9607, 0.9476, 0.9644, 0.9682, 0.9738, 0.9682, 0.9700, 0.9719, 0.9738, 0.9738, 0.9700, 0.9663, 0.9569, 0.9607, 0.9663, 0.9625, 0.9719, 0.9719, 0.9588, 0.9625, 0.9663, 0.9663, 0.9644, 0.9682, 0.9588, 0.9682, 0.9700, 0.9569, 0.9644, 0.9663, 0.9682, 0.9700, 0.9663, 0.9644, 0.9682, 0.9532, 0.9625, 0.9644, 0.9719, 0.9682, 0.9738, 0.9625, 0.9682, 0.9363, 0.9682, 0.9625, 0.9551, 0.9625, 0.9644, 0.9625, 0.9663, 0.9757, 0.9644, 0.9700, 0.9682, 0.9663, 0.9663, 0.9663, 0.9682, 0.9625, 0.9663, 0.9682, 0.9700, 0.9663, 0.9625, 0.9663, 0.9719, 0.9700, 0.9551, 0.9682, 0.9682, 0.9757, 0.9663, 0.9625, 0.9625, 0.9625, 0.9625, 0.9700, 0.9700, 0.9663, 0.9682, 0.9644, 0.9682, 0.9607, 0.9700, 0.9700, 0.9700, 0.9663, 0.9644, 0.9607, 0.9700, 0.9719, 0.9625, 0.9700, 0.9719, 0.9588, 0.9738, 0.9682, 0.9700, 0.9663, 0.9700, 0.9551, 0.9719, 0.9700, 0.9700, 0.9663, 0.9682, 0.9663, 0.9700, 0.9663, 0.9700, 0.9663, 0.9719, 0.9700, 0.9607, 0.9663, 0.9607, 0.9607, 0.9663, 0.9663, 0.9663, 0.9682, 0.9775, 0.9738, 0.9700, 0.9663, 0.9625, 0.9625, 0.9644, 0.9700, 0.9663, 0.9682, 0.9644, 0.9682, 0.9682, 0.9663, 0.9700, 0.9588, 0.9551, 0.9513, 0.9719, 0.9719, 0.9569, 0.9682, 0.9644, 0.9644, 0.9607, 0.9625, 0.9644, 0.9682, 0.9700, 0.9607, 0.9738, 0.9625, 0.9644, 0.9738, 0.9625, 0.9700, 0.9682, 0.9700, 0.9644, 0.9700, 0.9719, 0.9682, 0.9625, 0.9588, 0.9682, 0.9644, 0.9663, 0.9700, 0.9644, 0.9663, 0.9682, 0.9719, 0.9625, 0.9644, 0.9682, 0.9682, 0.9700, 0.9644, 0.9682, 0.9607, 0.9663, 0.9700, 0.9719, 0.9757, 0.9700, 0.9532, 0.9757, 0.9738, 0.9700, 0.9588, 0.9700, 0.9663, 0.9663, 0.9682, 0.9663, 0.9588, 0.9607, 0.9644, 0.9775, 0.9700, 0.9682, 0.9682, 0.9700, 0.9738, 0.9738, 0.9700, 0.9700, 0.9625, 0.9757, 0.9719, 0.9719, 0.9682, 0.9663, 0.9700, 0.9775, 0.9757, 0.9738, 0.9738, 0.9625, 0.9738, 0.9738, 0.9700, 0.9738, 0.9663, 0.9700, 0.9682, 0.9663, 0.9644, 0.9700, 0.9682, 0.9625, 0.9644, 0.9607, 0.9682, 0.9682, 0.9644, 0.9625, 0.9682, 0.9700, 0.9719, 0.9625, 0.9738, 0.9700, 0.9588, 0.9719, 0.9682, 0.9719, 0.9682, 0.9625, 0.9719, 0.9569, 0.9345, 0.9700, 0.9682]\n",
            "           }\n",
            "'''\n",
            "history = {\n",
            "    \"loss\": [57.769, 47.981, 41.037, 38.852, 37.162, 36.341, 35.837, 35.291, 34.761, 34.301, 34.167, 33.650, 33.463, 33.047, 32.810, 32.671, 31.847, 31.718, 31.120, 30.759, 30.229, 30.779, 29.787, 29.365, 27.695, 26.747, 26.251, 25.524, 24.937, 24.433, 23.719, 22.992, 22.382, 21.780, 21.169, 20.990, 21.748, 19.787, 19.454, 19.084, 18.302, 17.907, 17.014, 16.451, 15.742, 15.521, 14.772, 14.716, 14.043, 13.691, 13.042, 12.538, 12.478, 11.911, 11.351, 11.181, 11.022, 10.518, 10.077, 0.9957, 0.9823, 0.9630, 0.8761, 0.8779, 0.8332, 0.7962, 0.7912, 0.7568, 0.7256, 0.7349, 0.7058, 0.7440, 0.6732, 0.6305, 0.6190, 0.6195, 0.6192, 0.5759, 0.5838, 0.5596, 0.5769, 0.5250, 0.5305, 0.5204, 0.4958, 0.5382, 0.5228, 0.4891, 0.4438, 0.4732, 0.5681, 0.4518, 0.4498, 0.4049, 0.4677, 0.4500, 0.4246, 0.3993, 0.4274, 0.4125, 0.3813, 0.3971, 0.3766, 0.3844, 0.3997, 0.3651, 0.3741, 0.3813, 0.3586, 0.3493, 0.3584, 0.3673, 0.3590, 0.3597, 0.3504, 0.3303, 0.3588, 0.3262, 0.3163, 0.3014, 0.2915, 0.3087, 0.3288, 0.3068, 0.3364, 0.3476, 0.4553, 0.3920, 0.3384, 0.3187, 0.3039, 0.3300, 0.3383, 0.3006, 0.2741, 0.2710, 0.2702, 0.2682, 0.2943, 0.2532, 0.2731, 0.2671, 0.2796, 0.2624, 0.2545, 0.2574, 0.2467, 0.2483, 0.2553, 0.2657, 0.2824, 0.2577, 0.2523, 0.2601, 0.2279, 0.2288, 0.2334, 0.2435, 0.2413, 0.2623, 0.2142, 0.2218, 0.2371, 0.2753, 0.2183, 0.2189, 0.2406, 0.2616, 0.2338, 0.2477, 0.2100, 0.2284, 0.2261, 0.2297, 0.2256, 0.2115, 0.2218, 0.1985, 0.2510, 0.2054, 0.2223, 0.2689, 0.2042, 0.1865, 0.1921, 0.2033, 0.2061, 0.1930, 0.2307, 0.1909, 0.2301, 0.2244, 0.1850, 0.1833, 0.2618, 0.2041, 0.2072, 0.2042, 0.2362, 0.1903, 0.1919, 0.1970, 0.2332, 0.2080, 0.1784, 0.1871, 0.2020, 0.1704, 0.1898, 0.2319, 0.1888, 0.1646, 0.1719, 0.2010, 0.1881, 0.1793, 0.1919, 0.1823, 0.1655, 0.1832, 0.1792, 0.1939, 0.1831, 0.2001, 0.1932, 0.1837, 0.1883, 0.2048, 0.1690, 0.1737, 0.2324, 0.1624, 0.1607, 0.1848, 0.1687, 0.1783, 0.1985, 0.2269, 0.1700, 0.1448, 0.1602, 0.1634, 0.1839, 0.1795, 0.1805, 0.1956, 0.1593, 0.1545, 0.1662, 0.1571, 0.2056, 0.1543, 0.1616, 0.1731, 0.2528, 0.2400, 0.1725, 0.1583, 0.1466, 0.1522, 0.1723, 0.1622, 0.1578, 0.1442, 0.1460, 0.1556, 0.1652, 0.1835, 0.1703, 0.1739, 0.1740, 0.1597, 0.1458, 0.1693, 0.1582, 0.1447, 0.1669, 0.1967, 0.1799, 0.1578, 0.1567, 0.1414, 0.1572, 0.1543, 0.1478, 0.1935, 0.1605, 0.1546, 0.1522, 0.1578, 0.1515, 0.1316, 0.1291, 0.1744, 0.1845, 0.1284, 0.1486, 0.1397, 0.1394, 0.1455, 0.1795, 0.1688, 0.1533, 0.1411, 0.1324, 0.1956, 0.1770, 0.1595, 0.1338, 0.1376, 0.1195, 0.1250, 0.1472, 0.1557, 0.1505, 0.1533, 0.1703, 0.1660, 0.1462, 0.1286, 0.1759, 0.1365, 0.1580, 0.1472, 0.1326, 0.1409, 0.1271, 0.1326, 0.1377, 0.1364, 0.1254, 0.1379, 0.1587, 0.1541, 0.1400, 0.1265, 0.1582, 0.1672, 0.1922, 0.1359, 0.1350, 0.1321, 0.1371, 0.1333, 0.1303, 0.1414, 0.1447, 0.1133, 0.1158, 0.1480, 0.1368, 0.1644, 0.1670, 0.1398, 0.1698, 0.1325, 0.1365, 0.1288, 0.1344, 0.1239, 0.1147, 0.1588, 0.1576, 0.1286, 0.1359, 0.1202, 0.1436, 0.1474, 0.1393, 0.1174, 0.1274, 0.1413, 0.1330, 0.1195, 0.1372, 0.1208, 0.1271, 0.1530, 0.1173, 0.1128, 0.1163, 0.1453, 0.1449, 0.1181, 0.1332, 0.1684, 0.1401, 0.1361, 0.1247, 0.1226, 0.1491, 0.1151, 0.1162, 0.1188, 0.1298, 0.1322, 0.1288, 0.1308, 0.1253, 0.1145, 0.1018, 0.1553, 0.1204, 0.1332, 0.1074, 0.1087, 0.1344, 0.1148, 0.1040, 0.1060, 0.1269, 0.1280, 0.1231, 0.1299, 0.1335, 0.1170, 0.1097, 0.1426, 0.1308, 0.1182, 0.1384, 0.1560, 0.1089, 0.1138, 0.1208, 0.1205, 0.1086, 0.1136, 0.1303, 0.1251, 0.1104, 0.1145, 0.1363, 0.1311, 0.1050, 0.1212, 0.1276, 0.1169, 0.1277, 0.1333, 0.1098, 0.1113, 0.1392, 0.1612, 0.1177, 0.1221, 0.1216, 0.1017, 0.1262, 0.1275, 0.1064, 0.1052, 0.1014, 0.1327, 0.1198, 0.1194, 0.1014, 0.0992, 0.1120, 0.1095, 0.1262, 0.1190, 0.1297, 0.1212, 0.1085, 0.0960, 0.1312, 0.1115, 0.1009, 0.1077, 0.1021, 0.1070, 0.1194, 0.1055, 0.1056, 0.1039, 0.1205, 0.0945, 0.1308, 0.1236, 0.1048, 0.1084, 0.1140, 0.1093, 0.1039, 0.1034, 0.1205, 0.1043, 0.1180, 0.1322, 0.1245, 0.1018, 0.0926, 0.1246, 0.1082, 0.0920, 0.1221, 0.1160, 0.1007, 0.1020, 0.1102, 0.1009, 0.0985, 0.1228, 0.1139, 0.0906, 0.0921, 0.1156, 0.1165, 0.0981, 0.1045, 0.1119, 0.1089, 0.1120, 0.1384, 0.0983, 0.1147, 0.1042, 0.1175, 0.0942, 0.1071, 0.0971, 0.1009, 0.0988, 0.0914, 0.1543, 0.1105, 0.1134, 0.1081, 0.0868, 0.0879, 0.0949, 0.1020, 0.1062, 0.0891, 0.1175, 0.0980, 0.1008, 0.1141, 0.1077, 0.1288, 0.1016, 0.1391, 0.0897, 0.1108, 0.0925, 0.0954, 0.1240, 0.0895, 0.1033, 0.0831, 0.0920, 0.0841, 0.1280, 0.0913, 0.1187, 0.1011, 0.0889, 0.0893, 0.1046, 0.1231, 0.0971, 0.0966, 0.0998, 0.1118, 0.1011, 0.0937, 0.1331, 0.1226, 0.0946, 0.0900, 0.0809, 0.1007, 0.1145, 0.0958, 0.0858, 0.1076, 0.1000, 0.0865, 0.1264, 0.0798, 0.0847, 0.0925, 0.0943, 0.0850, 0.0939, 0.0884, 0.0869, 0.1129, 0.1017, 0.1185, 0.1088, 0.0849, 0.1021, 0.1089, 0.0937, 0.0891, 0.1089, 0.0953, 0.1205, 0.0905, 0.0802, 0.1087, 0.1042, 0.0799, 0.1077, 0.1137, 0.0893, 0.0939, 0.0882, 0.0891, 0.1093, 0.1045, 0.0889, 0.0982, 0.0857, 0.0908, 0.0995, 0.1228, 0.0932, 0.1064, 0.0892, 0.0838, 0.0963, 0.1040, 0.1115, 0.0933, 0.1065, 0.0893, 0.0866, 0.0805, 0.0951, 0.0963, 0.0843, 0.1009, 0.0829, 0.0855, 0.0721, 0.1006, 0.0911, 0.1269, 0.0914, 0.0770, 0.0938, 0.0945, 0.0968, 0.1544, 0.1147, 0.0826, 0.0827, 0.0836, 0.0815, 0.0773, 0.0988, 0.0995, 0.1033, 0.0915, 0.0787, 0.0798, 0.1105, 0.0899, 0.0853, 0.0907, 0.0888, 0.1449, 0.1019, 0.0917, 0.0948, 0.1073, 0.0989, 0.0852, 0.0828, 0.0738, 0.0926, 0.1081, 0.1034, 0.0789, 0.0807, 0.0793, 0.0918, 0.0734, 0.0779, 0.1063, 0.0800, 0.0889, 0.1000, 0.0825, 0.0862, 0.1046, 0.1222, 0.0886, 0.0748, 0.0889, 0.0970, 0.0689, 0.0574, 0.1175, 0.0798, 0.0747, 0.0978, 0.1006, 0.0881, 0.0896, 0.0898, 0.1008, 0.0965, 0.1111, 0.0907, 0.1146, 0.1039, 0.0928, 0.0678, 0.0839, 0.0933, 0.0928, 0.0862, 0.0852, 0.0688, 0.1007, 0.0957, 0.0758, 0.0862, 0.0915, 0.0821, 0.1047, 0.0792, 0.0866, 0.0797, 0.0701, 0.0802, 0.1094, 0.0752, 0.0911, 0.0928, 0.1206, 0.0752, 0.0746, 0.0682, 0.0787, 0.0864, 0.0866, 0.0978, 0.0766, 0.0705, 0.0700, 0.0915, 0.0723, 0.0839, 0.1027, 0.0954, 0.0786, 0.1000, 0.0873, 0.0809, 0.0789, 0.0835, 0.1038, 0.0748, 0.0616, 0.1008, 0.0873, 0.0818, 0.0877, 0.0927, 0.0937, 0.0719, 0.0892, 0.0798, 0.0877, 0.0837, 0.0707, 0.0852, 0.0836, 0.0894, 0.0812, 0.0805, 0.0839, 0.0660, 0.0857, 0.0841, 0.0805, 0.0860, 0.0720, 0.0773, 0.0797, 0.0868, 0.0758, 0.0908, 0.1174, 0.0957, 0.0769, 0.0741, 0.1099, 0.0730, 0.0664, 0.0943, 0.0835, 0.0883, 0.0825, 0.0809, 0.0861, 0.0997, 0.0778, 0.0753, 0.0713, 0.0786, 0.1210, 0.0843, 0.0702, 0.0937, 0.0777, 0.0815, 0.0757, 0.0668, 0.0956, 0.0838, 0.0780, 0.0675, 0.0828, 0.0896, 0.0859, 0.0711, 0.0780, 0.0744, 0.0669, 0.0954, 0.1008, 0.0821, 0.1108, 0.0785, 0.0633, 0.0816, 0.0762, 0.0630, 0.0854, 0.0898, 0.0818, 0.0761, 0.0949, 0.0701, 0.0738, 0.1112, 0.0821, 0.0753, 0.0770, 0.0935, 0.0778, 0.0721, 0.0778, 0.0776, 0.0859, 0.0918, 0.1001, 0.0750, 0.0794, 0.0864, 0.0739, 0.0747, 0.0754, 0.0736, 0.0805, 0.0798, 0.0809, 0.0833, 0.0667, 0.0715, 0.1075, 0.0918, 0.0755, 0.0804, 0.0960, 0.0633, 0.0681, 0.0715, 0.0724, 0.0898, 0.0642, 0.0920, 0.0669, 0.0719, 0.1051, 0.1078, 0.1167, 0.0709, 0.0607, 0.0677, 0.0789, 0.0725, 0.0827, 0.0969, 0.0621, 0.0650, 0.0617, 0.0733, 0.1277, 0.0928, 0.0706, 0.0785, 0.0707, 0.0684, 0.0616, 0.0716, 0.0698, 0.0956, 0.0824, 0.0664, 0.0768, 0.0832, 0.0611, 0.0789, 0.0834, 0.0697, 0.0886, 0.0805, 0.0796, 0.0778, 0.1044, 0.0667, 0.1134, 0.0822, 0.0848, 0.0627, 0.0606, 0.0762],\n",
            "    \"accuracy\": [0.0029, 0.0205, 0.0340, 0.0380, 0.0469, 0.0500, 0.0525, 0.0501, 0.0568, 0.0631, 0.0577, 0.0631, 0.0672, 0.0735, 0.0769, 0.0773, 0.0900, 0.0935, 0.1093, 0.1112, 0.1232, 0.1328, 0.1486, 0.1539, 0.1732, 0.1995, 0.2072, 0.2261, 0.2383, 0.2560, 0.2777, 0.2855, 0.2997, 0.3203, 0.3336, 0.3386, 0.3354, 0.3852, 0.3987, 0.4052, 0.4228, 0.4492, 0.4773, 0.4919, 0.5223, 0.5287, 0.5534, 0.5655, 0.5756, 0.5917, 0.6113, 0.6371, 0.6373, 0.6541, 0.6776, 0.6826, 0.6898, 0.7093, 0.7197, 0.7251, 0.7320, 0.7370, 0.7725, 0.7682, 0.7855, 0.7972, 0.7948, 0.8040, 0.8189, 0.8167, 0.8251, 0.8140, 0.8397, 0.8544, 0.8561, 0.8565, 0.8493, 0.8618, 0.8639, 0.8762, 0.8686, 0.8866, 0.8800, 0.8843, 0.8933, 0.8862, 0.8863, 0.8938, 0.9052, 0.8983, 0.8841, 0.9097, 0.9059, 0.9183, 0.8955, 0.9030, 0.9097, 0.9161, 0.9106, 0.9201, 0.9256, 0.9164, 0.9277, 0.9177, 0.9163, 0.9271, 0.9233, 0.9193, 0.9294, 0.9303, 0.9243, 0.9224, 0.9296, 0.9297, 0.9320, 0.9357, 0.9282, 0.9347, 0.9353, 0.9417, 0.9465, 0.9389, 0.9329, 0.9388, 0.9318, 0.9345, 0.9168, 0.9386, 0.9442, 0.9430, 0.9459, 0.9418, 0.9370, 0.9448, 0.9519, 0.9510, 0.9470, 0.9471, 0.9436, 0.9524, 0.9451, 0.9516, 0.9436, 0.9513, 0.9521, 0.9494, 0.9521, 0.9518, 0.9503, 0.9486, 0.9472, 0.9522, 0.9553, 0.9518, 0.9572, 0.9588, 0.9547, 0.9493, 0.9544, 0.9499, 0.9611, 0.9567, 0.9540, 0.9500, 0.9617, 0.9617, 0.9553, 0.9519, 0.9569, 0.9557, 0.9638, 0.9579, 0.9548, 0.9588, 0.9586, 0.9626, 0.9578, 0.9646, 0.9512, 0.9655, 0.9600, 0.9519, 0.9665, 0.9660, 0.9652, 0.9613, 0.9619, 0.9643, 0.9591, 0.9668, 0.9588, 0.9627, 0.9665, 0.9651, 0.9516, 0.9649, 0.9645, 0.9636, 0.9569, 0.9674, 0.9665, 0.9641, 0.9567, 0.9670, 0.9696, 0.9654, 0.9633, 0.9699, 0.9670, 0.9585, 0.9686, 0.9718, 0.9683, 0.9613, 0.9670, 0.9677, 0.9638, 0.9654, 0.9702, 0.9649, 0.9686, 0.9641, 0.9668, 0.9613, 0.9654, 0.9677, 0.9667, 0.9654, 0.9714, 0.9662, 0.9601, 0.9740, 0.9728, 0.9670, 0.9706, 0.9668, 0.9648, 0.9611, 0.9752, 0.9737, 0.9698, 0.9702, 0.9679, 0.9677, 0.9658, 0.9660, 0.9724, 0.9722, 0.9706, 0.9715, 0.9643, 0.9746, 0.9712, 0.9714, 0.9626, 0.9677, 0.9752, 0.9741, 0.9749, 0.9727, 0.9661, 0.9737, 0.9709, 0.9746, 0.9724, 0.9709, 0.9706, 0.9674, 0.9718, 0.9700, 0.9719, 0.9719, 0.9749, 0.9700, 0.9733, 0.9738, 0.9696, 0.9692, 0.9716, 0.9727, 0.9725, 0.9768, 0.9725, 0.9719, 0.9734, 0.9686, 0.9733, 0.9733, 0.9740, 0.9714, 0.9735, 0.9776, 0.9763, 0.9698, 0.9689, 0.9807, 0.9716, 0.9743, 0.9744, 0.9746, 0.9695, 0.9730, 0.9750, 0.9766, 0.9766, 0.9654, 0.9696, 0.9760, 0.9787, 0.9766, 0.9794, 0.9756, 0.9766, 0.9741, 0.9754, 0.9721, 0.9728, 0.9744, 0.9760, 0.9798, 0.9705, 0.9775, 0.9705, 0.9752, 0.9759, 0.9760, 0.9790, 0.9768, 0.9781, 0.9763, 0.9773, 0.9762, 0.9724, 0.9744, 0.9768, 0.9772, 0.9709, 0.9731, 0.9692, 0.9784, 0.9772, 0.9762, 0.9775, 0.9768, 0.9763, 0.9752, 0.9760, 0.9810, 0.9798, 0.9733, 0.9772, 0.9741, 0.9753, 0.9765, 0.9725, 0.9797, 0.9773, 0.9787, 0.9776, 0.9772, 0.9797, 0.9734, 0.9750, 0.9765, 0.9766, 0.9795, 0.9737, 0.9760, 0.9771, 0.9814, 0.9771, 0.9752, 0.9800, 0.9778, 0.9765, 0.9782, 0.9765, 0.9728, 0.9809, 0.9797, 0.9797, 0.9757, 0.9746, 0.9794, 0.9762, 0.9714, 0.9776, 0.9775, 0.9778, 0.9791, 0.9753, 0.9819, 0.9784, 0.9790, 0.9762, 0.9788, 0.9769, 0.9784, 0.9788, 0.9814, 0.9804, 0.9722, 0.9798, 0.9757, 0.9828, 0.9804, 0.9763, 0.9807, 0.9806, 0.9811, 0.9772, 0.9792, 0.9788, 0.9766, 0.9747, 0.9791, 0.9804, 0.9743, 0.9797, 0.9797, 0.9754, 0.9771, 0.9832, 0.9788, 0.9779, 0.9784, 0.9826, 0.9810, 0.9769, 0.9791, 0.9804, 0.9797, 0.9753, 0.9773, 0.9826, 0.9797, 0.9787, 0.9823, 0.9763, 0.9776, 0.9816, 0.9801, 0.9772, 0.9714, 0.9825, 0.9795, 0.9800, 0.9828, 0.9779, 0.9778, 0.9813, 0.9811, 0.9826, 0.9778, 0.9792, 0.9784, 0.9806, 0.9829, 0.9797, 0.9806, 0.9773, 0.9800, 0.9776, 0.9790, 0.9823, 0.9820, 0.9797, 0.9795, 0.9810, 0.9813, 0.9801, 0.9809, 0.9791, 0.9803, 0.9816, 0.9801, 0.9801, 0.9828, 0.9766, 0.9813, 0.9835, 0.9807, 0.9800, 0.9825, 0.9826, 0.9822, 0.9768, 0.9830, 0.9781, 0.9768, 0.9787, 0.9820, 0.9830, 0.9788, 0.9822, 0.9844, 0.9785, 0.9809, 0.9810, 0.9813, 0.9811, 0.9832, 0.9807, 0.9779, 0.9806, 0.9851, 0.9828, 0.9778, 0.9788, 0.9848, 0.9819, 0.9804, 0.9800, 0.9798, 0.9779, 0.9819, 0.9784, 0.9816, 0.9803, 0.9835, 0.9797, 0.9813, 0.9814, 0.9825, 0.9842, 0.9724, 0.9839, 0.9810, 0.9816, 0.9852, 0.9835, 0.9839, 0.9814, 0.9803, 0.9839, 0.9801, 0.9830, 0.9828, 0.9813, 0.9804, 0.9778, 0.9820, 0.9762, 0.9852, 0.9797, 0.9828, 0.9811, 0.9778, 0.9839, 0.9823, 0.9848, 0.9814, 0.9836, 0.9781, 0.9848, 0.9792, 0.9810, 0.9849, 0.9825, 0.9814, 0.9781, 0.9826, 0.9819, 0.9814, 0.9794, 0.9836, 0.9820, 0.9768, 0.9810, 0.9830, 0.9839, 0.9854, 0.9809, 0.9801, 0.9836, 0.9839, 0.9804, 0.9804, 0.9828, 0.9797, 0.9855, 0.9830, 0.9822, 0.9829, 0.9829, 0.9830, 0.9822, 0.9830, 0.9798, 0.9836, 0.9785, 0.9825, 0.9841, 0.9811, 0.9806, 0.9829, 0.9848, 0.9811, 0.9816, 0.9804, 0.9836, 0.9841, 0.9807, 0.9829, 0.9851, 0.9804, 0.9809, 0.9826, 0.9825, 0.9839, 0.9830, 0.9811, 0.9829, 0.9820, 0.9813, 0.9851, 0.9833, 0.9819, 0.9791, 0.9833, 0.9816, 0.9841, 0.9847, 0.9828, 0.9804, 0.9792, 0.9835, 0.9828, 0.9851, 0.9854, 0.9855, 0.9814, 0.9826, 0.9826, 0.9830, 0.9842, 0.9832, 0.9849, 0.9825, 0.9838, 0.9779, 0.9851, 0.9848, 0.9832, 0.9833, 0.9807, 0.9753, 0.9832, 0.9854, 0.9855, 0.9842, 0.9836, 0.9845, 0.9819, 0.9829, 0.9830, 0.9842, 0.9841, 0.9832, 0.9806, 0.9841, 0.9845, 0.9822, 0.9836, 0.9773, 0.9830, 0.9833, 0.9829, 0.9836, 0.9822, 0.9851, 0.9849, 0.9854, 0.9845, 0.9798, 0.9820, 0.9848, 0.9836, 0.9863, 0.9830, 0.9867, 0.9835, 0.9807, 0.9866, 0.9823, 0.9825, 0.9861, 0.9838, 0.9823, 0.9791, 0.9842, 0.9851, 0.9825, 0.9828, 0.9861, 0.9868, 0.9779, 0.9863, 0.9860, 0.9803, 0.9816, 0.9848, 0.9849, 0.9822, 0.9820, 0.9830, 0.9825, 0.9833, 0.9810, 0.9828, 0.9845, 0.9868, 0.9838, 0.9825, 0.9839, 0.9838, 0.9847, 0.9855, 0.9806, 0.9819, 0.9867, 0.9830, 0.9822, 0.9855, 0.9800, 0.9852, 0.9829, 0.9844, 0.9861, 0.9848, 0.9811, 0.9852, 0.9825, 0.9832, 0.9803, 0.9852, 0.9836, 0.9844, 0.9854, 0.9826, 0.9832, 0.9835, 0.9861, 0.9868, 0.9848, 0.9823, 0.9851, 0.9845, 0.9809, 0.9835, 0.9851, 0.9803, 0.9841, 0.9842, 0.9852, 0.9830, 0.9816, 0.9849, 0.9876, 0.9806, 0.9854, 0.9848, 0.9839, 0.9842, 0.9845, 0.9851, 0.9842, 0.9836, 0.9848, 0.9842, 0.9855, 0.9835, 0.9836, 0.9832, 0.9849, 0.9839, 0.9861, 0.9857, 0.9811, 0.9839, 0.9838, 0.9847, 0.9857, 0.9858, 0.9852, 0.9836, 0.9851, 0.9829, 0.9813, 0.9832, 0.9855, 0.9858, 0.9810, 0.9870, 0.9861, 0.9810, 0.9851, 0.9842, 0.9854, 0.9836, 0.9857, 0.9814, 0.9855, 0.9860, 0.9861, 0.9841, 0.9800, 0.9852, 0.9867, 0.9817, 0.9861, 0.9836, 0.9854, 0.9863, 0.9838, 0.9844, 0.9857, 0.9860, 0.9835, 0.9841, 0.9836, 0.9852, 0.9836, 0.9849, 0.9868, 0.9816, 0.9832, 0.9851, 0.9807, 0.9864, 0.9863, 0.9852, 0.9854, 0.9861, 0.9826, 0.9828, 0.9838, 0.9848, 0.9820, 0.9863, 0.9826, 0.9848, 0.9845, 0.9851, 0.9848, 0.9819, 0.9852, 0.9851, 0.9854, 0.9854, 0.9836, 0.9844, 0.9839, 0.9857, 0.9848, 0.9835, 0.9858, 0.9844, 0.9841, 0.9847, 0.9826, 0.9845, 0.9838, 0.9836, 0.9860, 0.9841, 0.9811, 0.9855, 0.9848, 0.9835, 0.9816, 0.9876, 0.9861, 0.9847, 0.9848, 0.9841, 0.9863, 0.9841, 0.9860, 0.9858, 0.9807, 0.9828, 0.9817, 0.9876, 0.9868, 0.9877, 0.9847, 0.9866, 0.9833, 0.9836, 0.9876, 0.9855, 0.9874, 0.9858, 0.9795, 0.9845, 0.9864, 0.9848, 0.9851, 0.9873, 0.9863, 0.9848, 0.9860, 0.9807, 0.9861, 0.9871, 0.9842, 0.9848, 0.9864, 0.9841, 0.9835, 0.9855, 0.9842, 0.9838, 0.9844, 0.9847, 0.9848, 0.9876, 0.9811, 0.9857, 0.9841, 0.9877, 0.9873, 0.9829],\n",
            "    \"val_loss\": [55.881, 41.869, 38.291, 36.717, 35.459, 34.314, 33.612, 33.569, 33.072, 32.631, 32.044, 31.475, 31.204, 30.815, 30.587, 30.424, 29.694, 29.536, 29.226, 28.910, 27.498, 27.185, 29.836, 26.334, 24.872, 25.182, 23.932, 22.457, 22.430, 21.379, 20.921, 19.520, 19.316, 18.546, 18.389, 25.340, 16.700, 16.309, 16.623, 15.885, 15.161, 14.831, 13.657, 12.539, 11.927, 11.452, 11.272, 10.722, 10.430, 0.9995, 0.9386, 0.9441, 0.8761, 0.7704, 10.071, 0.7391, 0.7366, 0.7219, 0.6939, 0.6137, 0.6594, 0.6288, 0.5794, 0.5830, 0.5517, 0.5090, 0.5065, 0.5425, 0.4743, 0.4132, 0.4333, 0.4558, 0.4199, 0.3899, 0.3744, 0.3514, 0.3743, 0.3476, 0.3420, 0.3931, 0.3798, 0.3292, 0.3507, 0.3350, 0.3016, 0.3273, 0.3172, 0.2874, 0.2783, 0.4489, 0.2972, 0.3311, 0.2736, 0.2662, 0.2985, 0.2469, 0.2646, 0.2272, 0.2595, 0.2519, 0.2789, 0.2305, 0.2429, 0.2781, 0.2547, 0.2443, 0.2452, 0.2571, 0.2381, 0.2365, 0.2433, 0.2616, 0.2094, 0.2172, 0.2136, 0.2151, 0.2184, 0.2352, 0.2190, 0.2022, 0.1937, 0.2278, 0.2153, 0.2161, 0.2167, 0.2232, 0.3159, 0.2480, 0.2121, 0.2078, 0.2118, 0.2186, 0.2210, 0.1976, 0.1835, 0.1831, 0.2115, 0.2178, 0.2058, 0.1944, 0.1954, 0.2007, 0.2055, 0.1714, 0.2347, 0.1937, 0.1741, 0.1918, 0.1914, 0.1952, 0.1842, 0.2211, 0.1981, 0.1929, 0.1955, 0.1706, 0.1878, 0.2082, 0.1722, 0.1775, 0.1586, 0.1909, 0.1710, 0.1895, 0.1674, 0.1812, 0.1899, 0.1865, 0.2014, 0.1763, 0.1643, 0.1688, 0.1899, 0.1758, 0.1863, 0.1763, 0.1816, 0.1623, 0.1811, 0.1789, 0.2204, 0.2285, 0.1625, 0.1522, 0.1621, 0.1794, 0.1755, 0.1785, 0.1630, 0.1748, 0.2088, 0.1588, 0.1569, 0.1614, 0.2018, 0.1919, 0.1756, 0.1593, 0.1844, 0.1826, 0.1641, 0.1732, 0.1999, 0.1565, 0.1488, 0.1517, 0.1588, 0.1325, 0.1603, 0.1952, 0.1612, 0.1499, 0.1647, 0.1604, 0.1868, 0.1796, 0.1732, 0.1869, 0.1550, 0.1795, 0.1463, 0.1748, 0.1651, 0.1576, 0.1560, 0.1752, 0.1673, 0.1773, 0.1687, 0.1998, 0.1830, 0.1694, 0.1689, 0.1754, 0.1513, 0.1736, 0.2622, 0.2147, 0.1670, 0.1440, 0.1779, 0.2171, 0.1876, 0.1656, 0.2027, 0.1750, 0.1697, 0.1884, 0.1572, 0.1823, 0.1861, 0.1628, 0.1499, 0.1714, 0.2272, 0.2061, 0.2010, 0.1753, 0.1607, 0.1481, 0.1963, 0.1734, 0.1979, 0.1471, 0.1637, 0.1658, 0.1901, 0.1676, 0.2098, 0.1802, 0.1625, 0.1581, 0.1570, 0.1776, 0.1623, 0.1695, 0.1840, 0.2122, 0.1888, 0.1508, 0.1756, 0.1360, 0.1708, 0.1679, 0.1587, 0.1846, 0.1504, 0.1622, 0.1612, 0.1936, 0.1664, 0.1444, 0.1455, 0.1712, 0.1920, 0.1545, 0.1662, 0.1495, 0.1481, 0.2243, 0.1743, 0.1698, 0.1847, 0.1580, 0.1516, 0.1902, 0.1878, 0.1378, 0.1583, 0.1848, 0.1612, 0.1901, 0.1500, 0.1470, 0.1500, 0.2146, 0.1947, 0.1710, 0.1713, 0.1741, 0.1918, 0.1644, 0.1563, 0.1814, 0.1845, 0.1760, 0.1548, 0.1658, 0.1452, 0.1757, 0.1753, 0.2012, 0.1720, 0.1943, 0.1529, 0.1842, 0.1665, 0.1866, 0.1834, 0.1497, 0.1491, 0.1403, 0.1497, 0.1479, 0.1707, 0.1688, 0.1443, 0.1473, 0.1456, 0.1699, 0.1816, 0.1855, 0.1997, 0.1661, 0.1696, 0.1546, 0.1523, 0.1789, 0.1640, 0.1485, 0.1509, 0.2125, 0.1996, 0.1642, 0.1972, 0.1658, 0.1947, 0.2123, 0.1582, 0.1575, 0.1626, 0.1715, 0.1822, 0.1455, 0.1548, 0.1355, 0.1611, 0.1593, 0.1595, 0.1403, 0.1885, 0.1708, 0.1662, 0.1892, 0.1439, 0.2018, 0.1633, 0.1636, 0.1722, 0.1752, 0.1663, 0.1572, 0.1750, 0.1612, 0.1667, 0.1668, 0.1718, 0.1533, 0.1711, 0.1521, 0.1892, 0.1759, 0.1906, 0.1646, 0.1431, 0.1461, 0.1582, 0.1430, 0.1461, 0.1232, 0.1581, 0.1465, 0.1385, 0.1691, 0.1770, 0.1387, 0.1432, 0.1889, 0.1603, 0.1644, 0.2270, 0.1775, 0.1336, 0.1554, 0.1701, 0.1542, 0.1366, 0.1554, 0.1694, 0.1606, 0.1341, 0.1840, 0.1856, 0.1487, 0.1379, 0.1744, 0.2148, 0.1515, 0.1581, 0.1477, 0.1445, 0.1492, 0.1566, 0.1795, 0.1559, 0.1993, 0.1609, 0.1466, 0.2062, 0.1819, 0.1681, 0.1824, 0.1484, 0.1552, 0.1698, 0.1660, 0.1553, 0.1411, 0.1475, 0.1653, 0.1573, 0.1660, 0.1719, 0.1670, 0.1481, 0.1664, 0.1467, 0.1474, 0.1568, 0.1357, 0.1529, 0.1531, 0.1687, 0.1817, 0.1563, 0.1708, 0.1548, 0.1421, 0.2019, 0.1668, 0.1507, 0.1457, 0.1576, 0.1544, 0.1511, 0.1766, 0.1631, 0.1576, 0.1763, 0.1567, 0.1759, 0.1575, 0.1501, 0.1681, 0.1454, 0.1298, 0.2864, 0.1595, 0.1417, 0.1557, 0.1761, 0.1269, 0.1489, 0.1694, 0.1559, 0.1519, 0.1265, 0.1670, 0.1807, 0.1581, 0.1791, 0.1665, 0.1553, 0.1561, 0.1736, 0.1478, 0.1591, 0.1712, 0.1628, 0.1492, 0.1588, 0.1465, 0.1524, 0.1575, 0.1494, 0.2064, 0.1740, 0.1772, 0.1605, 0.1536, 0.1582, 0.1598, 0.1424, 0.1637, 0.1485, 0.1837, 0.1729, 0.1655, 0.1435, 0.1673, 0.1710, 0.1547, 0.1605, 0.1562, 0.1674, 0.1488, 0.1618, 0.2006, 0.1592, 0.1703, 0.1518, 0.1745, 0.1595, 0.1787, 0.1664, 0.1657, 0.1608, 0.1593, 0.1839, 0.1881, 0.1663, 0.1575, 0.1296, 0.1454, 0.1820, 0.1437, 0.1429, 0.2049, 0.1651, 0.1509, 0.1497, 0.1405, 0.1554, 0.1709, 0.1854, 0.1652, 0.1609, 0.1582, 0.1523, 0.1483, 0.1510, 0.1564, 0.1438, 0.1552, 0.1339, 0.1364, 0.1338, 0.1507, 0.1611, 0.1393, 0.1526, 0.1466, 0.1495, 0.1653, 0.1519, 0.1545, 0.1467, 0.1513, 0.1627, 0.1755, 0.1574, 0.1536, 0.1738, 0.1650, 0.1344, 0.1879, 0.1743, 0.1553, 0.1560, 0.1642, 0.1537, 0.1742, 0.1519, 0.1438, 0.1560, 0.1459, 0.1429, 0.1690, 0.1417, 0.1728, 0.1560, 0.1512, 0.1507, 0.1502, 0.1523, 0.1417, 0.1861, 0.1581, 0.1543, 0.1404, 0.1255, 0.1760, 0.1468, 0.1364, 0.1654, 0.1518, 0.1556, 0.1596, 0.1629, 0.1214, 0.1862, 0.1488, 0.1692, 0.1753, 0.1704, 0.1556, 0.2247, 0.1441, 0.1452, 0.1627, 0.1476, 0.1414, 0.1349, 0.1887, 0.1861, 0.1664, 0.1567, 0.1363, 0.1581, 0.1395, 0.1488, 0.1540, 0.1667, 0.1543, 0.1676, 0.1519, 0.1543, 0.2195, 0.1552, 0.1536, 0.1499, 0.1490, 0.1846, 0.1658, 0.1705, 0.1566, 0.1504, 0.1388, 0.1446, 0.1524, 0.1348, 0.1609, 0.1556, 0.1411, 0.1865, 0.1459, 0.1683, 0.1665, 0.1638, 0.2008, 0.1478, 0.1436, 0.1833, 0.1609, 0.1415, 0.1488, 0.1744, 0.1448, 0.1482, 0.1729, 0.1502, 0.1481, 0.1427, 0.1580, 0.1934, 0.1471, 0.1549, 0.1446, 0.1741, 0.1389, 0.1436, 0.1346, 0.1358, 0.1347, 0.1609, 0.1599, 0.1438, 0.1525, 0.1600, 0.1523, 0.1418, 0.1538, 0.1444, 0.1480, 0.1641, 0.1663, 0.1893, 0.1548, 0.1535, 0.1835, 0.1679, 0.1506, 0.1841, 0.1952, 0.1531, 0.1348, 0.1293, 0.1471, 0.1860, 0.1446, 0.1473, 0.1596, 0.1418, 0.1461, 0.1359, 0.1539, 0.1768, 0.1570, 0.1763, 0.1899, 0.1608, 0.1801, 0.1462, 0.1541, 0.1620, 0.1691, 0.1618, 0.1642, 0.1433, 0.2043, 0.1441, 0.1614, 0.1727, 0.1715, 0.1690, 0.1363, 0.1521, 0.1445, 0.1497, 0.1546, 0.1531, 0.1461, 0.1781, 0.1899, 0.1763, 0.1537, 0.1502, 0.1386, 0.1724, 0.1777, 0.1753, 0.1946, 0.1831, 0.1560, 0.1758, 0.1782, 0.1608, 0.1853, 0.1888, 0.1754, 0.1602, 0.1489, 0.1920, 0.1639, 0.1265, 0.1635, 0.1695, 0.1522, 0.1712, 0.2001, 0.1402, 0.1914, 0.1411, 0.1453, 0.1471, 0.2010, 0.2134, 0.1659, 0.1587, 0.1948, 0.1483, 0.1796, 0.1781, 0.1684, 0.1828, 0.1556, 0.1424, 0.1477, 0.1614, 0.1445, 0.1648, 0.1566, 0.1624, 0.1473, 0.1489, 0.1862, 0.1592, 0.1567, 0.1868, 0.1487, 0.1411, 0.1648, 0.1369, 0.1472, 0.1493, 0.1749, 0.1397, 0.1598, 0.1503, 0.1298, 0.2341, 0.1784, 0.1504, 0.1989, 0.1495, 0.1527, 0.1464, 0.1497, 0.1509, 0.1432, 0.1929, 0.2025, 0.1379, 0.1372, 0.1367, 0.1480, 0.1442, 0.1567, 0.1592, 0.1463, 0.1804, 0.1666, 0.1736, 0.1393, 0.1445, 0.1968, 0.2032, 0.1598, 0.1634, 0.1534, 0.1651, 0.1383, 0.1405, 0.1573, 0.1487, 0.1562, 0.1664, 0.1663, 0.1448, 0.1778, 0.2014, 0.2617, 0.1601, 0.1554, 0.1364, 0.1527, 0.1940, 0.1653, 0.2179, 0.1506, 0.1375, 0.1309, 0.1173, 0.1648, 0.1947, 0.1659, 0.1741, 0.1528, 0.1366, 0.1443, 0.1383, 0.1513, 0.1482, 0.1984, 0.1770, 0.1602, 0.1496, 0.1665, 0.1394, 0.1632, 0.1532, 0.1629, 0.1598, 0.1833, 0.1541, 0.1905, 0.1629, 0.1421, 0.1741, 0.1644, 0.1614, 0.1458, 0.1577, 0.1812],\n",
            "    \"val_accuracy\": [0.0056, 0.0356, 0.0412, 0.0431, 0.0581, 0.0730, 0.0712, 0.0618, 0.0712, 0.0674, 0.0861, 0.1049, 0.1067, 0.1086, 0.0936, 0.1180, 0.1142, 0.1236, 0.1461, 0.1311, 0.1629, 0.1704, 0.1479, 0.1910, 0.2397, 0.2285, 0.2603, 0.3109, 0.3127, 0.3483, 0.3558, 0.4176, 0.4213, 0.4326, 0.4232, 0.2846, 0.5356, 0.5019, 0.4944, 0.5075, 0.5318, 0.5768, 0.5787, 0.6760, 0.6760, 0.7135, 0.6985, 0.7210, 0.7453, 0.7715, 0.7472, 0.7622, 0.7940, 0.8352, 0.7228, 0.8521, 0.8390, 0.8408, 0.8502, 0.8876, 0.8502, 0.8801, 0.9064, 0.8801, 0.9045, 0.9176, 0.9139, 0.9082, 0.9232, 0.9326, 0.9251, 0.9270, 0.9307, 0.9457, 0.9476, 0.9588, 0.9551, 0.9438, 0.9569, 0.9251, 0.9494, 0.9607, 0.9476, 0.9513, 0.9607, 0.9625, 0.9513, 0.9663, 0.9607, 0.9401, 0.9625, 0.9494, 0.9644, 0.9700, 0.9682, 0.9682, 0.9663, 0.9757, 0.9738, 0.9682, 0.9625, 0.9738, 0.9607, 0.9682, 0.9663, 0.9682, 0.9719, 0.9682, 0.9682, 0.9682, 0.9700, 0.9700, 0.9794, 0.9719, 0.9738, 0.9757, 0.9794, 0.9719, 0.9775, 0.9775, 0.9738, 0.9700, 0.9757, 0.9738, 0.9738, 0.9757, 0.9813, 0.9813, 0.9794, 0.9813, 0.9700, 0.9738, 0.9738, 0.9794, 0.9757, 0.9775, 0.9775, 0.9682, 0.9700, 0.9738, 0.9794, 0.9738, 0.9794, 0.9813, 0.9682, 0.9738, 0.9813, 0.9775, 0.9738, 0.9757, 0.9775, 0.9682, 0.9794, 0.9775, 0.9757, 0.9794, 0.9719, 0.9700, 0.9831, 0.9831, 0.9794, 0.9738, 0.9775, 0.9831, 0.9813, 0.9850, 0.9757, 0.9738, 0.9719, 0.9850, 0.9775, 0.9794, 0.9738, 0.9775, 0.9757, 0.9794, 0.9794, 0.9794, 0.9794, 0.9794, 0.9700, 0.9700, 0.9775, 0.9813, 0.9831, 0.9775, 0.9757, 0.9738, 0.9813, 0.9719, 0.9794, 0.9850, 0.9850, 0.9738, 0.9775, 0.9700, 0.9831, 0.9775, 0.9813, 0.9757, 0.9794, 0.9757, 0.9738, 0.9813, 0.9813, 0.9794, 0.9794, 0.9831, 0.9831, 0.9757, 0.9775, 0.9831, 0.9775, 0.9794, 0.9738, 0.9738, 0.9757, 0.9813, 0.9813, 0.9738, 0.9850, 0.9775, 0.9813, 0.9850, 0.9794, 0.9775, 0.9813, 0.9831, 0.9813, 0.9738, 0.9813, 0.9813, 0.9775, 0.9738, 0.9775, 0.9831, 0.9663, 0.9757, 0.9813, 0.9813, 0.9794, 0.9757, 0.9813, 0.9813, 0.9794, 0.9775, 0.9775, 0.9719, 0.9813, 0.9794, 0.9738, 0.9813, 0.9831, 0.9794, 0.9794, 0.9813, 0.9757, 0.9794, 0.9794, 0.9813, 0.9775, 0.9775, 0.9757, 0.9831, 0.9831, 0.9794, 0.9813, 0.9794, 0.9738, 0.9850, 0.9813, 0.9850, 0.9831, 0.9757, 0.9794, 0.9813, 0.9831, 0.9757, 0.9813, 0.9850, 0.9831, 0.9813, 0.9813, 0.9813, 0.9831, 0.9850, 0.9831, 0.9869, 0.9813, 0.9757, 0.9813, 0.9831, 0.9813, 0.9813, 0.9794, 0.9831, 0.9794, 0.9831, 0.9831, 0.9775, 0.9813, 0.9850, 0.9775, 0.9850, 0.9850, 0.9813, 0.9831, 0.9869, 0.9775, 0.9794, 0.9757, 0.9813, 0.9813, 0.9850, 0.9850, 0.9775, 0.9813, 0.9813, 0.9757, 0.9775, 0.9775, 0.9831, 0.9794, 0.9775, 0.9757, 0.9813, 0.9831, 0.9757, 0.9831, 0.9794, 0.9794, 0.9757, 0.9813, 0.9719, 0.9813, 0.9813, 0.9850, 0.9831, 0.9831, 0.9850, 0.9775, 0.9850, 0.9831, 0.9813, 0.9813, 0.9775, 0.9831, 0.9813, 0.9813, 0.9775, 0.9794, 0.9794, 0.9775, 0.9775, 0.9794, 0.9850, 0.9850, 0.9813, 0.9794, 0.9831, 0.9850, 0.9794, 0.9794, 0.9813, 0.9794, 0.9831, 0.9813, 0.9775, 0.9850, 0.9831, 0.9831, 0.9831, 0.9757, 0.9850, 0.9794, 0.9813, 0.9813, 0.9813, 0.9831, 0.9850, 0.9794, 0.9850, 0.9831, 0.9738, 0.9850, 0.9794, 0.9850, 0.9850, 0.9813, 0.9831, 0.9850, 0.9813, 0.9813, 0.9794, 0.9794, 0.9813, 0.9813, 0.9831, 0.9794, 0.9831, 0.9794, 0.9813, 0.9813, 0.9813, 0.9831, 0.9813, 0.9813, 0.9850, 0.9813, 0.9831, 0.9831, 0.9831, 0.9831, 0.9831, 0.9813, 0.9850, 0.9813, 0.9813, 0.9831, 0.9831, 0.9794, 0.9831, 0.9869, 0.9831, 0.9831, 0.9794, 0.9850, 0.9813, 0.9869, 0.9831, 0.9850, 0.9775, 0.9813, 0.9850, 0.9831, 0.9869, 0.9757, 0.9831, 0.9794, 0.9869, 0.9850, 0.9850, 0.9813, 0.9850, 0.9831, 0.9831, 0.9831, 0.9831, 0.9738, 0.9794, 0.9813, 0.9775, 0.9850, 0.9850, 0.9813, 0.9794, 0.9813, 0.9831, 0.9850, 0.9813, 0.9831, 0.9813, 0.9813, 0.9775, 0.9850, 0.9794, 0.9850, 0.9850, 0.9831, 0.9831, 0.9813, 0.9794, 0.9813, 0.9813, 0.9813, 0.9813, 0.9869, 0.9831, 0.9813, 0.9775, 0.9813, 0.9794, 0.9831, 0.9831, 0.9831, 0.9775, 0.9831, 0.9831, 0.9813, 0.9831, 0.9794, 0.9775, 0.9831, 0.9813, 0.9794, 0.9850, 0.9700, 0.9831, 0.9831, 0.9813, 0.9813, 0.9869, 0.9813, 0.9794, 0.9888, 0.9831, 0.9813, 0.9813, 0.9813, 0.9813, 0.9831, 0.9794, 0.9794, 0.9831, 0.9794, 0.9813, 0.9831, 0.9813, 0.9794, 0.9794, 0.9775, 0.9850, 0.9813, 0.9850, 0.9794, 0.9813, 0.9794, 0.9794, 0.9813, 0.9813, 0.9794, 0.9757, 0.9813, 0.9813, 0.9794, 0.9831, 0.9775, 0.9813, 0.9831, 0.9794, 0.9831, 0.9813, 0.9813, 0.9831, 0.9831, 0.9813, 0.9831, 0.9775, 0.9850, 0.9775, 0.9794, 0.9850, 0.9794, 0.9813, 0.9813, 0.9831, 0.9850, 0.9831, 0.9794, 0.9775, 0.9813, 0.9850, 0.9831, 0.9831, 0.9813, 0.9850, 0.9831, 0.9850, 0.9850, 0.9850, 0.9813, 0.9794, 0.9813, 0.9813, 0.9813, 0.9850, 0.9794, 0.9850, 0.9794, 0.9831, 0.9813, 0.9813, 0.9850, 0.9757, 0.9831, 0.9794, 0.9850, 0.9757, 0.9831, 0.9831, 0.9794, 0.9850, 0.9831, 0.9850, 0.9831, 0.9850, 0.9850, 0.9831, 0.9831, 0.9831, 0.9775, 0.9813, 0.9813, 0.9831, 0.9869, 0.9794, 0.9831, 0.9850, 0.9831, 0.9813, 0.9831, 0.9794, 0.9850, 0.9831, 0.9813, 0.9850, 0.9813, 0.9813, 0.9850, 0.9831, 0.9794, 0.9831, 0.9813, 0.9813, 0.9850, 0.9850, 0.9831, 0.9813, 0.9813, 0.9831, 0.9869, 0.9831, 0.9850, 0.9888, 0.9813, 0.9850, 0.9813, 0.9813, 0.9775, 0.9850, 0.9794, 0.9813, 0.9813, 0.9813, 0.9850, 0.9813, 0.9757, 0.9831, 0.9831, 0.9794, 0.9850, 0.9813, 0.9794, 0.9813, 0.9794, 0.9775, 0.9831, 0.9850, 0.9813, 0.9850, 0.9850, 0.9850, 0.9794, 0.9831, 0.9813, 0.9831, 0.9813, 0.9719, 0.9850, 0.9850, 0.9850, 0.9831, 0.9757, 0.9831, 0.9813, 0.9813, 0.9850, 0.9831, 0.9850, 0.9850, 0.9831, 0.9813, 0.9869, 0.9850, 0.9813, 0.9831, 0.9794, 0.9831, 0.9794, 0.9794, 0.9831, 0.9831, 0.9831, 0.9831, 0.9831, 0.9813, 0.9869, 0.9850, 0.9831, 0.9794, 0.9869, 0.9869, 0.9888, 0.9813, 0.9813, 0.9813, 0.9831, 0.9850, 0.9831, 0.9869, 0.9813, 0.9850, 0.9813, 0.9850, 0.9813, 0.9813, 0.9831, 0.9813, 0.9831, 0.9869, 0.9813, 0.9813, 0.9869, 0.9831, 0.9831, 0.9831, 0.9794, 0.9813, 0.9831, 0.9775, 0.9813, 0.9831, 0.9757, 0.9850, 0.9813, 0.9813, 0.9869, 0.9813, 0.9757, 0.9869, 0.9831, 0.9831, 0.9831, 0.9794, 0.9831, 0.9813, 0.9794, 0.9775, 0.9813, 0.9831, 0.9813, 0.9794, 0.9831, 0.9794, 0.9813, 0.9794, 0.9831, 0.9831, 0.9813, 0.9831, 0.9831, 0.9813, 0.9813, 0.9831, 0.9775, 0.9850, 0.9850, 0.9813, 0.9831, 0.9813, 0.9813, 0.9850, 0.9831, 0.9794, 0.9831, 0.9813, 0.9850, 0.9850, 0.9831, 0.9794, 0.9794, 0.9813, 0.9831, 0.9831, 0.9794, 0.9813, 0.9813, 0.9813, 0.9831, 0.9850, 0.9850, 0.9850, 0.9831, 0.9831, 0.9850, 0.9831, 0.9831, 0.9850, 0.9831, 0.9794, 0.9813, 0.9831, 0.9869, 0.9869, 0.9831, 0.9775, 0.9794, 0.9813, 0.9775, 0.9813, 0.9813, 0.9831, 0.9813, 0.9794, 0.9813, 0.9813, 0.9831, 0.9831, 0.9775, 0.9850, 0.9831, 0.9813, 0.9831, 0.9831, 0.9813, 0.9831, 0.9869, 0.9831, 0.9831, 0.9831, 0.9813, 0.9813, 0.9850, 0.9794, 0.9850, 0.9831, 0.9850, 0.9831, 0.9794, 0.9869, 0.9682, 0.9794, 0.9813, 0.9738, 0.9813, 0.9794, 0.9831, 0.9831, 0.9794, 0.9850, 0.9831, 0.9757, 0.9813, 0.9831, 0.9850, 0.9850, 0.9813, 0.9831, 0.9831, 0.9831, 0.9831, 0.9850, 0.9813, 0.9794, 0.9831, 0.9813, 0.9794, 0.9831, 0.9813, 0.9831, 0.9850, 0.9831, 0.9831, 0.9813, 0.9813, 0.9831, 0.9794, 0.9813, 0.9850, 0.9850, 0.9813, 0.9738, 0.9850, 0.9813, 0.9869, 0.9869, 0.9850, 0.9813, 0.9757, 0.9850, 0.9869, 0.9869, 0.9869, 0.9831, 0.9813, 0.9813, 0.9850, 0.9850, 0.9850, 0.9850, 0.9850, 0.9850, 0.9757, 0.9813, 0.9794, 0.9794, 0.9813, 0.9813, 0.9850, 0.9813, 0.9831, 0.9831, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9831, 0.9794, 0.9813, 0.9850, 0.9831, 0.9813, 0.9813]\n",
            "}\n",
            "history = pickle.load(open('/content/drive/MyDrive/_PTYXIAKI/history_4.pkl', 'rb'))\n",
            "# Plot training & validation accuracy values\n",
            "plt.plot(history.history['accuracy'])\n",
            "plt.plot(history.history['val_accuracy'])\n",
            "plt.title('Model accuracy')\n",
            "plt.ylabel('Accuracy')\n",
            "plt.xlabel('Epoch')\n",
            "plt.legend(['Train', 'Validation'], loc='upper left')\n",
            "plt.show()\n",
            "\n",
            "# Plot training & validation loss values\n",
            "plt.plot(history.history['loss'])\n",
            "plt.plot(history.history['val_loss'])\n",
            "plt.title('Model loss')\n",
            "plt.ylabel('Loss')\n",
            "plt.xlabel('Epoch')\n",
            "plt.legend(['Train', 'Validation'], loc='upper left')\n",
            "plt.show()"
            ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "metadata":
            {
            "id": "SKFDKVu252PV"
            },
        "outputs":
            [
            ],
        "source":
            [
            "# @title (BETA) [MAY] Augmented Main\n",
            "# Source: 10.1109/ICSEC.2018.8712781 | https://ieeexplore.ieee.org/abstract/document/8712781"
            ]
        }
    ],
"metadata":
    {
    "colab":
        {
        "authorship_tag": "ABX9TyMuwzw20EeiINTiskz6jxLi",
        "mount_file_id": "1LvBZY4PeEQUJon-PwTQvgFAYOzOvgIS4",
        "provenance":
            [
            ]
        },
    "kernelspec":
        {
        "display_name": "Python 3",
        "name": "python3"
        },
    "language_info":
        {
        "name": "python"
        }
    },
"nbformat": 4,
"nbformat_minor": 0
}
